## 3챕터 - 신경망 이어서...
계단함수는 numpy의 트릭을 활용해서 구현할 수 있는데, boolean 인덱싱을 활용해서 x>0인지 아닌지를 통해 boolean array를 만들고 arr.astype(np.int) 를 활용해서 true는 1, false는 0으로 만든다면 이것이 바로 계단함수의 구현이라 할 수 있다.
혹은 np.array(x>0, dtype=np.int) 를 통해 바로 계단함수를 거친 결과로 만들 수 있다.

시그모이드 함수는 h(x) = 1 / 1+e^(-x) 로 표현할 수 있다. 시그모이드는 S형태의 결정함수이다. 스칼라로 연산할 수 있기에 브로드캐스트로 모든 ndarray에 적용시킬수 있다.

#### 계단 함수 / 시그모이드 함수
계단 형태의 갑작스러운 변화 / 곡선을 통해 연속적으로 변화

하지만 둘 모두 입력이 작을 때 출력이 0에 가깝고 입력이 클 때 출력이 1에 가까운 구조를 갖는다. 큰 관점에서는 같은 모양을 하고 잇는 것이다.
또한 두 함수는 둘 모두 **비선형 함수**이다. 계단함수는 꺾인 직선, 시그모이드는 곡선으로 표현되는데 선형은 선 하나로 표현이 가능해야하기에 둘 모두 비선형이다. 선형함수는 f(x)=ax+b 이다.

선형함수로는 아무리 층을 깊게 해도 층을 쌓지않고도 표현할 수 있기에 층을 쌓는 의미가 없다. 예를 들어 선형함수가 h(x)=cx라고 하면, 아무리 이걸 3~4번 겹쳐 쌓아도 y(x) = c * c * c * x 로 표현이 가능하고, 이 또한 결국 a=c^3 로 표현하면 y(x)=ax와 다를바 없다.
**따라서 층을 쌓는 것에서 이점을 얻고자 하면** 활성화함수로 계단, 시그모이드와 같은 **비선형 함수**를 사용해야 한다.
#### ReLU 함수
시그모이드 함수가 오래전부터 이용했다면, 최근에는 ReLU함수를 주로 사용한다. ReLU는 간단하게, Rectified (정류된) 이라는 뜻이다. 0이 넘으면 그대로 출력하고, 0 이하면 0을 출력하는 함수이다. 즉 양수면 그대로 출력하고 0 이하면 0으로 출력한다.
np.maximum(0, x)를 해서 x의 값과 0값을 비교해서 더 큰 값을 선택하도록 하여 구현할 수 있다.

> 왜 ReLU를 사용할까? 너무 간단한 활성함수 같아 보이는데??? 이러면 음수의 값을 신경망에 사용하지 않겠다는 것 아닐까?
> 왜 Sigmoid가 아닌 ReLU를 사용하는지 찾아보았다. 4개의 장점을 찾을 수 있었다.
> 1) 계산 효율성이 좋다. 활성화 함수에 입력한 값을 얼마나 빠르게 출력할 수 있을에 따라 계산 효율성이 결정되는데, Sigmoid는 지수함수가 포함되기에 연산에 있어서 나눗셈같은 복잡한 연산이 수행되지만, ReLU는 간단하게 계산할 수 있기에 효율성이 높다,
> 2) Gradient 소실 방지 : Sigmoid의 경우에는 가중치가 소실되는 문제가 있다(?). 입력값이 매우 크거나 매우 작으면 그 기울기가 0에 가까워지는데, 기울기가 0에 가까워지면 가중치 업데이트가 매우 느려지고 (학습이 느려짐) 신경망 수렴이 어렵다. local minimum에 갇힐수있다. ReLU는 양수 입력에 '일정한' 값을 가지기에 이 문제를 방지 가능하다.
> 3) 포화문제(Saturation problem)를 방지하여 심층 네트워크에 적합 : 포화란 활성화 함수의 출력이 1 혹은 0 에 매우 가까워져서 가중치와 편향을 업데이트하기 어려워져 신경망의 학습 능력이 제한되는 경우를 의미한다. 
> 4) ReLU는 희소활성화(sparse activation)를 생성한다. 많은 뉴런이 0을 출력하게하기에 **활성화**가 희소하게 된다. 한번에 몇개의 뉴런만 활성화 되는 것. 계산비용도 줄지만 과적합을 줄이는데에 도움이 된다.
> ReLU 함수의 단점도 있다.  DNN에서 일부 뉴런이 비활성화되어 출력되는 Dying ReLU문제가 있다고 한다. 이 문제 해결을 위해 음수부분에 작은 기울기를 추가한 Leaky ReLU를 활용한다고 한다.
> 
> 그렇다면 ReLU만 사용하고 Sigmoid는 사용안하는가? 그건 아니다. 물론 활성화 함수로 ReLU가 널리 사용되고는 있지만, 아직 RNN(순환 신경망)과 같은 일부 경우엔 Sigmoid를 주로 사용한다. Sigmoid가 0~1사이를 가지면서도 확률이나 백분율 표현에 유용하기 떄문이다. RNN에서 Sigmoid를 사용하면 출력값이 원하는 확률범위 내에 있는지 확인가능하다. 또한 입력데이터에 미묘한 변화를 감지하기 위해 곡선형태인 Sigmoid를 사용한다. 만약 시간 변화에 따라 입력 데이터의 변화를 감지해야한다면 Sigmoid 함수가 적합하다고 할 수 있다.
> 또한 Sigmoid는 0과 1사이의 실수값으로 매핑되기에 특정 클래스에 속할 확률로 해석이 가능한데, 데이터가 만약 1/0이 아니라 이진/범주형이라면 Sigmoid를 활성함수로 사용하는것이 좋다.
> 
> ReLU는 **연속적이거나 작업에 회귀가 포함되어 있다면 사용하기 좋다.** ReLU는 출력값이 무한이기에 연속변수에 적합하고, 이는 회귀 문제에 일반적으로 사용된다. 또한 심층신경, 깊은네트워크에 Gradient 소실 문제가 없으므로 효율적으로 사용될 수 있다.
> 
> 하이퍼볼릭 탄젠트라는 tanh 활성함수를 사용하여 -1부터 1범위의 곡선활성함수를 사용할수도있는데, 최적 가중치를 찾는 과정에서 zigzag현상을 보완할수있지만 이 또한 Gradient 소실문제를 가지고있다.

#### 다차원 배열 계산
2차원 ndarray는 행렬이라고 흔히 부른다. 행렬의 곱을 우리가 구할 땐,
1) A행렬의 n번째 행과 B행렬의 n번째 열의 원소들을 차례대로 곱한다.
2) 각 원소들의 곱한 값들을 모두 합하여 n번째 행 n번째 열 값으로 정한다. 
3) 그 예로, A의 첫번째 행과 B의 두번째 열을 1,2번 과정을 거친뒤 해당 값을 첫번째 행의 두번째 열에 넣는다.
4) 모든 조합 (A의 행 n개와 B의 열 n개를 조합하여..)에서의 결과를 작성하면 끝

이러한 행렬곱을 Numpy에서도 가능하다. 바로 두 행렬을 np.dot(A, B)로 하여 A행렬과 B행렬의 행렬곱 ndarray를 반환할 수 있다.

행렬에서 주의해야할 점은, 만약 A의 행과 B의 열의 원소개수가 다르면 **계산 할 수 없다**. 이 점을 유의해야함.
만약 3x2의 행렬이 있고, 2x4의 행렬이 있으면 이는 둘다 곱하는 원소의 개수가 '2'로 행렬곱이 가능하다 (결과로 3x4의 행렬이 도출됨.)
3x2 행렬을 2라는 1차원 배열에 곱하는 것도 어찌보면 2x1이라는 행렬에 곱하는 것이기에 연산이 가능하고, 결과로 3이 나타난다.
#### 신경망에서의 행렬곱
이러한 numpy 행렬을 통해 신경망 구현이 가능하다. 신경망이 x1, x2가 있고 y1, y2, y3이 있다고 하면, 각각 y 노드로 가는 가중치 w를 [ [ x1w1, x1w2, x1w3 ], [x2w1, x2w2, x2w3] ] 이렇게 2x3의 행렬로 표현한다. 그리고 행렬곱을 활용하여 3이라는 결과를 얻을 수 있다. (각각 y1, y2, y3값)

3층 신경망을 예로 볼때
x1 x2(0층 신경망)가 있고 이들이 엮여서 만들어진 a1, a2, a3가 있고, 이들로 만들어진 b1, b2가 있고, 이들로 만들어진 y1, y2가 있다고 해보자. 가중치가 세 번 나올것이다. 3층 신경망이다.
이 모습은 2 * 2x3 * 3x2 * 2x2 이다. 최종적인 결과는 2의 결과로 나타날 것이다. 
편향 b를 뜻하는 노드도 추가해준다. a1의 수식은 다음과 같다. w11 x1 + w12 x2 + b1 해석하면, 다음 층의 첫번째 노드로 가는 가중치를 이전 계층의 첫번째 노드 곱하여 이전 게층의 두번째 노드 가중치와 노드의 곱을 더한다. 거기에 편향 1 * b를 더하면 다음 계층의 첫번째 원소인 a1값을 구할수있다.
행렬곱을 활용하여 간소화 가능하다. A = XW + B 이다. X는 (x1 x2) 이고 W는 ((w11 w21 w31) (w12 w22 w32)) B는 (b1, b2, b3)이다.

결론적으로는 X, W1, B1을 사전에 입력받기만 한다면 이들을 활성화 함수를 활용하여 다음 a를 표시하고, 활성화 함수를 이용하여 다른 값으로 변경한다.  예제는 Sigmoid 사용

이렇게 입력값, 가중치W, 편향B를 활용해서 A를 만들었다면 이를 활성함수를 거쳐 Z를 만들고 다시 이들을 입력값으로 하여 두번째 가중치 W를 활용하고... 반복하여 최종적인 y값을 얻어낸다.

만약 3층 신경망을 코드로 설계한다고 가정하면.. X를 입력받은 뒤 W1값과 B1값을 정하고, W1 * X1 + B1 하여 A1를 구하고 Sigmoid를 거쳐 Z1를 구한다. 그렇게 구해진 Z1로 이번에는 2층을 구하기 위해 W2값과 B2값을 토대로 W2 * Z1 + B2하여 A2를 구하고 다시 Sigmoid로 Z2를 구한다. 마지막 3층을 구하기 위해 W3 * Z2 + B3하여 A3를 구하고 최종적으로 활성함수를 거쳐 Y를 구한다.

출력층의 활성함수는 다른 함수로 정할수도 있다. (풀고자 하는 문제가 회귀면 항등함수, 2클래스 분류면 Sigmoid 함수, 다중 클래스 분류면 Soft-Max 함수가 일반적임)
#### 구현 정리
3층 신경망을 구현하기 위해 처음에 W1, b1, W2, b2, W3, b3를 정의한다. 그리고 사용자로부터 X를 입력받는다. 위에서 정의한 W, b를 활용해서 a1=np.dot(x, W1)+b1 => z1=sigmoid(a1) => a2=np.dot(z1, W2)+b2 => z2=sigmoid(a2) => a3=np.dot(z2, W3)+b3 => y=활성함수(a3) => return y를 한다.

최종적으로 만들어진 y를 출력하면 된다. 이렇게 만든것이 3층 신경망이다.
#### 출력층 활성함수
위에서 잠깐 이야기했는데, 어느 문제냐에 따라 출력층의 활성함수가 달라진다. 일반적으로 **회귀는 항등함수(그대로 출력), 2클래스 분류는 Sigmoid, 다중클래스 분류는 Soft-Max를 사용**한다.

기계학습은 **분류(classification)와 회귀(regression)** 로 나뉘는데, 분류란 성별분류처럼 나누는것이고, 회귀는 입력데이터에서 연속적인 수치를 예측하는 문제?
사진 속 인물의 몸무게를 예측한다 하면 그건 회귀이고, 사진 속 인물의 성별을 분류한다 하면 분류이다. (마찬가지로 비만인지 아닌지 구분하는거면 그건 분류이다. 회귀는 연속적인 값을 예측하는 것이다)
#### 항등함수, 소프트맥스 함수 구현
회귀에 사용되는 항등함수는 구현이랄것도 없이 그냥 그대로 출력해주면 된다. identity_func(x): return x 라고 그냥 끝내도 된다. 
분류에서 사용하는 Soft-Max 함수는 y_k = e^(a_k)/ 시그마(1에서n)e^(a_i) 이다.
SoftMax 함수는 모든 입력신호로부터 화살표를 받아서 출력하는 것이다.
브로드캐스트로 e^(A) 한 결과를 sum하여 전부 더하고, e(A)에 전부 더한 결과를 나누는 것이다.

Softmax는 오버플로의 문제가 있다. 지수함수가 쉽게 큰 값을 내놓기에 문제다. e^10만해도 2만이 없는다. e^1000만 해도 무한대를 나타내는 inf가 나타난다. 이런 큰 값끼리 나눗셈을 하면 불안정하다.
이를 개선할 수 있는데...
C * e^(a_k) / C * 시그마(1부터n)e^(a_i) 로 하면 지수함수 안에 C를 넣어 e^(a_k + logC)로 만들 수 있다. 한마디로 자연지수승에 어떠한 분모,분자에 값을 더하거나 빼도 똑같이 작동한다. 이를 활용하여 최댓값을 브로드캐스트로 빼서 오버플로를 막는 방안을 생각할 수 있다.
> C는 C^(lne)로 만들수 있고, 이는 e^lnc라고 표현할 수 있다. (lne가 1이기에)
> e^a * e^c 는 e^(a+c)로 활용할 수 있다는 것을 사용.
> e^(ln2) = y 이다.  lny = ln2 이다. y는 2이다. 그러면 e^(ln2) = 2이고, 2^(lne)와 같으니 e^(ln2) = 2^(lne)이다.
> 그냥 요약해서, C를 e로 지수화시키면 e^logC이다.

1000이 넘는 값을 np.exp(a) / np.sum(np.exp(a)) 사용하면 오버플로 가능성이 있지만, 임의의 C값을 c=np.max(a)로 구한뒤 a-c하여 너무 큰 값들을 낮춘다. 이러한 np.exp(a-c) / np.sum(np.exp(a-c))하면 잘 작동한다.

SoftMax의 특징은 0과 1사이의 실수라는 점이다. 또한 SoftMax **함수 출력의 총 합은 1이다**. 따라서 SoftMax를 확률로 사용가능하다. n번 클래스일 확률을 분석할 수 있는것.
또한 대소 관계는 변하지 않는다. 이전에 a1이 a2보다 컸다고 해서 z1이랑 z2 결과가 역전되지는 않는다.
다중클래스에서 분류를 한다고 하면 Softmax함수를 쓰든 안쓰든 가장 큰 출력을 내는 뉴런(노드)는 변하지않는다. 따라서 출력에서 SoftMax를 생략해도 된다. 실제로 현업에서는 지수함수계산 자원을 아끼고자 생략하는것이 일반적이다.

기계학습은 학습/추론 두단계로 이루어지는데 학습은 Softmax를 쓰지만 실제 추론을 할 때는 Softmax를 생략하는것이 일반적이다.

#### 출력층 뉴런 수 정하기
출력층 뉴런 수는 푸는 문제에 맞게 적절히 정해야한다. 분류에선 분류하고 싶은 클래스의 수로 설정하는것이 일반적이다. (예를들어 0부터 9까지 분류해야하면 클래스는 10개이기에 출력 뉴런 수는 10개이다.)

#### MNIST 손글씨 숫자 인식
신경망 실제로 구현해보며 학습한 것을 실습한다.  MNIST는 기계학습에서 유명한 손글씨 데이터셋이다. 이것을 활용해 간단한 실습, 논문 발표 연구까지 다양하게 사용한다.  0부터 9까지 숫자 이미지로 구성되고 훈련이미지 6만장, 시험이미지 1만장이다. 훈련이미지로 학습하고 학습한 모델로 시험이미지를 얼마나 잘 분류하는지 평가한다.

추론 과정을 신경망의 '순전파' 라고도 한다.

MNIST 이미지는 28x28크기의 색이없는 이미지이고 각 픽셀은 0~255까지 값을 가진다. 또한 각 이미지는 실제 의미하는 숫자가 레이블(정답)로 붙어있다.
MNIST 데이터셋을 받아서 이미지를 numpy 배열로 변환해주는 스크립트를 활용한다.

MNIST data를 load할때 flatten= True로하면 1차원배열로 불러와진다. 원래 이미지로 변경하고싶으면 img.reshape(28, 28)로 다시 돌릴수있다. (shape를 28x28로 변경)
load_mnist()에서 인자로 normalize를 True하면 0~255까지의 픽셀값이 0.0~1.0으로 변경된다. 이렇게 정규화같은 전처리도 가능하다.

현업에서도 신경망을 위한 전처리를 활발히 사용한다. 전처리로 식별능력개선, 학습속도향상 등 사례가 많다. 현업에선 데이터전체 분포를 고려한 전처리를 많이한다. 확산범위 제한하거나, 데이터들이 0 중심으로 분포하게 이동하거나, 전체데이터를 균일하게 분포시키는 데이터백색화를 하거나...

#### 배치처리
입력은 100장이미지로 입력받는 경우 100x784로 입력이 시작되어도 출력은 0~9까지 분류되는 100x10 가 나타날 수 있다. (100장의 분류결과가 한번에 출력)
이렇게 하나로 묶은 입력데이터를 **배치(batch; 묶음)** 라고 함. 이미지 여러장이 한번에 묶여있는것임.

배치처리를 하면 컴퓨터 계산에 큰 이점이 있다. 대부분의 수치계산 라이브러리들은 큰 배열 계산에 최적화되어있다. 또한 데이터 전송이 병목이 되기도하는데, 이러한 부하를 줄일수도있다.
그니까, 입력 데이터를 묶어서 배치 단위로 추론처리를 하면 더욱 빠르게 결과를 얻어낼 수 있다.

#### 3챕터 완료 (106페이지 까지)