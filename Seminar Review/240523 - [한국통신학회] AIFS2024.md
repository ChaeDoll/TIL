# 한국통신학회 AI Frontiers Summit 2024
## 개회식
## 네이버클라우드
- 어마어마하게 transformer가 급격히 커지고있다. 조단위가 넘어가고 있음.
- transformer가 만능은 아님. 모델사이즈 점점 커지고 학습에 입력데이터를 더 많이 넣고싶고, 더 빠르게하고싶다? => 모델사이즈가 점점커지고 transformer가 모든것을 대체하는데 반드시 좋기만한건아님
- transformer사이즈는 410x / 2years 인데 AI 하드웨어 메모리는? 2x/2years
- transformer사이즈를 감당하기엔 아직 ai 하드웨어 메모리가 감당하긴 어려운 상황이다.
- 반도체가 어떤 계산능력을 가지고있나? 보지도않는다. 메모리에서 나온다. 

- LLM과 같은 초거대 AI를 보면 GPU를 어떻게 관리?
- 무어의 법칙이 아직까진살아있다.
- 최적화를 어떻게 할거냐 연구.
- 배치사이르를 올린다?(throughput을 높게) Latency에 불리해진다. 버스 고객이 많으면 버스가 느려지는것같은느낌
- latency... 
- GPU는 그래픽을 위한것이니, 효율적인 NPU를 개발해야한다. 모델이 커지기 때문에 더 큰 많은 SRAM을 가진 하드웨어를 설계해야한다.
- NVIDIA는 학습을 위한 개발을 하기에 Inference는 비효율적이다.
- 아무리 좋은 하드웨어 만들어도 CUDA에 익숙해진 개발자들이 쓰려하지않는다. (CUDA가 뭐지?)
- 토큰당 3ms정도로 어마어마하게 빨라졌다.
- 어떻게 NVIDIA에 대항? 

## Keynote (AI for Health with Wearables) - Prof. Chenyang Lu (Washington University in St. Louis)
- Reliable prediction of clinical outcomes : Wearable data are fine-grained, noisy, incomplete
- Scaling predictive models
- Deep Learning for a large and diverse cohort
- Need : Predict Outcomes of Pancreatic Surgery
    - Pancreatic cancer has a 5year survival rate less than 5% (5년 생존률이 5%보다 낮다?)
    - Surgery is the only cure(수술만이 치료방법) but commonly followed by complication 복잡함이 따른다?
    - predict postoperative complications before surgery
- 목표 : predict postoperative complications with preoperative data (수술 전 데이터로 수술 후의 합병증을 예측)
- Machine learning apporach
    - samll cohort => avoid deep models...
- denoise time series of daily features (SSA)
- Predictiong Personalized Treatment Response
- IoMT for Precision Medicine

-- 11:15까지 Break Time --

## 고려대학교 김찬우 교수
- Basics of Neural Sequence-to-Sequence Processing
- Automatic Speech Recognition (ASR)
- Natural Language Understanding (NLU)
- 앞으로의 트렌드는 어떻게 되는지...
- Ondevice Speech 인식 and 자연어 이해 (ondevice로 음성인식이 탑재) 차츰 발전하여 이제 2023에 default로 ondevice가 되었음
- Speech와 Language쪽이 나뉘어 발전되기 시작한다. 현재에 있어서는 GPT-4 이후 GPT-4o, Speech-GPT
- On-Device LLM이 2024년에 탑재됨 (갤럭시 s24).
- ASR, NLU는 모두 머신러닝으로 이루어짐.
- BERT는 구글에서 나왔는데 이걸 Pre-Training과 Fine-Tuning한다. NLU에선 Intent와 Slot을 찾아야한다.
- NLU를 위한 Generative Model
- text만 이용하던 LLM에서 이제 Speech GPT라는 모델이 나왔다. text vocab와 speech vocab를 합쳤다.
- speech뿐만 아니라 text까지 input output에 대한 것까지 고려한다.
Whisper같은 경우를 본다면 멀티랭귀지 인코더디코더가 되어있다?
- 요약 : 

## 대한통운 CJ와 AI (김민수 상무 CJ대한통운)
- 어떻게 하면 더 싸고 빠르게 배송하고자 하는 이커머스 산업 내 경쟁이 심화됨
- 이를 위해 로봇기반 유연한 자동화, AI빅테이터 분석을 통한 지능화, 디지털트윈기반 최적화, 개방형 물류 IT플랫폼 활용
- 물류산업에서 어떻게 AI와빅데이터를 사용중인지? 최적화에 관해 많이 사용됨. 배송데이터를 활용한 라우팅 솔루션, 창고관리를 얼마나 잘 할것인가. 디지털트윈을 활용하고 있음.
- IT시스템이 중요하고, 이를 통해서 데이터를 모아 AI/빅데이터가 가능해진다.
- 이러한 분석 및 예측 기술을 통해 물량 예측을하고, Dynamic Pricing, 택배 상자 물성분석, 수송 최적운임에 사용된다.
- 국내 운송시장 이슈는 다단계 거래 구조와 영세한 사업자 위주의 플레이어 및 아날로그 중심의 업무 방식때문에 여전히 비효율적임
- 이러한 문제를 해결하여 비용이 절감됨. 또한 복화운송을 통해 기존 운송비 대비 10~20%절감 가능. 
- 산업 특성에 따라 어떠한 AI툴을 사용해야하는지가 정말 다르다. 

## 서울대학교 곽노준 교수 - Deep Image Generative Models
- 영상 생성 모델에 대한 설명
- 시를 넣으면 이미지가 생성됨. 비디오까지 생성하는 SORA도 무료로 사용할 수 있다. 토큰을 교체, zero shot 모델로 source 이미지를 활용해 다른 이미지로 변경이 가능하다. 포즈를 바꾸던가, 일어나있는 강아지를 자게 만들거나..
- Generative Models은 Estimating Distribution이다.
- 왜 생성형 모델이 어렵나면, 샘플링을 위한 모델이 너무 높은 Dimension이 필요하다.
- Denoising Autoencoders란 x를 Encode하여 Z를 만들고 Decode하여 X'를 얻는다. 근데 x에 노이즈를 추가하여 넣고, 이를 decode하여 더 좋은 결과를 얻어낸다. 
- GAN (Generative Adversarial Networks) 가장 큰 문제는 학습이 어렵다? generator가 성능이 좋아서 학습이 안된다.
- Diffusion모델 같은 경우엔 2020년에 각광받기 시작함. 작은모델에서 diffusion하기 시작한 모델이 더..
- DDPM이 바로 Denoising Diffision Probabilistic Model이다.
- DDIM이라는 모델도 있고 LDM이라는 모델도 있다. 
- 아무튼 엄청나게 빠르게 발전하고 있는 분이야다. 소리를 통해 영상도 생성할 수 있다.

## DGIST 소진현 교수
- 개인은 모두 차이가 있기에 여러사람의 데이터들을 합쳐서 학습이 가능하다. 근데 Privacy를 지키면서 Communication Efficiency를 향상시켜야 한다. 최근 연구에 따르면 데이터 보내지않고 학습된 local 데이터만 가지고도 학습이 가능? Secure Aggregation. 각각 모델을 마스킹하여 서버가 개개인의 정보를 알 수 없게한다. 학습만 가능하고 정보를 알수는 없다. 연합학습은 dropouts라는 문제가 있다. google은 이를 위해 방법을 제시함.
- 새로운 관점 : Encoding & Secret Sharing => Uploading shares => Reconstruction
- 세가지 Objectives가 있다.
- 정말 많은 사용자가 있더라해도 user1의 데이터를 뽑아내는 것은 불가능한것이 아닌데 이를 생각하여 개인의 Privacy를 지키며 사람들을 위한 정보를 학습하는 과정을 들었다.

## 서울대 주한별 교수 - Generative Modeling for Photorealistic 3D Digital Humans
- Scan이 주어졌을 때, 사용자가 착용한 옷을 분리하고 다른 사람에게 적용하는 기술이다. 기본 body Pose를 가지는 SMPL-X를 학습하고 이를 활용해 사람을 샘플링하는 것이 활용되었다. 이를 위해 사람에 대한 것을 학습해야 한다. 바로 생성하기엔 이미지에 비해 3D는 어려운 분야이다. input으로 원하는 Pose를 받고 2.5D로 만듦
- 2.5D가 나오면 3D로 깎는다. Optimized mesh를 얻음
- 이렇게 두 단계로 나누어 generative 모델을 만들어 낸다.
- 원하는 human pose를 SMPL-X에 적용, 이후 Diffusion Model을 통해 적용시킴.
- 생성을 앞, 뒤를 따로따로 찍고 만들면 되지않나? 하지만 실제로 앞뒤가 다른 모델이 생성되곤 한다.
- train data가 있으면 좋겠지만, 이러한 데이터가 많지가 않다. public으로는 500개 정도밖에 없는 정도.
- Dual Normal Map을 만들고 이를 토대로 Optimization을 한다. (깎아주기)
- 이렇게 Dual normal map generation을 해서 2.5D를 만들고 Normal map-based mesh optimization을 통해 Optimized mesh를 만들어 내는것까지 완성했는데, 이를 추가적으로 Refine step을 통해 더 개선했다.
- 이미 만든 3d 모델을 주고 Diffusion Model에게 더 개선해서 만들어달라는걸 요청하면 진짜 잘 해낸다. Realism이 떨어지면 해당 부족한 부분을 update하여 추가적으로 보완해낼 수 있다.
- 최종적으로 더 좋은 얼굴 퀄리티를 가진 3D모델을 얻어낼 수 있다.
- pose를 바꿔주면 원하는 pose에 맞는 모델이 생성된다. 재밌는것은, input으로 텍스트를 준다면 그에 맞는 output이 나타난다. 
- 두 번째 논문 : NCHO 
- 우리가 원하는것은 사람은 그대로, 옷만 갈아입히는것.
- Compositinal Generative Model. Human을 만드는것과 Object를 만드는것 두개를 결합한 모델이다. 이러면 Human을 유지하며 Object를 뽑아낼 수 있다.
- Human+Backpack의 모델이 있다면 Human Part만 바꿔도 Object Part는 유지가 되면서 Human만 바뀐다. 이렇게 나누어서 유지했기에 악세사리가 유지되는것이다. 반대로 Object를 바꾸고 Human을 유지하면 이것도 잘 된다.
- 이것을 통해 디지털휴먼을 쉽게 만드는것이 가능해진다.
- sourceHuman을 통해, 열심히 데이터를 캡처하고 Collection했다.
- 문제는, 이 sourceHuman 경우에만 학습이 되었을 것이다. 이 human밖에 학습이 안되었을텐데 그런데 이것에 의의가 무엇인가? 여기서는 Object를 학습시키는것이다.
- human model이 설명하지 못하는 부분을 Object로 만들고, 이렇게 Object를 뽑아내는 것이다.
- 이렇게 Object들을 뽑아냈다면, 임의의 다른사람에도 입힐 수 있게되는 것이다. 더 재밌는것은, object를 추가적으로 계속 붙여낼 수 있다.
- 마지막 논문 : GALA 모델 (Generating Animatable Layered Assets from a Single Scan)
- Scan된 것에 대해 user가 명령어를 입력하면 그 Object를 분리해준다. 옷을 입고있는 object를 셔츠와 맨몸인 사람으로 분리한다. 원래는 없던 부분을 생성해서 없는부분도 채워주는것이다.
- 그리고 이렇게 분리한 것들을 합쳐서 새롭게 만들수도있다. 다시 Composed가 가능하다. 애니메이션도 가능하다. singlescan으로 모든것이 가능하다.
- 타겟이 있을 때, Canonicalize하여 기본이 되는 mesh의 skeleton과 연결을 시켜준다. 기본 mesh를 왔다갔다하며 그 과정도 자연스레 학습이 된다.
- 3D 스캔이 필요하지도 않다. 이미지 한장으로 3D스캔을 얻을수도 있다. 이 상태에서 decomposed도 가능
- 심지어 text를 통해 생성모델로 만들어진 3d모델을 decomposed하는것도 가능하다. 
- body뿐만 face도 자유자제로 변경이 가능하다.
- motion은 캡처가 굉장히 어렵다. 이를 위한 장소도 만들고 모션도 학습하고... 모션생성모델을 만들기 위해 노력중이다.

## 16:30분 강의 ZOOm
### GAN (Generative Adversarial Networks)
- generative를 학습하여 fake를 real처럼... generative가 real을 더욱 잘 만들어낼수있게됨
- 위조지폐와 generator와 같은역할. 경찰이 discriminator 같은 역할 (가짜탐지기 그런건가?)
- GAN과 NeRF를 합쳐서 GIRAFFE 
- 최근엔 4D 널프가 생기고 있다. text를 주면 비디오를 주는 (카메라 각도 변경) 방향으로 나아가지 않을까 싶다.
- NeRF가 가지는 다른 비디오 생성과 다른점? 장점? => 카메라 각도를 볼 수 있다는것. Text를 통해 비디오를 만들어 냈을 때, 이 비디오가 원하는 각도가 아니어도 텍스트를 통해 바꿀수있겠지만 그게 마음대로 잘 안됨. NeRF사용하면 카메라 각도를 바꿀수있기에 Text to Movie의 현실화?

## 16:51분 마지막 발표? (포스택 공대 Associate Professor 오태현)
### Learning to Visualize the Invisible Signals
- AI 관련한 컴퓨터비전.. 등.. 트렌드인만큼 아카이브 엄청 많이 올라오고 있기도 함.
- 본인이 하고있는 연구에 대한 발표를 진행하심
- 눈에 보이지 않는 것을 보이게 만드는 이유? 궁극적으로 무엇을 위해 Perception을 만드는지. 
- 사람의 Intelligence(인지)는 perception과 higher cognitive processes로 이루어짐?
- 이러한게 LLM으로 대체할 수 있지 않을까? 말을 하고있네? 눈도 붙여볼까? 귀도 붙여볼까?
    - Multi-modal Interface로 발전하고 있는 추세이다.
- 현재 방향, 추후 방향이 어떤가
- 사람은 지각능력 뿐만 아니라 눈, 귀가 아니더라도 다른감각의 인지를 통해서도 상상력으로 무언가를 채운다. 즉, 상상력이 필요하다. Crossmodal Translation을 통해 ...하려고 함
- 앞으로 중요한 연구? 눈에 보이는것은 이미 잘 연구되어있는데, incomplete perception to other perception
- 오늘의 토픽은, 눈에 보이지않는것을 보이게 만드는 학습법으로. 눈에 보이지 않는것을 보이는것으로 만들어주는 과정을 할것? Invisible-to-visible => Cross-modal translation => 1st step toward AGI 
- 소리는 어느곳에나 있다. Natural co-occurrence
- 예를들어...땅만보고 걸어가다가 빵빵 소리가 들린다? "코너 튀어나갈떄 조심해야겠다" 이런 소리를 통한 상상력으로 visual event를 만들어냄.
- 소리를 통해 얻을수있는점? 사람의 시야는 좁은데 이러한 시야를 넓혀주는 역할을 해준다.
- 그러면 소리를 어떻게 매핑시키는가? cell의 activity? activation?
- Humanlike한 machine perception을 만드는것이 목표인것이다.
- 연구의 목표는 Sound를 이미지로 변환하는것. sound안에 어떤 visual semantics가있는지 알고, 그것을 visualizing하는것
- 사람이 살아오면서 정보를 쌓듯, 오디오
- 이걸 어따쓰냐! 라는 생각할수있는데 유용한 어플리케이션 소개. => 청각장애인분들이 눈앞에 보이는걸 보고있지만 뒤쪽에서 오는 자동차에 대한 생각을 못할때 대신 알람울려줌. 예술쪽에서 소리를 통한 무언가 많은데 소리로 이미지화에 활용가능?. 예능프로에서 특수 음원 사운드효과같은것 많이 보았을텐데, 지금은 이 장면에 이 소리가 들어갔으면 좋겠다 라는 생각에 SOundDB가 굉장히 많다. 하나하나 들어보면서 찾는상황이다. 아이코닉화 되어있다면 이런것들이 정리되기 쉬운듯?
- sound에서 image로 전환하는 ...과정에 Challenges가 있다. 소리와 VIsualevent가 동시에 발생하지않는 문제도 있다. Challenge가 있지만 어떻게 시도? Direct end2end learning을 했었다. => 이렇게하면안됨.
- 모듈바이모듈하여 통합
- 현재 연구 진행에는 모델카테고리 제한이 없다? 50개까지는 가능하다?
- 이미지 encoder를 학습시킨다. 최대한 정보를 담도록. 이것을 freeze하고, BigGAN이나 ImageNet을 통해 학습시킨다. 확장해서 디퓨전모델도 가능하다?
- 이렇게 함으로써 visual embedding space를 어쩌구...
- Audio feature중에 visual과 연관된 애들만 학습이 가능하고 fix할수있다.
- 그리고는 바로 Image generator에 넣는다. 오디오 임베딩은 어쩌구..
- 사람은 엔진사운드를 듣고 잔디깎이 트랙터 구분 어렵지만 얘네는 해낼수있다.
- 강아지+물소리 합쳐서 넣어주면 강가에 있는 강아지 이미지가 나온다. 합성에 대해서도 좋다.
- 재밌는것은, 비디오를 많이 학습해주니 볼륨이 크면 가깝고 볼륨이 작으면 먼 이미지로 생성이 되더라
- 인식가능한 이미지를 생성해 내고있다. 모듈베이스? 모달리티 갭이 있다? 
- 희망적인 메세지는 무엇이냐. 다행히 simple linear projection만으로 다룰수있다? 덕분에 gap을 무시할수있다.
- 소리를 조절하여 잔잔, 격렬한 이미지를 조절가능
- 사진을 주지않고도 아바타 생성이 가능하도록..?
- 비슷한 접근? 최근 핫했던 multi modal language model이 있었다. 이렇게 귀 눈 붙이는게 연구중이던 연구와 비슷하다.
### Text-to-3D / 4D
- CLIP도 있고 Diffusion모델도 있다. Text-to-3D using 2D Diffusion 기존 pretrained 2D로 만든ㄷ다.
- Denoisiong Diffusion Models . 노이징부터 디노이징시키는것이 핵심. 원하는 방향의 이미지가 생성되도록 만들수있다.
- DreamFusion에서는 텍스트로부터 이미지를 생성하는... 모델을활용했는데. 생성모달쪽에서 Gradient ...???
- 처음엔 러프한 모델이 나오면 렌더링를 사용해 이미지화하고 이 이미지에 강제로 노이즈를 섞어버린다. 그 노이즈를 디노이징 (컨디션을 주어) 시킨다. 그러면 텍스트를 잘 따르는 이미지 방향을 제시? 그 괴리감을 이용해 뭐를 개선해서 3D부분을 점ㅁ점 업데이틀르 해준다.
- 4D 제네레이션으로 확장했다. animation이 가능한 아바타모델을 통해 vertex나 color를 dreamfusion과 같은 형태로 학습하여 이제 이미지를 통해 아바타+애니메이션까지 생성할수있다? 바로 3D디퓨전모델로 만들었다.
- 만든 결과(2022)물을 보면 한번도 고든램지 3d를 pretrain한적없는데도 이미지?text를 통해 3D와 애니메이션을 만들더라.
- Paint-it이라는 연구를 했다.
- 어떤 환경이냐에 따라 물리적ㅎ ㅕㅇ태 등 반영하는... 360도를 돌려봐도 좋은 퀄리티의 결과물이 나온다.
- 기존연구와 얼마나 다른지? 기존연구는 text-to-image diffusion model을 통해 이미지를 multi-view-rasterize를 하고 textured 3d mesh를 t   exture map을 통해 .... 근데 얻는결과가 그리 좋지않았다. 
- DC-PBR
- 달라진점은, 뒷부분은 직접 texture map을 어... 학습되지않은 CNN. 그리고 random noise로 해놓고 네트워크가 직접 최적화하도록?
- 이렇게 한 이유는 3D Capture가 나오는이유는 noisy한것도맞지만 표현자체가 live하게 업데이트가 되면 그 노이즈가...ㄹㄴㅇ한다는 특성이있었고...
- Paint-it과 CLIP-Actor를 결합해보니... 나름 괜찮은 퀄리티의 텍스쳐+애니메이션이 생성되더라
- 전체적인 경로는 이렇다. Sound/text -> Break down한뒤 분할정복하여 해결하자. -> text/image
- 마지막으로는 Visual Data? 눈에 보이지않은 아주작은 이미지를 ..? 움직이고있는지 아닌지 잘 모르는데 증폭하면 인식하기 쉬워진다. 더 인식할수있도록 증폭시켜서 인식하게 만드는연구...
- 교량에서 차가 지나갈때 교량이 지나가는것에 대한 안정성 검사, Health monitoring과같은 어플리케이션에 사용이 가능하다.
- 영화에서도 좀더 격렬한 액션신 만들때 사용가능하고 충격에 대한것을 좀 더 뚜렷히 볼수있고... 아지랑이를 증폭시켜 매연이 어느정도 나오는지 측정도 가능한.. 등 
- 정리 : 사운드카메라, Text-to-4D, 모션증폭 / 최근 멀티모달 llm을 통해 눈에 보이지않는 마음 사람의 마음을 이해하는 연구를 하고있다. LLM을 통해 사람의 의도를 이해하는 연구를 수행하고있다. 사람들이 영상보고있는데 왜웃는지 메커니즘을이해하는 연구
