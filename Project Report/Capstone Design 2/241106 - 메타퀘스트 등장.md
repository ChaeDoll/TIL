현재 상태를 브리핑해본다.
일단 Image Segmentation과 Object Detection 모두 직접 Unity 라이브러리를 활용하여 Ondevice로 활용할 예정이다. 두 모델 다 받고 (YOLO와 DeepLabv3) Sentis에서 활용한다.

그런데 문제도 있다.
직접 모델을 다운받아 돌리니까 성능때문인지 Delay가 무지막지하게 심하다.
아무래도 스마트폰의 성능의 한계가 있는것같다.

원래 메타 퀘스트도 좋은 퍼포먼스를 보이기 위해 사용하고자 하였으나, 
이미지 활용 (실시간 영상처리)이 막혀있음 (금지되어 있음)
또한 ARDK의 Object Detection같은 경우 스마트폰에서만 작동함. 이러는 문제가 있었다.
하지만 장점으로는 **스마트폰보다 좋은 GPU 성능이 있다는것이다.**

이 말은 즉, 어차피 Segmentation과 Object Detection 모두 직접 돌린다면, 더욱 성능이 좋은 Meta Quest로 돌려도 되지않을까 하는 의문이 드는것이다.
메타 퀘스트로는 실시간 영상 수집이 안되지않냐고??
근데! MediaPipe API? 그것을 활용하면 가능하지 않을까!?
화면에 물체가 보이는 문제가 있으면 어떻게 하냐고?

이것은 어때? 3프레임 중, 1프레임은 감지(이때는 다른 요소들 disable) 2프레임은 감지된 애들 표시 또 1프레임은 감지, 2프레임은 표시... 이런 형태로 
한번 감지하고 그걸 0.1초 보여주고, 다시 한 프레임으로 감지하고 그걸 0.1초동안 보여주고 반복!

좋은 아이디어가 될수도!

--- 
Meta Quest에서 우선 바깥에 움직이며 사용하기 위해서는 Boundaryless가 되어있어야한다.
메타 퀘스트에서는 앱 사용을 위해서는 경계를 설정해야하고, 밖으로 나가면 경계를 재설정하라는 경고창이 뜬다. 그런데 Boundaryless를 적용하면 경계가 사라진다!
https://developers.meta.com/horizon/documentation/unity/unity-boundaryless/
해당 페이지를 참고하여 Boundaryless를 구현하였고, SceneManager과 BoundayManager라는 스크립트를 통해 앱을 시작할 때 Boundary가 꺼지도록 설정하였다.
- [x] 경계 제거 성공

그 다음엔 MediaProjection API를 활용하여 1024x1024의 텍스처를 얻어내야한다.
얻어낸 텍스처를 활용하여 딥러닝의 입력데이터로 입력하면 성공적으로 이를 완수할 수 있다.
https://github.com/trev3d/QuestDisplayAccessDemo/blob/master/Assets/DisplayCapture/DisplayCaptureManager.cs
일단 MediaProjection 가이드를 보면서 따라했다.
그러다가 빌드하려니까 막혔다. 
```
Configuration `releaseRuntimeClasspath` contains AndroidX dependencies, but the `android.useAndroidX` property is not enabled, which may cause runtime issues. See the Console for details.
```
랜다. 그래서 프로젝트 > Library > Bee > Android > Prj > IL2CPP > Gradle > gradle.properties에 들어가서 
`android.useAndroidX = true , android.enableJetifier = true` 이거 두 줄 넣었다. 
자꾸 없어져가지고 custom Gradle Properties 설정 키고 거기 안에 넣으니까 되더라.

이번에는 또다른 문제가 발생했다.
```
A failure occurred while executing com.android.build.gradle.internal.tasks.CheckAarMetadataWorkAction See the Console for details.
```
랜다. 근데 정말 어이없는 이유로 에러 고쳤다. KeyStore Manager 추가하니까 되더라.
추가적으로 Android API버전을 34로 올려야한다. (Android 14.0 / API 34)

빌드에 성공했는데 아무일도 안 일어난다. 텍스트로 디버깅을 해봐도 아무일도 안일어난다.
보니까 자바파일도 가져와야하는듯 하다.

Demo를 참고하여 쓸데없는 요소들을 없애고 필요한 애들만 남겨보았다. 제발 빌드 돼라 ㅠㅠ
빌드가 자꾸 안되길래 찾아보니 Cache때문에 문제가 생길수도 있다고 한다.
C > Users > .gradle > caches를 모두 삭제하고, AppData > LocalLow > Unity 내부 파일 모두 삭제하였다.
그리고 시스템 환경변수도 건드렸다. 

**이것만 가능하면 진짜 큰거 가능한데...**
진짜로!
스마트폰보다 좋다! 할수있는데...

---
Demo가 잘 작동하는 것은 확인했다. (미러링은 불가능하지만 실제로 캡처는 가능했음. 왜냐면 MediaProjection 자체가 Mirroring과 동일한 방식으로 뽑아오는 API이기에 둘이 중첩돼서 안켜짐)

계속되는 빌드 오류에, 그냥 Demo를 Fork하여 가져오고 이를 변형하여 사용하는것이 더욱 빠르겠다고 생각함.

Demo에서 필요없는 QR Reader 기능들은 모두 없애고, Sentis 라이브러리를 설치한뒤 ONNX 모델을 Import하여 실행 가능한지 확인할 예정이다.

Sentis 2.1.0 버전이기에 새롭게 코드를 작성하고, 출력을 일단 받아본다. (잘 작동하는지 먼저 테스트)

```
        debugText.text = "Level 1";
        Texture2D screenTexture = displayCaptureManager.ScreenCaptureTexture;
        debugText.text = "Level 2";
        Graphics.Blit(screenTexture, targetRT);
        debugText.text = "Level 3";
        using var input = TextureConverter.ToTensor(targetRT, imageWidth, imageHeight, 3);
        debugText.text = "Level 4";
        worker.Schedule(input);
        debugText.text = "Level 5";
        var output = worker.PeekOutput() as Tensor<float>;
        debugText.text = "Level 6";
        var results = output.DownloadToArray();
        debugText.text = "Level 7";
        debugText.text = results.ToString();
```
총 7단계로 나눴는데, 1단계까지만 나온다. 일단 onNewFrame이 호출된다는것을 안것은 좋다.
그런데.. displayCaptureManager.ScreenCaptureTexture가 실패했다는것이다.
내가 DisplayCaptureManager를 그냥 Import하여 사용했는데, static으로 된 Instance이기에 
`DisplayCaptureManager displayManager = DisplayCaptureManager.Instance;` 같은 방식을 사용해서 Instance를 가져와줘야 했다.
으악 팅긴다. 젠장...
> 컨트롤러를 지팡이처럼 쓰는것도 가능하려나?? 멀면 삐..삐..삐.. 가까우면 삐삐삐삐삐삐삐ㅣ 하는 자동차마냥 RayCast로 레이저를 쏴서 거리 측정 또는 물체 파악

계속 팅긴다 계속 계속...
updateInterval 변수를 선언하여 0.3초에 한번씩 onNewFrame 이벤트가 실행되도록 하였다.
이제 팅기진 않는데, 왜 worker.Schedule(input)이 작동을 안하지???

inputTensor로 받은걸 shape찍어본다.
어?
GPUCompute에서 GPUPixel로 Backend를 변경하니까 잘 작동한다.

이제 Pannel을 소환하고, 거기에 box가 출력되도록 설정해보자...
빌드 진짜 오래걸린다 하하하...


뭔 아무리해도 안되냐...
...
힘들다...

결국 Sentis도 1.4.0 pre-3으로 변경했다. 모델도 yolov7-tiny로 다시 돌아왔다.
시행착오가 너무 많다. 힘들다...

왜이리 안될까
현재 진행상황.
```cs
using Anaglyph.DisplayCapture;
using System.Collections;
using System.Collections.Generic;
using TMPro;
using Unity.Sentis;
using Unity.VisualScripting;
using UnityEngine;
using UnityEngine.UI;
using static Unity.Sentis.Model;

public class ObjectDetectionManager : MonoBehaviour
{
    public TMP_Text debugText;
    public RawImage displayPannel;

    [Header("Init Setting")]
    [SerializeField] private ModelAsset modelAsset;
    [SerializeField] private TextAsset labelAssets;
    [SerializeField] private Sprite borderSprite; 
    [SerializeField] private Texture2D borderTexture;
    [SerializeField] private Font font;
    [SerializeField] private float updateInterval = 0.5f;

    private float lastUpdateTime = 0;
    private DisplayCaptureManager displayCaptureManager;
    private Model model;
    private IWorker engine;
    private string[] labels;
    private const int imageWidth = 640;
    private const int imageHeight = 640;
    private TensorFloat inputTensor;

    List<GameObject> boxPool = new List<GameObject>();

    public struct BoundingBox
    {
        public float x;
        public float y;
        public float width;
        public float height;
        public string label;
        public float confidence;
    }

    void Start()
    {
        labels = labelAssets.text.Split('\n');
        model = ModelLoader.Load(modelAsset);
        engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);
        displayCaptureManager = DisplayCaptureManager.Instance;
    }

    public void ObjectDetection(Texture2D inputTexture)
    {
        if (Time.time - lastUpdateTime < updateInterval) return;
        lastUpdateTime = Time.time;
        if (inputTexture == null)
        {
            return;
        }
        ClearAnnotations();

        inputTensor?.Dispose();
        inputTensor = TextureConverter.ToTensor(inputTexture, imageWidth, imageHeight, 3);

        engine.Execute(inputTensor);
        var output = engine.PeekOutput() as TensorFloat;
        float displayWidth = displayPannel.rectTransform.rect.width;
        float displayHeight = displayPannel.rectTransform.rect.height;
        float scaleX = displayWidth / imageWidth;
        float scaleY = displayHeight / imageHeight;

        int foundBoxes = output.shape[0];
            for (int n = 0; n < foundBoxes; n++)
            {
                var box = new BoundingBox
                {
                    x = (output[n, 1] + output[n, 3]) / 2 * scaleX, 
                    y = (output[n, 2] + output[n, 4]) / 2 * scaleY,
                    width = (output[n, 3] - output[n, 1]) * scaleX,
                    height = (output[n, 4] - output[n, 2]) * scaleY,
                    label = labels[(int)output[n, 5]],
                    confidence = Mathf.FloorToInt(output[n, 6] * 100 + 0.5f)
                };

                DrawBox(box, n);
            }
    }

    public void DrawBox(BoundingBox box, int id)
    {
        GameObject panel;
        if (id < boxPool.Count)
        {
            panel = boxPool[id];
            panel.SetActive(true);
        }
        else
        {
            panel = CreateNewBox(Color.yellow);
        }
        RectTransform rt = panel.GetComponent<RectTransform>();
        rt.sizeDelta = new Vector2(box.width, box.height);
        panel.transform.localPosition = new Vector3(box.x, -box.y);

        //Set label text
        var label = panel.GetComponentInChildren<Text>();
        label.text = box.label + " (" + box.confidence + "%)";
    }

    public GameObject CreateNewBox(Color color)
    {
        var panel = new GameObject("ObjectBox");
        panel.AddComponent<CanvasRenderer>();
        Image img = panel.AddComponent<Image>();
        img.color = color;
        img.sprite = borderSprite;
        img.type = Image.Type.Sliced;
        panel.transform.SetParent(displayPannel.transform, false);

        var text = new GameObject("ObjectLabel");
        text.AddComponent<CanvasRenderer>();
        text.transform.SetParent(panel.transform, false);
        Text txt = text.AddComponent<Text>();
        txt.font = font;
        txt.color = color;
        txt.fontSize = 40;
        txt.horizontalOverflow = HorizontalWrapMode.Overflow;

        RectTransform rt2 = text.GetComponent<RectTransform>();
        rt2.offsetMin = new Vector2(20, rt2.offsetMin.y);
        rt2.offsetMax = new Vector2(0, rt2.offsetMax.y);
        rt2.offsetMin = new Vector2(rt2.offsetMin.x, 0);
        rt2.offsetMax = new Vector2(rt2.offsetMax.x, 30);

        boxPool.Add(panel);
        return panel;
    }

    public void ClearAnnotations()
    {
        foreach (var box in boxPool)
        {
            box.SetActive(false);
        }
    }

    private void OnDisable()
    {
        engine?.Dispose();
    }
}

```

음 뭘해도 안되면 뭘 해야할까?
기분 꿀꿀하네.

다시 Yolov7-tiny Sample보면서 해보자.

어? 돌아간다. 어???
일단 Output 판떼기 머리의 Left Eye Anchor에 부착한다. (왜냐면 MediaProjectionAPI가 왼쪽 눈 기준이니까)
그리고 Backend.CommandBuffer 라는것이 있으니 실행해보자.안된다 Backend.GPUPixel이 정석인듯.

엥 CPU로 돌리는게 제일 나은데??? 왜지???
일단 돌아간다! 좀 이상해도 돌아는간다! 스마트폰보다 낫다! ㅋㅋㅋㅋㅋㅋ

