현재 상태를 브리핑해본다.
일단 Image Segmentation과 Object Detection 모두 직접 Unity 라이브러리를 활용하여 Ondevice로 활용할 예정이다. 두 모델 다 받고 (YOLO와 DeepLabv3) Sentis에서 활용한다.

그런데 문제도 있다.
직접 모델을 다운받아 돌리니까 성능때문인지 Delay가 무지막지하게 심하다.
아무래도 스마트폰의 성능의 한계가 있는것같다.

원래 메타 퀘스트도 좋은 퍼포먼스를 보이기 위해 사용하고자 하였으나, 
이미지 활용 (실시간 영상처리)이 막혀있음 (금지되어 있음)
또한 ARDK의 Object Detection같은 경우 스마트폰에서만 작동함. 이러는 문제가 있었다.
하지만 장점으로는 **스마트폰보다 좋은 GPU 성능이 있다는것이다.**

이 말은 즉, 어차피 Segmentation과 Object Detection 모두 직접 돌린다면, 더욱 성능이 좋은 Meta Quest로 돌려도 되지않을까 하는 의문이 드는것이다.
메타 퀘스트로는 실시간 영상 수집이 안되지않냐고??
근데! MediaPipe API? 그것을 활용하면 가능하지 않을까!?
화면에 물체가 보이는 문제가 있으면 어떻게 하냐고?

이것은 어때? 3프레임 중, 1프레임은 감지(이때는 다른 요소들 disable) 2프레임은 감지된 애들 표시 또 1프레임은 감지, 2프레임은 표시... 이런 형태로 
한번 감지하고 그걸 0.1초 보여주고, 다시 한 프레임으로 감지하고 그걸 0.1초동안 보여주고 반복!

좋은 아이디어가 될수도!

--- 
