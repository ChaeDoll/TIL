현재 상태를 브리핑해본다.
일단 Image Segmentation과 Object Detection 모두 직접 Unity 라이브러리를 활용하여 Ondevice로 활용할 예정이다. 두 모델 다 받고 (YOLO와 DeepLabv3) Sentis에서 활용한다.

그런데 문제도 있다.
직접 모델을 다운받아 돌리니까 성능때문인지 Delay가 무지막지하게 심하다.
아무래도 스마트폰의 성능의 한계가 있는것같다.

원래 메타 퀘스트도 좋은 퍼포먼스를 보이기 위해 사용하고자 하였으나, 
이미지 활용 (실시간 영상처리)이 막혀있음 (금지되어 있음)
또한 ARDK의 Object Detection같은 경우 스마트폰에서만 작동함. 이러는 문제가 있었다.
하지만 장점으로는 **스마트폰보다 좋은 GPU 성능이 있다는것이다.**

이 말은 즉, 어차피 Segmentation과 Object Detection 모두 직접 돌린다면, 더욱 성능이 좋은 Meta Quest로 돌려도 되지않을까 하는 의문이 드는것이다.
메타 퀘스트로는 실시간 영상 수집이 안되지않냐고??
근데! MediaPipe API? 그것을 활용하면 가능하지 않을까!?
화면에 물체가 보이는 문제가 있으면 어떻게 하냐고?

이것은 어때? 3프레임 중, 1프레임은 감지(이때는 다른 요소들 disable) 2프레임은 감지된 애들 표시 또 1프레임은 감지, 2프레임은 표시... 이런 형태로 
한번 감지하고 그걸 0.1초 보여주고, 다시 한 프레임으로 감지하고 그걸 0.1초동안 보여주고 반복!

좋은 아이디어가 될수도!

--- 
Meta Quest에서 우선 바깥에 움직이며 사용하기 위해서는 Boundaryless가 되어있어야한다.
메타 퀘스트에서는 앱 사용을 위해서는 경계를 설정해야하고, 밖으로 나가면 경계를 재설정하라는 경고창이 뜬다. 그런데 Boundaryless를 적용하면 경계가 사라진다!
https://developers.meta.com/horizon/documentation/unity/unity-boundaryless/
해당 페이지를 참고하여 Boundaryless를 구현하였고, SceneManager과 BoundayManager라는 스크립트를 통해 앱을 시작할 때 Boundary가 꺼지도록 설정하였다.
- [x] 경계 제거 성공

그 다음엔 MediaProjection API를 활용하여 1024x1024의 텍스처를 얻어내야한다.
얻어낸 텍스처를 활용하여 딥러닝의 입력데이터로 입력하면 성공적으로 이를 완수할 수 있다.
