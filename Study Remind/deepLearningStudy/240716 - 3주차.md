### 5-1챕터 (147페이지부터 170페이지까지)
### 5-2챕터 (170페이지부터 187페이지까지)

## 오차역전파법
오차역전파법 이해 방법에 두가지 존재.
1) 수식을 통한 것
2) 계산 그래프를 통한 것

계산그래프는 복수의 노드, 에지로 표현됨. 사과를 2개 산 뒤, 소비세 10%를 적용한 가격을 지불한다 가정한다. 사과의 가격은 100원, 사과의 개수는 2개, 소비세는 1.1배와 같이 각 핵심 요소를 뽑아 변수로 취급하고, 노드에 덧셈노드 혹은 곱셈노드를 활용하여 그래프로 표현한 뒤 최종 가격을 얻어낼 수 있다.

이러한 특징은 '국소적 계산'의 전파를 이용한다. 국소적 계산이란 전체 일에는 상관없이 노드가 가진 정보로 정보를 출력하는것을 의미한다.
소비세가 1.1배라고 하였는데, 왼쪽에 어떠한 수가 오던 소비세의 곱셈노드 입장에서는 그저 물건의 총 가격과 소비세 1.1을 곱해주기만 하면 된다.

이렇게 연달아 계산이 왼쪽에서 오른쪽으로 진행되는 것을 **순전파**라고 하는데, 오른쪽에서 왼쪽으로, 반대로 향하는 것이 바로 **역전파**이다. 미분 계산시에 역전파를 활용한다.

##### 연쇄법칙
연쇄법칙으로 역전파의 계산을 활용할 수 있다. 연쇄법칙은 z=t^2, t=x+y와 같이 두 수식이 연결되어 있을 때, dz/dx를 구하고 싶다면 dz/dt와 dt/dx를 곱하여 구할 수 있다는 공식이다.
dz/dx = dz/dt * dt/dx 나누어 떨어지면 = dz/dx 이다.
이러한 연쇄법칙을 적용하면 z=t^2, t=x+y 에서 dz/dx = 2t * 1 = 2(x+y)이다.
이러한 연쇄법칙 또한 역전파를 통한 계산그래프로 나타낼 수 있다.

##### 역전파 공식
- **역전파에서 덧셈노드는 입력 값을 그대로 흘려보낸다.** 입력이 dL/dz라면 출력도 dL/dz * 1이다. 덧셈 노드는 즉, 1을 곱하기만 하고 그대로 전달한다. \
- **역전파에서 곱셈노드는 순전파에서 입력된 값들을 바꾸어 곱한다.** 만약 순전파에서 x와 y가 곱셈에 입력되어 z가 되었다면, x로 갈때는 y를 곱해주고, y로 갈때는 x를 곱해준다. 
  ex) 10 x 5 = 50 이라는 계산이 있고, 역전파 상류에서 1.3의 값이 온다고 가정. 그렇다면 10이 온 방향으로는 1.3x5 = 6.5를 흘려보내고, 5가 온 방향으로는 1.3x10 = 13으로 흘려보낸다.
  이처럼 곱셈 역전파에서는 순전파에서의 입력신호 값을 사용하기 위해 변수에 저장해두어야 한다.
- 사과와 귤 문제를 해결하면, 최종 가격에서 1로 시작, 소비세 역전파는 650, 그 위는 1.1이다. 그다음엔 덧셈노드를 만났기에 그대로 흘려보내어 사과와 귤 모두 1.1이다. 그리고 사과부터 곱셈노드를 만나 사과의 개수는 출력신호가 110이고, 사과의 가격은 2.2이다. 귤은 귤의 개수가 1.1 * 150 = 165이고, 귤의 가격은 1.1 * 3 = 3.3이다.

덧셈노드, 곱셈노드에서의 역전파 공식을 파이썬을 통해 구현한다. 곱셈노드는 MulLayer, 덧셈노드는 AddLayer로 구분할 수 있다. 클래스 내부에 순전파 함수, 역전파 함수를 작성할 수 있다. 
덧셈노드는 AddLayer로하고, 초기화 없이 forward는 x와 y를 받아 더한뒤 돌려주고, backward는 1을 곱하여 돌려준다.

이처럼 계산그래프로 표현 가능한 문제를 코드로도 순전파, 역전파 표현할 수 있게 되었다.
##### 활성화 함수 계층 구현
계산 그래프를 신경망에 어떻게 적용할까. 

**ReLU** 부터 우선 본다. ReLU는 x입력에 있어서 0이하는 0, 0보다 큰 경우엔 x를 그대로 출력하는 활성화 함수이다. 이를 미분하면 입력값이 주어질 때 0이하일 때는 0, 0보다 크면 1이다.
따라서 역전파에서 입력이 주어질 때, 출력하고자 하는 x가 0보다 크면 입력(dL/dy)을 그대로 전달하고 (곱하기 1이니까), x이하면 0을 흘려보낸다.
파이썬에서는 mask를 활용하여 불린인덱싱으로 x이하인 값들을 전부 0으로 만들었다.

**Sigmoid** 계층으로 넘어간다. y=1/1+exp(-x) 이다. 구현을 위해서 exp노드, /노드가 필요하다. (-x를 위해 곱셈노드(-1 * x)가 들어가고 1+exp를 위해 덧셈노드가 들어간다.)
**'/' 노드**는 y=1/x를 해주는 역할. x^(-1)이고, 미분하면 -1 * x^(-2)로 표현할 수 있다.
dy/dx = -1/x^2 인데, y=1/x인것을 통해 **dy/dx = -y^2**으로도 표현할 수 있다.
즉 '/' 노드는 순전파에서의 출력을(역전파에서 입력) 제곱하고 마이너스 곱하여 전달하면 된다.
1+exp(-x)를 /노드에 넣어 y로 만드는 순전파 과정이 있다면, y부터 시작하는 역전파 dL/dy를 넣는다면 -dL/dy * y^2 으로 표현이 가능하다.
**exp 노드**는 y=exp(x)를 수행하는 노드이고, 미분식 또한 그대로 dy/dx = exp(x)이다. 
-x를 exp노드에 순전파를 거쳐 exp(-x)로 만들었다면, 역전파로 exp 노드를 거치는 것 역시 exp(-x)를 곱해주면 된다.
입력이 -dL/dy * y^2 이면, -dL/dy * y^2 * exp(-x) 이다. 이후 곱셈노드를 만났기에 순전파 때의 입력을 바꿔서 넣어 -1을 곱해주면 최종적인 값은 dL/dy * y^2 * exp(-x) 이다.
이제 이 전체 과정을 Sigmoid라고 칭하면, dL/dy가 입력될 때, dL/dy * y^2 * exp(-x)를 해주는 식으로 코드를 작성하면 Sigmoid의 역전파 수식이 완성된다. 또한 이 식 또한 정리하여 작성하면 169페이지 과정을 거쳐 dL/dy * y(1-y)가 완성된다. 순전파의 출력값인 y값 만으로 계산할 수 있게 된 것이다.

----------
##### 추가적인 활성화함수 Affine/Softmax 계층 구현하기
신경망 순전파에서는 가중치 신호의 총합을 계산하기 위해 행렬곱을 사용했다. (w1x1 + w2x2 + .. + wnxn 보다 행렬곱 W * X + b 를 사용) Y = np.dot(X, W) + B 처럼 계산함. 그리고 이 Y를 활성화 함수로 변환 후 다음층으로 넘기는게 신경망 순전파 흐름.
#### Affine 계층
신경망 순전파에서 수행하는 **행렬곱을 Affine Transformation (어파인 변환)** 이라고 한다. 신경망에서 가중치와 노드값, 편향을 통해 결과값을 얻는 과정을 계산그래프로 표현한다.
행렬 변수 X와, W가 dot(행렬곱)노드에 입력되고, 이렇게 얻은  X * W와 편향 B를 덧셈노드에 입력하여 Y를 얻어낸다.
**주의 해야할 점은** 지금까지는 스칼라값으로 계산했다면, 이제는 행렬이 흐른다는 사실을 기억해야 한다. 행렬의 역전파도 행렬 원소마다 전개하여 계산한다면 스칼라값 사용한 계산그래프와 같은 순서로 이루어진다.

실제로 dL/dX = dL/dY * W^T 이고, dL/dW = X^T * dL/dY가 나올 것이다. (덧셈, 곱셈노드의 역전파 공식을 활용)
w^T에서 T는 전치행렬 (i, j)를 (j, i)로 위치를 바꿈. (2,3) shape였다면 (3,2)가 된다.
결과로 따지면 아래와 같은 흐름이 된다.
dL/dY (3,) => dL/dY(3,) => dL/dY(3,) * W^T(3, 2) 가 dL/dX이다.
dL/dY (3,) => dL/dY(3,) => X^T(2, 1) * dL/dY(1,3)
**행렬곱을 위해서는 곱에 대응하는 차원 원소수를 맞춰야하는 것을 기억해야함.**
또한 **X와 dL/dX는 형상이 동일해야한다.** 이 두가지가 매우 중요하여 이를 기준으로 식을 위치시켜야 한다.
X는 (2,)이다. 이를 만들기 위해서 W(2,3)와 dL/dY(3,)를 조합하여 dL/dX (2,)를 만들고자 한다면, 자연스럽게 차원수를 맞추기 위해 W^T하여 (3,2)로 맞춰야한다.
왜냐면 dL/dY(3,)는 1차원의 3열이기에 (1, 3)이라고 생각하면 된다. 그러면 dL/dY * W^T 순서로 곱하면 (1,2)이 완성될 것이고, dL/dX와 동일한 것을 볼 수 있다.

dL/dW 역시 (2, 3)이 되어야하고, dL/dY(1,3)과 X(1, 2)를 활용하여 (2,3)을 만들기 위해서는 X^T(2,1)로 만든뒤 행렬곱하여 X^T * dL/dY (2 x 1 x 3) => (2,3)으로 만들 수 있다.
**조립은 우리가 생각해서 하면 된다**
##### 배치용 Affine 계층
- 지금까진 입력데이터가 X 하나일 때만 고려했다. 데이터 N개를 묶어서 배치로 순전파를 하는경우에 어떻게 사용할까
- dL/dX (N, 2)가 된다. 가중치 W(2, 3)와 B(3,)을 통해 Y(N, 3)을 만든다. 기존과 다른것은 X가 (N,2)가 된 것 뿐. 역전파도 행렬 형상만 조심하면 된다.
- 편향 더할 때도 제대로 매핑될 수 있게 주의해야한다.
#### SoftMax와 손실함수 계층
입력이미지가 Affine 계층과 ReLU계층을 거치며 변환되고, 마지막에 SoftMax를 통해 정규화되어 확률로 나타나게 된다. 이제 Softmax 함수를 손실함수(교차 엔트로피 오차)를 포함하여 구현한다.
Softmax는 y=1/1+exp(-x) 를 순전파로 하며 역전파로 dL/dY * y(1+y)