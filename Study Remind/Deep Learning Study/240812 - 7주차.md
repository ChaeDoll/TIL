## 7장. 합성곱 신경망(CNN) - 2
#### 목차
1) 합성곱/풀링 계층 구현하기
2) CNN 구현하기
3) CNN 시각화하기
4) 대표적인 CNN
#### 1) 합성곱/풀링 계층 구현하기
이전에 했던 forward, backward 메소드를 활용해서 모듈화 시킨 것처럼 만든다.
CNN에서 계층 사이 흐르는 데이터는 4차원 데이터이다.
형상이 (10, 1, 28, 28)이라면 
```
x[0] 으로 첫번째 데이터에, x[1]으로 두번째 데이터에 // x[0]의 형상은 (1, 28, 28)
x[0][0] 으로 첫번째 채널에 접근이 가능하다. // 형상은 (28, 28)
```
합성곱연산 구현을 위해 for문이 겹쳐있을 수 있지만, 귀찮음과 성능문제로 인해 for문대신 **im2col**이라는 함수로 구현이 가능하다.

im2col이란, 입력데이터를 필터링하기(가중치계산하기) 좋게 전개하는 함수이다. (펼치는 함수)
4차원 데이터가 있다면 이를 2차원으로 변환한다. 즉, 필터에 적용되는 영역들을 앞에서부터 순서대로 1줄로 펼치는 것이다. 
물론, 메모리를 더 많이 소비한다는 단점 있지만, 컴퓨터는 큰 행렬 묶어 계산에 탁월하다. 그래서 선형대수 라이브러리로 효율을 높일 수 있다.
```
# input_data는 (데이터수, 채널수, 높이, 너비)의 4차원 형상의 데이터
result = im2col(input_data, filter_h, filter_w, stride=1, pad=0) 이라는 인터페이스.
```

**Convolution 계층**
우선 forward에서 FN, C, FH, FW와 N, C, H, W를 가지고 OH, OW를 구한다. (OH=(H+2P-FH)/S + 1) 이렇게 구한 뒤, im2col로 x, FH, FW, Stride, Pad값을 넣어 구하고, 가중치 W를 펼친다. (필터를 전재한다.) 그런 뒤, **im2col로 나온 result와 전개한필터를 행렬곱하고 편향b를 더하여 마무리한다.** 
최종 결과는 OH, OW를 사용하여 reshape하여 형상을 맞추고, transpose하여 N,H,W,C 라는 결과를 (0, 3, 1, 2) 인덱스 축으로 변경하여 (N, C, H, W) 순서로 맞추어준다. 이것이 forward 계층 구현이다.

**역전파**를 구할 때는 im2col의 역전파 처리만 주의하면 되는데, 책에서 제공하는 col2im이라는 함수로 구현이 가능하다. 이것 외에는 Affine 계층과 똑같다.

**Polling 계층**
풀링 계층도 im2col로 입력 데이터를 전개한다. 풀링 계층에서 전개한 데이터는 한 줄이 하나의 Window일 것이다. 그러면 한 줄마다 (행 마다) 최댓값을 구하기만 하면 된다. 그렇게 세로로 한 줄 쭉 만들어진것을 reshape하여 출력 데이터 형상에 맞추면 된다.
#### 2) CNN 구현하기
합성곱과 풀링계층을 구현했다면 이제 조합하기만 하면된다. 
CNN의 형태인 Conv-ReLU-Pooling / Affine-ReLU / Affine-Softmax 를 만든다.

먼저 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼낸다. 그리고 합성곱 계층의 출력크기를 계산한다.
이후 가중치 매개변수를 초기화하여 각각 저장한다. W1 b1, W2 b2, W3 b3 (random)
그다음엔 순서있는 딕셔너리에 (OrderedDict) 있는 layer에 계층들을 차례대로 넣는다. 여기서는
Conv1->ReLU1->Pool1->Affine2->ReLU2->Affine3->SoftmaxWithLoss 가 된다.
이전에 만들어둔 class들을 활용한다.
먼저 계층을 차례대로 지나가면 layer.forward()를 실행시키고, 이렇게 전해지는 x를 토대로 다시 다음층에 넣고... 그러다가 최종적으로 마지막Layer인 SoftmaxWithLoss까지 전해진다.
**이렇게 Loss값이 구해지며 순전파는 끝이 난다.**

이제 오차역전파법으로 기울기를 구할텐데, 먼저 loss값을 순전파를 통해 구했다면,
역전파를 1부터 시작해서 SoftmaxWithLoss의 backward를 통해 dout을 구한다.
이후 Layers들을 reverse하여 모든 계층들을 backward처리하여 dout값을 구한다.
이렇게 모든 층을 backward했다면, 각각 계층에 dW와 db값이 저장되어있을 것이다.
```
grads={}
grads=['W1'] = self.layers['Conv1'].dW
grads=['b1'] = self.layers['Conv1'].db
grads=['W2'] = self.layers['Affine2'].dW
grads=['b2'] = self.layers['Affine2'].db
grads=['W3'] = self.layers['Affine3'].dW
grads=['b3'] = self.layers['Affine3'].db
return grads
```
최종적으로 grads에 값들을 저장한다.
이렇게 구현한 CNN을 통해 MNIST데이터셋으로 학습하면, 훈련데이터 정확도는 99.82%이고 시험 데이터의 정확도는 98.96%가 된다. 엄청높다.
#### 3) CNN 시각화하기
(30, 1, 5, 5)와 같이 필터30개, 채널1개, 5x5크기라면 1채널의 회색이미지로 표현이 가능하다.
학습전 필터는 무작위라 규칙성이 없는데, 학습을 마친 필터는 규칙성이 있는 이미지로 보인다.
이렇게 규칙성있는 필터가 보고있는것은 **에지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역)이다.** 
즉, 합성곱계층의 필터를 통해서 에지나 블롭 등 원시적 정보 추출이 가능하다.

그러면 겹겹이 쌓인 계층에서는 어떤 정보가 추출되는지?
딥러닝 시각화 연구에 따르면 **계층이깊어질수록 추출되는 정보는 더욱 추상화 된다.** 
층이 깊어질수록 뉴런이 반응하는 대상이 단순모양에서 '고급'정보로 변화해간다. 즉, 사물의 의미를 담도록, 이해하도록 변화해간다.
#### 4) 대표적인 CNN
가장 중요한 네트워크 두개를 소개해보면, 하나는 CNN 원조인 LeNet이고, 다른 하나는 딥러닝이 주목받도록 이끈 AlexNet이다. (LeNet과 AlexNet)
**LeNet**
LeNet은 손글씨 숫자 인식 네트워크이다. 합성곱계층과 풀링계층을 반복하며 단순히 원소줄이기를 반복. 그러다가 마지막으로 완전연결계층(FullyConnected)을 거쳐서 결과를 출력한다. Affine계층을 의미하는 것이겠지.
LeNet과 현재의 CNN에는 차이가 있다.  활성화함수. **LeNet은 Sigmoid 함수를 사용**한다. (현재는 주로 ReLU사용). 또한 **LeNet은 서브샘플링으로 중간데이터 크기를 줄이지만**, 현재는 최대풀링을 사용하는것이 주류이다. 하지만 의의가 있다면, 20년 전 