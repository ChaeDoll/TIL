## 7장. 합성곱 신경망(CNN) - 2
#### 목차
1) 합성곱/풀링 계층 구현하기
2) CNN 구현하기
3) CNN 시각화하기
4) 대표적인 CNN
#### 1) 합성곱/풀링 계층 구현하기
이전에 했던 forward, backward 메소드를 활용해서 모듈화 시킨 것처럼 만든다.
CNN에서 계층 사이 흐르는 데이터는 4차원 데이터이다.
형상이 (10, 1, 28, 28)이라면 
```
x[0] 으로 첫번째 데이터에, x[1]으로 두번째 데이터에 // x[0]의 형상은 (1, 28, 28)
x[0][0] 으로 첫번째 채널에 접근이 가능하다. // 형상은 (28, 28)
```
합성곱연산 구현을 위해 for문이 겹쳐있을 수 있지만, 귀찮음과 성능문제로 인해 for문대신 **im2col**이라는 함수로 구현이 가능하다.

im2col이란, 입력데이터를 필터링하기(가중치계산하기) 좋게 전개하는 함수이다. (펼치는 함수)
4차원 데이터가 있다면 이를 2차원으로 변환한다. 즉, 필터에 적용되는 영역들을 앞에서부터 순서대로 1줄로 펼치는 것이다. 
물론, 메모리를 더 많이 소비한다는 단점 있지만, 컴퓨터는 큰 행렬 묶어 계산에 탁월하다. 그래서 선형대수 라이브러리로 효율을 높일 수 있다.
```
# input_data는 (데이터수, 채널수, 높이, 너비)의 4차원 형상의 데이터
result = im2col(input_data, filter_h, filter_w, stride=1, pad=0) 이라는 인터페이스.
```

**Convolution 계층**
우선 forward에서 FN, C, FH, FW와 N, C, H, W를 가지고 OH, OW를 구한다. (OH=(H+2P-FH)/S + 1) 이렇게 구한 뒤, im2col로 x, FH, FW, Stride, Pad값을 넣어 구하고, 가중치 W를 펼친다. (필터를 전재한다.) 그런 뒤, **im2col로 나온 result와 전개한필터를 행렬곱하고 편향b를 더하여 마무리한다.** 
최종 결과는 OH, OW를 사용하여 reshape하여 형상을 맞추고, transpose하여 N,H,W,C 라는 결과를 (0, 3, 1, 2) 인덱스 축으로 변경하여 (N, C, H, W) 순서로 맞추어준다. 이것이 forward 계층 구현이다.

**역전파**를 구할 때는 im2col의 역전파 처리만 주의하면 되는데, 책에서 제공하는 col2im이라는 함수로 구현이 가능하다. 이것 외에는 Affine 계층과 똑같다.

**Polling 계층**
풀링 계층도 im2col로 입력 데이터를 전개한다. 풀링 계층에서 전개한 데이터는 한 줄이 하나의 Window일 것이다. 그러면 한 줄마다 (행 마다) 최댓값을 구하기만 하면 된다. 그렇게 세로로 한 줄 쭉 만들어진것을 reshape하여 출력 데이터 형상에 맞추면 된다.
#### 2) CNN 구현하기
합성곱과 풀링계층을 구현했다면 이제 조합하기만 하면된다. 
CNN의 형태인 Conv-ReLU-Pooling / Affine-ReLU / Affine-Softmax 를 만든다.

먼저 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼낸다. 그리고 합성곱 계층의 출력크기를 계산한다.
이후 가중치 매개변수를 초기화하여 각각 저장한다. W1 b1, W2 b2, W3 b3 (random)
그다음엔 순서있는 딕셔너리에 (OrderedDict) 있는 layer에 계층들을 차례대로 넣는다. 여기서는
Conv1->ReLU1->Pool1->Affine2->ReLU2->Affine3->SoftmaxWithLoss 가 된다.
이전에 만들어둔 class들을 활용한다.
먼저 계층을 차례대로 지나가면 layer.forward()를 실행시키고, 이렇게 전해지는 x를 토대로 다시 다음층에 넣고... 그러다가 최종적으로 마지막Layer인 SoftmaxWithLoss까지 전해진다.
**이렇게 Loss값이 구해지며 순전파는 끝이 난다.**

이제 오차역전파법으로 기울기를 구할텐데, 먼저 loss값을 순전파를 통해 구했다면,
역전파를 1부터 시작해서 SoftmaxWithLoss의 backward를 통해 dout을 구한다.
이후 Layers들을 reverse하여 모든 계층들을 backward처리하여 dout값을 구한다.
이렇게 모든 층을 backward했다면, 각각 계층에 dW와 db값이 저장되어있을 것이다.
