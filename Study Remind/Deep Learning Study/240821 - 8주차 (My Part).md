## 8장 - 딥러닝

#### 더 깊은 신경망
CNN과 같은 신경망을 층을 깊게 한 '심층 신경망'을 우리는 딥러닝이라고 부른다.
층이 깊어지며 채널 수가 더욱 늘어나는 특징을 가진 심층 CNN을 봐본다면,
**(이게 혹시 VGG 신경망 생김새인가??)** 3x3의 작은 필터를 사용한 합성곱계층으로 채널수는 늘어나고 풀링 계층을 통해 공간크기는 점차 줄여진다. 그리고 마지막 Fully-Connected 계층에서는 드롭아웃을 적용한다. (드롭아웃이 뭐더라 간단하게 소개)
가중치 초깃값으로는 He 초깃값을 사용하고 **(값 알려주기)** 가중치 매개변수 갱신은 Adam을 이용한다. **(매개변수 갱신 그래프 그림 예시 보여주기)**

VGG라는 신경망의 특징이 결국 지금까지 배웠던 신경망 기술을 잔뜩 녹인 신경망이다. 이것으로 MNIST를 학습하면 99.38% 정도가 된다. **(아주 훌륭한 성능)**
잘못 인식된 경우를 뽑아내서 봐보면 사람도 구분하기 어렵다. **(사진 보여주며)** 즉, 판별을 잘하는, 성능이 좋은 예시라고 할 수 있다.
#### 정확도를 더욱 높이기
발표된 기법들의 정확도 순위를 보면, Neural Netwroks, Deep, Convolutional 이라는 키워드가 돋보인다. 상위권 대부분 CNN을 기초로 한 기법들이다.
2016년 10월 MNIST 데이터셋 정확도 1위는 99.79%인 기법인데 이것도 CNN을 기초로했다.
> 그러면 2024년 8월 현재는 어떨까? What is the class of the image 사이트에서 가져와서 보여주기

이런 상위 랭킹의 기법들을 참고하면 정확도를 올리는 기법이나 힌트를 얻을 수 있다.
그 중 **데이터확장 (Data Augmentation)** 기법이 있다. 입력 이미지 (훈련 이미지)를 알고리즘을 동원해서 인위적 확장을 수행하는 기법이다.
데이터를 회전시키거나, 이동시키거나.. 그 외에도 일부를 잘라내거나 좌우를 뒤집는 방법도 있다. 
> 이미지들 예시 보여주기.. 또다른 데이터 확장 기법은 무엇이 있을까???
> 혹은 이미지 말고도 자연어처리는 데이터확장을 어떻게 진행할까? 갑자기 궁금함! (멀티모달 등)

층을 그렇다면 왜 깊게하는 것이 중요할까?
1) **신경변수의 매개변수 수가 줄어든다.**  (깊은 신경망은 깊지 않은경우보다 적은 매개변수로 같은 표현력을 만들 수 있음) - 예를들어 5x5 합성곱 연산 1회대신, 3x3 합성곱 연산 2회로 바꾸어본다면, 똑같은 1x1의 결과를 만들면서 (즉, 대체가능) 5x5는 매개변수가 25개였지만, 3x3은 2번해서 3x3x2 = 18이라는 더 적은 매개변수를 가질수있다. 여기선 단순히 1개의 합성곱과 2개의 합성곱을 비교했는데, 이 개수 차이는 층 깊어질수록 커진다.
   
   역으로 생각해서 또 3x3 합성곱 연산을 3회 반복하면 매개변수는 27개가 되지만, 이만한 탐색을 1회로 마치려면 7x7, 49개 매개변수가 필요하다.
   (매개변수를 줄이면 수용영역을 소화할 수 있다. 신경망의 비선형 힘을 가한다. 이가 겹쳐지며 더욱 복잡한것도 표현가능하게 된다.)
2) **학습의 효율성을 좋게한다.** 층을 깊게하면 학습데이터 양을 줄기에 더 빠르게 학습수행이 가능하다. 앞에서 합성곱계층은 에지같은 단순패턴에 뉴런이 반응하고 층 깊어지며 텍스쳐와 사물의 일부 같은 더 복잡한것에 반응한다 하였다. 
   얕은 신경망에서 만약 개를 본다면 해결을 위해 **한번에** 특징의 대부분을 이해해야 한다. 견종, 찍은 각도 등 다양한 변화에 대한 학습데이터가 필요하고, 이로인해 학습시간도 오래걸린다.
   하지만 신경망을 깊게하면 마치 역할을 나누듯 학습해야할 문제를 계층적 분해할 수 있다.
   첫 층이 에지에 전념하기에 더욱 적은 학습데이터로 효율적 학습을 진행하고, 층이 깊어질수록 이렇게 얻은 정보들을 다시 다음 계층에 활용하며 점차 고도의 패턴학습을 할 수 있게 된다.
	> 층마다 역할로 첫번째는 에지 점차 깊어질수록... 복잡패턴을 해결! 이라는 그림또는 이미지
	
#### 딥러닝 초기 역사
딥러닝은 2012년 ILSVRC(이미지인식기술챌린지) 대회에서부터 주목받기 시작했다. 이 때 딥러닝이 주목받은 전환점이 되었다.
이미지넷은 100만장 넘는 이미지 데이터셋이고, ILSVRC는 이 거대 데이터셋을 활용해 자웅을 겨루는 대회이다. 
몇가지 시험 항목 중, **분류**가 그 중 하나이다. 제대로 1000개의 클래스를 분류하는지 겨룬다. 
성적추이를 보면 2012년 이후부터는 항상 딥러닝 방식이 선두를 맡고있다고한다. 실제로 2012년에 AlexNet이 압도적으로 오류율을 크게낮추며 성공한 이후, 딥러닝을 활용한 기법이 꾸준히 정확도를 개선해왔다.
2015년엔 150층 넘는 심층 신경망 ResNet이 top5 오류를 3.5%까지 줄였다. (인간의 인식을 넘어선다한다.)

뛰어난 성적 거두고있는 딥러닝 중에서도 VGG, GoogLeNet, ResNet이 유명하고 다양한 딥러닝 분야에서 활용된다. 
> 이젠 또 뭐가 있을까???

#### VGG
VGG는 3x3라는 작은 필터를 사용한 합성곱계층을 연속적으로 거치며 풀링 계층까지 넣으며 크기를 절반으로 둘이는 크기를 반복하고 최종적으로 완전연결 계층을 통과해 결과를 출력한다.
구성이 간단하여 응용하게 좋다. (이미지 보여주기) 
> 어느 분야에 주로 활용되는지 (이미지 중) 예시 보여주기. 혹시 얘가 빠르다면?그럼 실시간 감지에 활용되려나?
#### GoogLeNet
이미지와 함꼐..
구성이 매우 복잡해보이는데, 세로 깊이뿐만 아닌 가로깊이도 깊은 특징을 가진다. 1x1, 3x3, 5x5의 합성곱과 3x3의 최대풀링을 모두 계산 후 필터결합하여 활용한다. 이러한 가로깊이가 있는 구조를 하나의 구성요소로 활용한다.
GoogLeNet에서는 1x1필터도 사용하는데, 이는 채널쪽으로 크기를 줄이는 것으로 매개변수 제거와 고속처리에 기여한다. 
#### ResNet
Residual Network는 마이크로소프트의 팀이 개발한 네트워크이고, 지금까지보다 층을 더 깊게 할 수 있는 특별한'장치'에 있다.
지금까지 층이 깊을수록 성능이 좋아진다했지만, 딥러닝학습에서 지나치게 층이 깊을때 오히려 성능이 떨어지는 경우도 많은 것을 알 수 있다. (예를 들면 가중치 소실 문제)
이런 문제를 ResNet은 **스킵연결(skip connection)** 을 통해 해결한다. 
이 방법은 가중치계산후ReLU=>가중치계산후ReLU=> 라는 두개의 합성곱계층 과정을 건너뛰어 그대로 이전 x를 이후 x로 전달하는 스킵연산을 추가하는 것이다.
이렇게 되면 2개의 합성곱계층을 거친 출력 + 거치기전의 입력x 가 된다. 이 x덕에 신호감쇠, 즉 가중치 소실문제를 줄여주는 것이다.
ResNet은 VGG신경망을 기반으로 스킵연결을 도입하여 층을 깊게한 것이다.
실험 결과 150층이라는 막대한 층을 거쳐도 정확도가 오르는것을 볼 수 있었다.
**imageNet은 실제 제품에 활용해도 좋다. 이를 전이학습이라 하는데, 이미 학습된 가중치를 다른 신경망에 복사하고 그 상태로 재학습 수행한다. VGG와 구성이 같은 신경망 구성하고 가중치를 초깃값으로 한뒤 새로운 데이터셋을 대상으로 파인튜닝하여 사용할 수 있는것. 보유 데이터셋 적을때 유용하다.**
#### 더 빠른 딥러닝(딥러닝 고속화)
기본에는 CPU가 계산을 답당했는데 최근 딥러닝 프레임워크 대부분은 GPU를 이용해 대량연산을 고속처리 한다. 
AlexNet을 가져와서 어느층에서 가장 시간이 오래걸리나 보면, 대부분의 시간을 합성곱계층에서 보낸다. 처리 시간을 보면 CPU에서는 전체 89%, GPU에서는 전체 95%의 처리시간이 합성곱계층이라 한다. 따라서 합성곱계층의 연산을 어떻게 효율적으로 고속화하냐가 딥러닝의 과제이다.

GPU를 이용한 고속화라했는데, GPU는 원래 영상, 이미지 연산을 위한 그래픽 전용 보드에 이용해왔다. GPU는 복잡한 계산에 대한 능력은 낮지만, 병렬적인 연산이 가능한데, 딥러닝에서는 대량의 단일곱셈누산(또는 큰 행렬의 곱)을 수행해야하고, 단순한 연산을 대량으로 해야하는것에 이러한 GPU가 특화된 것이다. 그래서 CPU보다 빠른것이다. (CPU는 연속적이고 복잡한 계산처리를 잘함)
실제로 CPU에서 40일 걸리던것이 GPU에서 6일까지 단축되고, cuDNN이라는 엔비디아의 CUDA라는 GPU컴퓨팅용 통합개발환경에서 동작하는 라이브러리로 더욱 빠른 결과를 얻을수있다. (딥러닝에 최적화된 함수등이 구현되어있음)

**분산학습** : 딥러닝만으로도 물론 가속되었지만, 그래도 심층신경망의 학습에는 며칠 몇주처럼 오래걸린다. 딥러닝은 많은 시행착오가 필요한데 뛰어난 신경망 만들려고 오랜시간 시험을 반복해야하는 점을 보았을때 1회 학습 시간을 최댛나 줄이고 싶어질것이다.

다수 GPU와 기기로 계산을 분산하기도 한다. 구글텐서플로, 마이크로소프트CNTK(Computational Network Toolkit)은 분산학습에 역점 두고 개발중이다. 이런 대기업의 거대한 데이터센터에서 저지연,고처리량 네트워크를 활용한 분산학습이 놀라운 효과를 보이고있다. 
GPU가 늘어날수록 학습속도도 빨라지는데, 여러기기를 연결하여 GPU를 100개까지 사용하니 56배나 속도가 빨라져 7일의 과제를 3시간만에 해결하는것을 볼수있었다.
물론 계산을 어떻게 분산시키냐는 어려운 문제이고, 컴퓨터사이의 통신 및 데이터동기화.. 쉽게 해결못하는 문제를 가지고있기에 이러한 텐서플로같은 프레임워크에게 맡기는것이 좋다.

**연산 정밀도, 비트 줄이기** : 계산능력도 중요하지만 메모리의 용량 또는 버스대역폭이 딥러닝 고속화에 병목이 될수있다. 왜냐면 대량의 가중치 매개변수, 중간데이터를 메모리에 저장해야하고
버스 대역폭면에서는 GPU의 버스 흐르는 데이터가 많아져 한계넘어섰을때 병목이된다. 따라서 네트워크 주고받는 데이터의 비트수는 최소가 바람직하다.
컴퓨터는 주로 64비트나 32비트의 부동소수점 수를 사용해 실수를 표현한다. 많은 비트를 사용하면 계산 오차는 줄지만 그만큼 계산에 드는 비용 및 메모리 사용량이 늘고 버스대역폭에 부담을 준다.

다행히 딥러닝은 고수치 정밀도를 요하지 않는다. 입력에 노이즈가 조금 있어도 출력 결과가 잘 달라지지 않는 강건함이 있기에 표현 비트수를 낮추어 흐르는데이터의 품질을 조금 퇴회해도 출력엔 큰 영향이 없다.
즉, 딥러닝은 64비트 (배정밀도)나 32비트(단정밀도)가 아닌 16비트(반정밀도)만 사용해도 학습에 문제가 없다고 한다. 실제로 엔비디아 GPU 파스칼아키텍쳐가 이 포맷을 지원하여 이러한 16비트의 반정밀도 부동소수점이 표준으로 이용될수도 있다.

이 외에도 딥러닝의 비트 수 줄이는 연구가 몇가지 진행중이다. 중간데이터를 1비트로 표현하는 방법도 있다. 딥러닝 고속화를 위한 비트 줄이기는 앞으로도 주시할 분야이다. 왜? 임베디드에도 나중엔 활용하고자 하니까! (저메모리)
#### 딥러닝의 활용
딥러닝을 손글씨 숫자 인식과같은 이미지 분류 중심으로 살펴보았는데, 이러한 사물인식뿐만 아니라 온갖문제에도 딥러닝은 적용가능하다. 이미지, 음성, 자연어 등
그 중 컴퓨터 비전 분야에서 몇가지 소개할 수 있다.
1) **사물 검출** : 이미지 속에 담긴 사물의 위치와 종류(클래스)를 알아내는 기술이다. (이미지예시)
   이러한 사물검출은 사물 인식보다 어렵다. 왜냐면 전체 이미지가 아닌, 어딘가에 있는 사물의 위까지 알아내야하고, 여러 사물이 동시에 있을수도 있기 때문이다.
   CNN으로 이러한 사물검출문제를 해결할 몇가지 제안되었고, 꽤 좋은 성능을 보여 딥러닝이 사물검출에도 좋다는것을 보여주었다.
   대표적으로 **R-CNN** 이라는 Region with CNN이 있다. 흐름을 보면(이미지) 입력이미지에서 후보영역을 뽑아내는 R과정을 거치고 해당 영역들에 대해 CNN 계산으로 클래스 분류를 진행한다.
   실제 처리흐름은 다소 복잡하지만 큰 틀에서 보면 이렇게 후보영역추출+CNN특징계산으로 구성되어 있는 것이다.
   후보영역추출은 다양한 기법을 활용가능한데 R-CNN논문에선 Selective Search라는 기법을 활용했고, 후보영역 추출까지 CNN으로 처리하는 Faster R-CNN 기법도 있다고 한다. 모든일을 하의 CNN에서 처리하기에 또 매우 빠르다 한다.
2) **분할** : 이미지를 픽셀수준에서 분류하는 문제를 말하는데, 픽셀단위로 객체마다 채색된 지도데터로 학습한다. 즉 지도학습. 추론시에는 입력이미지의 모든 픽셀을 분류하게 된다. (이미지를 보여준다. 지도용이미지, 즉 학습용 데이터가 이런방식의 객체마다의 채색지도를 사용한다.)
   가장 단순한 방법은 모든 픽셀 각각 추론하는것이있다. 예를 들어 직사각형 영역 중심 픽셀 클래분류 신경망을 만들어 모든 픽셀 대상으로 하나씩 추론작업 실행한다. 이러한 낭비를 줄여주는 기법으로 FCN(Fully Convolutional Network)가 고안되었다. 한번의 순전파 처리로 모든 픽셀 클래스 분류하는 놀라운 기법이다.
   직역하면 '합성곱 계층으로만 구성된 네트워크'가 되는데, 공간볼륨유지한채 출력까지 처리한다. (완)