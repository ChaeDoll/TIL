##### 추가적인 활성화함수 Affine/Softmax 계층 구현하기
신경망 순전파에서는 가중치 신호의 총합을 계산하기 위해 행렬곱을 사용했다. (w1x1 + w2x2 + .. + wnxn 보다 행렬곱 W * X + b 를 사용) Y = np.dot(X, W) + B 처럼 계산함. 그리고 이 Y를 활성화 함수로 변환 후 다음층으로 넘기는게 신경망 순전파 흐름.
#### Affine 계층
신경망 순전파에서 수행하는 **행렬곱을 Affine Transformation (어파인 변환)** 이라고 한다. 신경망에서 가중치와 노드값, 편향을 통해 결과값을 얻는 과정을 계산그래프로 표현한다.
행렬 변수 X와, W가 dot(행렬곱)노드에 입력되고, 이렇게 얻은  X * W와 편향 B를 덧셈노드에 입력하여 Y를 얻어낸다.
**주의 해야할 점은** 지금까지는 스칼라값으로 계산했다면, 이제는 행렬이 흐른다는 사실을 기억해야 한다. 행렬의 역전파도 행렬 원소마다 전개하여 계산한다면 스칼라값 사용한 계산그래프와 같은 순서로 이루어진다.

실제로 dL/dX = dL/dY * W^T 이고, dL/dW = X^T * dL/dY가 나올 것이다. (덧셈, 곱셈노드의 역전파 공식을 활용)
w^T에서 T는 전치행렬 (i, j)를 (j, i)로 위치를 바꿈. (2,3) shape였다면 (3,2)가 된다.
결과로 따지면 아래와 같은 흐름이 된다.
dL/dY (3,) => dL/dY(3,) => dL/dY(3,) * W^T(3, 2) 가 dL/dX이다.
dL/dY (3,) => dL/dY(3,) => X^T(2, 1) * dL/dY(1,3)
**행렬곱을 위해서는 곱에 대응하는 차원 원소수를 맞춰야하는 것을 기억해야함.**
또한 **X와 dL/dX는 형상이 동일해야한다.** 이 두가지가 매우 중요하여 이를 기준으로 식을 위치시켜야 한다.
X는 (2,)이다. 이를 만들기 위해서 W(2,3)와 dL/dY(3,)를 조합하여 dL/dX (2,)를 만들고자 한다면, 자연스럽게 차원수를 맞추기 위해 W^T하여 (3,2)로 맞춰야한다.
왜냐면 dL/dY(3,)는 1차원의 3열이기에 (1, 3)이라고 생각하면 된다. 그러면 dL/dY * W^T 순서로 곱하면 (1,2)이 완성될 것이고, dL/dX와 동일한 것을 볼 수 있다.

dL/dW 역시 (2, 3)이 되어야하고, dL/dY(1,3)과 X(1, 2)를 활용하여 (2,3)을 만들기 위해서는 X^T(2,1)로 만든뒤 행렬곱하여 X^T * dL/dY (2 x 1 x 3) => (2,3)으로 만들 수 있다.
**조립은 우리가 생각해서 하면 된다**
##### 배치용 Affine 계층
- 지금까진 입력데이터가 X 하나일 때만 고려했다. 데이터 N개를 묶어서 배치로 순전파를 하는경우에 어떻게 사용할까
- dL/dX (N, 2)가 된다. 가중치 W(2, 3)와 B(3,)을 통해 Y(N, 3)을 만든다. 기존과 다른것은 X가 (N,2)가 된 것 뿐. 역전파도 행렬 형상만 조심하면 된다.
- 편향 더할 때도 제대로 매핑될 수 있게 주의해야한다.
#### SoftMax와 손실함수 계층
입력이미지가 Affine 계층과 ReLU계층을 거치며 변환되고, 마지막에 SoftMax를 통해 정규화되어 확률로 나타나게 된다. 이제 Softmax 함수를 손실함수(교차 엔트로피 오차)를 포함하여 구현한다.
Softmax는 y=1/1+exp(-x) 를 순전파로 하며 역전파로 dL/dY * y(1+y)

---
이렇게 Softmax 계층을 역전파로 나타내보았는데, 교차엔트로피오차 계층을 구현하고 둘을 합치면 된다. 교차 엔트로피 오차 (Cross Entropy Error) 계층은 다소 복잡한데, 부록을 참고하여 그 형태를 알고자하였다.
> 왜 Softmax-Loss Function 계층 구조를 활용할까?

Softmax 함수 수식은 y_k = exp(a_k) / 시그마(1~n)exp(a_i) 이다. 이를 계산 그래프로도 나타낼 수 있다. 입력이 a1, a2, a3일때 출력 또한 y1, y2, y3로 나타난다.

교차엔트로피오차 수식은 L = -시그마(k)t_k * log(y_k) 이다.
이 또한 계산 그래프로 나타낼 수 있다. -1 * (t1 * log(y_1) + ... + tn * log(y_n)) = L 으로.

이제 역전파를 그린다.
**교차엔트로피오차 역전파**부터 시작한다. L에서 1로 시작하여 역전파를 표시한다.
-1 * (t1 * log(y_1) + ... + tn * log(y_n)) 과정은 곱셈노드이기에 t1 * log(y_1) + ... + tn * log(y_n) 로의 가중치는 -1이다. 이후 덧셈노드들을 마주쳐서 그대로 -1로 흘려보내진다. 
각각 t1 * log(y_1)과 같은 노드들은 곱셈노드이고, log(y_1) 방향으로의 가중치는 -1 * t1 하여 -t1이다.
그리고 log노드를 만난다. log노드의 역전파를 통해 y1노드에서의 가중치는 -t1/y1이 된다.
> Log 노드는 lnx의 미분은 1/x이다. ln(f(x)) = f'(x)/f(x) 이다. 그리하여 log노드를 만났을 때 역전파를 통해 -t1 * log(y1)의 미분 , 따라서 -t1 * (1/y1)이 되고, 그리하여 -t1/y1이 된다.

총 3개의 입력 신호들이 있었다면 (y1, y2, y3) 이들의 역전파 값은 (-t1/y1, -t2/y2, -t3/y3)가 된다.
이제 여기에 Softmax의 역전파를 결합한다.
**Softmax의 역전파**는 조금 복잡하다. 우선 위에서 Softmax-Loss 계층이기에 위에서 구한 역전파 값이 Softmax 역전파의 시작점이 된다. 각각의 -t1/y1, -t2/y2, -t3/y3값들이 곱셈노드를 만나 입력신호들이 바꾸어 곱해진다. -t1/y1의 기준에서 먼저 보겠다.
-t1/y1 * exp(a1)이 된다. y1 = exp(a1)/S이다. 이러한 성질을 활용하면 exp(a1) = S * y1으로 나타낼 수 있고, 이는 곧 -t1/y1 * exp(a1) 은 -t1 * S 로도 표현이 가능해진다.
(-t1S, -t2S, -t3S)가 다같이 '/'노드에 입력이 된다. -S(t1+t2+t3) 로 합칠 수 있고, /노드는 -y^2(순전파에서의 출력)이 되기에 -S(t1+t2+t3)에 1/S^2를 곱하여 1/S * (t1+t2+t3)가 된다.
**여기서** t1, t2, 3들은 원-핫 벡터의 성질을 갖는 정답레이블이기에 셋 중 하나만 1이고 나머지는 0일 것이다. 그렇다면 1/S * (t1+t2+t3) 또한 **1/S**라고 나타낼 수 있게 된다. (t1+t2+t3 = 1)
이제 덧셈노드를 만나 각각의 역전파로 1/S를 그대로 흘려보낸다. 이제 마지막으로 두갈래길에서 합쳐지는 exp노드를 만난다.
아까 곱셈길에서 S방향으로 -t1S가 되었었다. 이제 exp(a1) 방향으로는 1/S를 곱해줄 수 있다. -t1/y1 * 1/S = -t1/exp(a1)이 된다.
이제 1/S와 -t1/exp(a1) 두 역전파가 exp노드를 만난다. exp노드는 미분해도 그대로 exp이다. 나눠졌던 갈래길이 먼저 합쳐지기에 더해준다. 1/S + (-t1/exp(a1)) 이고, 1/S - t1/exp(a1)이다. 여기에 exp노드를 역전파로 거치면 (1/S - t1/exp(a1)) * exp(a1) 이다. S=exp(a1)/y1이다. 따라서 (y1/exp(a1) - t1/exp(a1)) * exp(a1) 하면 exp(a1)이 약분되고, **결과적으로 y1-t1만 남는다.** 나머지 a2, a3들도 y2-t2, y3-t3라는 역전파가 도출되었다.

복잡한 과정이 있지만 이처럼 차츰차츰 국소적인 계산을 역순으로 해쳐나가면 이처럼 간소화된 결과를 얻어낼수있다.

Softmax-Loss(교차엔트로피오차)는 입력값에서의 변화율이 y1-t1인점 기억! 예시에선 a1, a2, a3까지 했지만 an까지 해서 변화율을 yn-tn까지 구할 수 있다는 점을 알 수 있다.
##### 결과
결과적으로 y1-t1과 같은 결과를 내놓는데, y1,y2,y3는 Softmax의 출력이고 t1,t2,t3는 정답레이블이기에 y1-t1, y2-t2, y3-t3는 Softmax계층의 출력과 정답레이블의 뺄셈이 이루어진 값이다.
신경망 역전파에서는 이만큼의 차이, 오차가 앞 계층에 전달된다. 이것이 신경망 중요성질.

신경망 학습목적은 신경망의출력(Softmax출력)이 정답 레이블에 가까워지도록 가중치매개변수 값 조정하는 것이었다. 신경망 출력과 정답레이블의 오차를 효율적으로 앞에 전달해야하는데, y1-t1같은 결과는 Softmax계층출력과 정답레이블의 차이를 나타내는것으로, 오차를 그대로 드러낸다.

이렇게 말끔히 떨어지는것이 우연이 아니다. 교차엔트로피오차가 그렇게 설계되어있다.
회귀 출력층에서 사용하는 항등함수의 손실함수로 '오차제곱합'을 이용하는 이유도 이와 같다.  **항승함수의 손실함수로 오차제곱합 사용하면 역전파 결과가 y1-t1, y2-t2처럼 나온다.**

예시를 들어 설명. 정답 레이블이 0,1,0이라 할 때, Softmax계층 출력이 0.3, 0.2, 0.5라고 하자. 그러면 정답은 a2이지만 이때 확률은 겨우 0.2기에 제대로 정답을 못내는 신경망이다.
Softmax 역전파를 통해 y1-t1, y2-t2, y3-t3를 주어 (0.3, -0.8, 0.5)라는 큰 오차로 전파된다. 앞에서의 큰 오차로 가중치가 크게 변화되는 것이다. (얘는 아니야!!!!)

이번에는 정답레이블 0,1,0에 Softmax출력층이 0.01, 0.99, 0 이라고 해보자. 역전파가 보내는 오차는 0.01, -0.01, 0으로 매우 작은 학습정도로 줄어들게 되었다. 

실제로 코드로도 구현해보자.