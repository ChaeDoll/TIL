## 6장. 학습 관련 기술들
> # 신경망 학습의 핵심 개념
> - 가중치 매개변수 최적값 탐색하는 최적화 방법
> - 가중치 매개변수 초깃값, 하이퍼파라미터 설정 방법 등
>
>오버피팅 대응책 : 가중치 감소, 드롭아웃 등의 정규화 방법

#### 매개변수 갱신
신경망의 목적은 손실함수 값을 가능한 낮추는 매개변수(가중치, 편향)를 찾는 것. 이러한 문제를 **최적화**라고 한다. 신경망에서의 최적화는 어렵다. 왜냐면 매개변수 수가 엄청나게 많기 때문이다. 심층 신경망은 더욱 어렵다.
 
지금까지는 최적의 매개변수를 기울기(미분)을 통해 구했고, 기울어진 방향으로 매개변수를 갱신하는 일을 반복하는 **확률적 경사 하강법(SGD)** 을 했었다. 문제에 따라 이보다 똑똑한 방법도 있다.
확률적 경사 하강법의 단점이 무엇인지, 그에 따른 다른 최적화 방법은 무엇이 있는지 알 수 있다.

눈을 감고 가장 깊은 곳에 걸어들어가야 한다면, 우리는 바닥의 경사를 통해 더욱 안쪽 길로 들어가는 일을 반복할 것이다.
#### 확률적 경사 하강법 (SGD)
확률적 경사 하강법 (SGD) 의 수식이다.
$W <= W - l*dL/dW$
W는 갱신할 가중치 매개변수, dL/dW는 W에 대한 손실함수 기울기. l은 학습률을 의미한다. (학습률은 0.01이나 0.001과 같은 값을 미리 정의함) <=는 우변값을 좌변에 대입한다는 것.
이러한 수식은 그저 기울어진 방향으로 일정 거리만큼만 W를 이동시키겠다는 단순한 방법이다.

> dL/dW는 손실값의 미분이 0에 가까워지고 있는지 멀어지고 있는지를 알 수 있다. 접선의 기울기가 양수라면 기울기가 0이 되기 위해서는 W값을 감소해야 하기에 -l * dL/dW = 음의 방향으로 조정하게 된다. 또한 접선 기울기가 음수라면 기울기가 0이 되기 위해서는 W를 증가시켜야 하기에 -l * dL/dW = -l * (음수) = 양의 방향으로 조정하게 된다.

```
def __init__(self, lr=0.01):
	self.lr = lr
def update(self, params, grads):
	for key in params.keys():
		params[key] -= self.lr * grads[key]
```
파이썬으로 이와 같이 구현할 수 있는데, W파라미터를 $W=W-1*lr*기울기$  로 구하는 것이다.
이러한 SGD같은 것을 Optimizer(최적화를 진행하는자)라고 할 수 있다.
##### SGD 단점
구현 쉽고 단순하지만, 문제에 따라 비효율적일 때 존재한다. $f(x,y)=1/20*x^2+y^2$ 라는 수식이 있다면, 이는 길게 늘린 밥그릇과 같은 모양이 된다.
이러한 식에 SGD를 적용하면 최솟값인 0, 0까지 지그재그로 계속 탐색하게 된다. 비효율적이다.
이렇게 SGD는 **비등방성 함수 (방향에 따라 성질, 기울기가 달라지는 함수)** 에서는 탐색 경로가 비효율적이게 된다. 이 경우 무작정 기울어진 방향으로 가는 방식은 좋은 해결책이 되지 못한다.
기울어진 방향이 본래의 최솟값을 가르키지 않는다는 이유도 있다.
=> 이를 개선해주는 방법 : **모멘텀, AdaGrad, Adam**
#### 모멘텀 (Momentum)
운동량을 뜻하는 단어. 물리와 연관되어있음.
$v <= av - l*dL/dW$
$W<=W+v$
W라는 갱신할 매개변수를 v라는 것을 활용해 갱신한다. a는 0.9와 같은 값으로 설정하고, av는 물체가 아무런 힘 안받을 때 서서히 하강시키는 역할, 물리에서 지면마찰, 공기저항을 뜻한다. v는 물체의 속도.

모멘텀을 활용하면 SGD처럼 지그재그로 문제 해답에 도착하는 것이 아닌, 공이 바닥을 구르듯 움직인다. 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가기에 지그재그 움직임이 줄어든다.
#### AdaGrad
신경망 학습에선 학습률이 중요하다. 너무 작으면 학습이 길어지고 너무 크면 학습이 제대로 안된다.
학습률을 정하는 효과적인 기술로 **학습률 감소(Learning Rate Decay)** 가 있다. 학습 진행할 수록 학습률을 점차 감소시키는 방법이다. 실제 신경망 학습에 자주 쓰인다.
이러한 원리를 발전시킨 것이 AdaGrad이다. '각각' 매개변수에 맞춤형인 값을 만든다.
AdaGrad는 적응적으로 학습률을 조정하며 학습을 진행한다.
$h<=h+dL/dW(*)dL/dW$  ( * )는 원소별 곱셈
$W<=W-l\frac{1}{\sqrt{h}}\frac{dL}{dW}$
기존 기울기 값을 제곱하여 계속 더해주는 방식으로 h를 갱신한다. 그리고 이러한 h를 루트로 감싸서 W값에 적용하는 l값을 점차 감소시킨다.
AdaGrad를 파이썬으로 구현할 때는 가장 중요한 것이 바로 나누기하는 1/루트h 에서 h값이 너무 작아질수있기에 +1e-7을하여 0으로 나누어지는것을 방지하는 것이다.

> 가장 기초적인 원리는 결국 W = W - l* dL/dW 이다. 여기의 변형이 계속 이루어지는것.

(계속 제곱하여 더해가는 h값이기에, 학습 진행할수록 갱신 강도가 약해지고, 어느순간 갱신량은 0이되어 갱신하지 않게된다. 이 문제를 개선한 RMSProp이라는 방법도 있다. 먼 과거의 기울기를 잊고 새로운 기울기 정도를 크게 반영하는 방법인데, 지수이동평균이라하여 과거 기울기 반영 규모를 기하급수적으로 감소시키는 방법.)

AdaGrad는 실제로 결과를 보면 최솟값을 향해 매우 효율적이게 움직이는 모습을 볼 수 있다. 
#### Adam
모멘텀은 공구르기, AdaGrad는 원소마다 갱신 정도를 조정. 이 두가지를 융합한 AdaGrad+Momentum 이것이 바로 Adam이다.
굴러가면서 갱신정도를 적응적으로 조정하는 것. 또한 하이퍼파라미터의 편향보정도 들어간다.

SGD, 모멘텀, AdaGrad, Adam 네 가지를 알아보았는데, 어느 하나 정답이라는 것은 없다.
각자 잘 해결하는 문제, 잘 해결하지 못하는 문제가 있을 뿐이다. 아직도 많은 연구에서 SGD를 사용하고 있고, Adam을 요즘에는 많이 사용한다. 각자 상황을 고려하여 쓰는 것이다.
### MNIST로 비교한 갱신 방법들
SGD, 모멘텀, AdaGrad, Adam 중에서 MNIST에 가장 효과가 안좋은 것은 SGD였고, 나머지 세개는 비슷하지만 근소하게 AdaGrad가 성능이 더 좋았다. 다만, 이 경우는 각 층이 100개 뉴런으로 구성된 5층 신경망에서 ReLU를 사용했을 때의 일이다. 이러한 하이퍼파라미터(학습률, 신경망 층 깊이 등)에 따라 결과가 달라지는 것을 명심. => 그래도 알 수 있는것은 SGD보단 다른 3개가 더 빠르고 정확하다.
### 가중치 초깃값
가중치의 초깃값을 무엇으로 하느냐도 중요하다. 권장 초깃값은 무엇일까
초깃값을 모두 0으로 설정하면? => **나쁜 아이디어**이다. 올바른 학습이 진행되지 않음.

오차역전파법에서 모든 가중치 값이 똑같이 갱신되기에 문제가 된다. 가중치를 여러개 갖는 의미가 사라진다. 초깃값은 그래서 무작위로 설정해야 한다.
##### 시그모이드 에서의 가중치 초깃값
가중치 표준편차를 1로 해도 문제가 발생한다.
시그모이드를 사용한 경우를 예로 들면, 출력이 0혹은1에 가까워지자 미분이 0에 다가간다. 그래서 역전파 기울기 값이 점점 작아지다 사라진다. 이것이 **기울기 소실** 문제이다. 층이 깊은 딥러닝에서는 기울기 소실이 더욱 큰 문제가 된다.

가중치 표준편차를 0.01로 바꿔 실험을 반복한다면? 이번에는 0.5 정도로 집중된다. 0과 1에 치우쳐지지는 않아 기울기 소실문제는 일어나지 않았다. 하지만 활성화값이 치우쳐진것이 문제이다. 다수 뉴런이 거의 같은 값을 표현한다면, 여러개를 둔 의미가 없다. 이렇게 **너무 작은 표준편차는 표현력을 제한한다**는 문제를 갖는다.

권장 가중치 초깃값이 있다. 일명 **Xavier 초깃값**이다. 활성화함수가 선형인 것이 전제일 때 초깃값의 표준편차가 $\frac{1}{\sqrt{n}}$인 분포를 사용하는 것이다. (sigmoid나 tanh함수는 좌우대칭이라 중앙부근이 선형인 함수로 볼 수 있음) 이를 활용해보니 넓게 분포되는 것을 볼 수 있다.
##### ReLU에서의 가중치 초깃값
ReLU에 특화된 초깃값은 카이밍 히의 이름을 따 **He 초깃값**이라고 부른다. 앞 계층 노드가 n개일 때 표중편차가 $\sqrt{\frac{2}{n}}$ 인 정규분포를 사용한다. 실제로 ReLU의 표준편차값이 0.01, Xavier값, He값을 적용해보면 0.01의 경우는 학습이 거의 이루어지지않고, Xavier은 층이 깊어지면서 문제가 생기고, He초깃값은 모든 층에서 층이 깊어져도 분포가 균일하게 유지된다.

> [!중요]
> 초기 표준편차값을 Sigmoid는 Xavier값, ReLU는 He값을 사용한다. 이것이 현재 모범사례이다.

