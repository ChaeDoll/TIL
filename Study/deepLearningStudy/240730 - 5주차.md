## 6장. 학습 관련 기술들
> # 신경망 학습의 핵심 개념
> - 가중치 매개변수 최적값 탐색하는 최적화 방법
> - 가중치 매개변수 초깃값, 하이퍼파라미터 설정 방법 등
>
>오버피팅 대응책 : 가중치 감소, 드롭아웃 등의 정규화 방법

#### 매개변수 갱신
신경망의 목적은 손실함수 값을 가능한 낮추는 매개변수(가중치, 편향)를 찾는 것. 이러한 문제를 **최적화**라고 한다. 신경망에서의 최적화는 어렵다. 왜냐면 매개변수 수가 엄청나게 많기 때문이다. 심층 신경망은 더욱 어렵다.
 
지금까지는 최적의 매개변수를 기울기(미분)을 통해 구했고, 기울어진 방향으로 매개변수를 갱신하는 일을 반복하는 **확률적 경사 하강법(SGD)** 을 했었다. 문제에 따라 이보다 똑똑한 방법도 있다.
확률적 경사 하강법의 단점이 무엇인지, 그에 따른 다른 최적화 방법은 무엇이 있는지 알 수 있다.

눈을 감고 가장 깊은 곳에 걸어들어가야 한다면, 우리는 바닥의 경사를 통해 더욱 안쪽 길로 들어가는 일을 반복할 것이다.
#### 확률적 경사 하강법 (SGD)
확률적 경사 하강법 (SGD) 의 수식이다.
$W <= W - l*dL/dW$
W는 갱신할 가중치 매개변수, dL/dW는 W에 대한 손실함수 기울기. l은 학습률을 의미한다. (학습률은 0.01이나 0.001과 같은 값을 미리 정의함) <=는 우변값을 좌변에 대입한다는 것.
이러한 수식은 그저 기울어진 방향으로 일정 거리만큼만 W를 이동시키겠다는 단순한 방법이다.

> dL/dW는 손실값의 미분이 0에 가까워지고 있는지 멀어지고 있는지를 알 수 있다. 접선의 기울기가 양수라면 기울기가 0이 되기 위해서는 W값을 감소해야 하기에 -l * dL/dW = 음의 방향으로 조정하게 된다. 또한 접선 기울기가 음수라면 기울기가 0이 되기 위해서는 W를 증가시켜야 하기에 -l * dL/dW = -l * (음수) = 양의 방향으로 조정하게 된다.

```
def __init__(self, lr=0.01):
	self.lr = lr
def update(self, params, grads):
	for key in params.keys():
		params[key] -= self.lr * grads[key]
```
파이썬으로 이와 같이 구현할 수 있는데, W파라미터를 $W=W-1*lr*기울기$  로 구하는 것이다.
이러한 SGD같은 것을 Optimizer(최적화를 진행하는자)라고 할 수 있다.
##### SGD 단점
구현 쉽고 단순하지만, 문제에 따라 비효율적일 때 존재한다. $f(x,y)=1/20*x^2+y^2$ 라는 수식이 있다면, 이는 길게 늘린 밥그릇과 같은 모양이 된다.
이러한 식에 SGD를 적용하면 최솟값인 0, 0까지 지그재그로 계속 탐색하게 된다. 비효율적이다.
이렇게 SGD는 **비등방성 함수 (방향에 따라 성질, 기울기가 달라지는 함수)** 에서는 탐색 경로가 비효율적이게 된다. 이 경우 무작정 기울어진 방향으로 가는 방식은 좋은 해결책이 되지 못한다.
기울어진 방향이 본래의 최솟값을 가르키지 않는다는 이유도 있다.
=> 이를 개선해주는 방법 : **모멘텀, AdaGrad, Adam**
#### 모멘텀 (Momentum)
운동량을 뜻하는 단어. 물리와 연관되어있음.
$v <= av - l*dL/dW$
$W<=W+v$
W라는 갱신할 매개변수를 v라는 것을 활용해 갱신한다. a는 0.9와 같은 값으로 설정하고, av는 물체가 아무런 힘 안받을 때 서서히 하강시키는 역할, 물리에서 지면마찰, 공기저항을 뜻한다. v는 물체의 속도.

모멘텀을 활용하면 SGD처럼 지그재그로 문제 해답에 도착하는 것이 아닌, 공이 바닥을 구르듯 움직인다. 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가기에 지그재그 움직임이 줄어든다.
#### AdaGrad
신경망 학습에선 학습률이 중요하다. 너무 작으면 학습이 길어지고 너무 크면 학습이 제대로 안된다.
학습률을 정하는 효과적인 기술로 **학습률 감소(Learning Rate Decay)** 가 있다. 학습 진행할 수록 학습률을 점차 감소시키는 방법이다. 실제 신경망 학습에 자주 쓰인다.
이러한 원리를 발전시킨 것이 AdaGrad이다. '각각' 매개변수에 맞춤형인 값을 만든다.
AdaGrad는 적응적으로 학습률을 조정하며 학습을 진행한다.
$h<=h+dL/dW(*)dL/dW$  ( * )는 원소별 곱셈
$W<=W-l*1/$