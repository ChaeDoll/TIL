## 6장. 학습 관련 기술들
> # 신경망 학습의 핵심 개념
> - 가중치 매개변수 최적값 탐색하는 최적화 방법
> - 가중치 매개변수 초깃값, 하이퍼파라미터 설정 방법 등
>
>오버피팅 대응책 : 가중치 감소, 드롭아웃 등의 정규화 방법

#### 매개변수 갱신
신경망의 목적은 손실함수 값을 가능한 낮추는 매개변수(가중치, 편향)를 찾는 것. 이러한 문제를 **최적화**라고 한다. 신경망에서의 최적화는 어렵다. 왜냐면 매개변수 수가 엄청나게 많기 때문이다. 심층 신경망은 더욱 어렵다.
 
지금까지는 최적의 매개변수를 기울기(미분)을 통해 구했고, 기울어진 방향으로 매개변수를 갱신하는 일을 반복하는 **확률적 경사 하강법(SGD)** 을 했었다. 문제에 따라 이보다 똑똑한 방법도 있다.
확률적 경사 하강법의 단점이 무엇인지, 그에 따른 다른 최적화 방법은 무엇이 있는지 알 수 있다.

눈을 감고 가장 깊은 곳에 걸어들어가야 한다면, 우리는 바닥의 경사를 통해 더욱 안쪽 길로 들어가는 일을 반복할 것이다.

확률적 경사 하강법 (SGC) 의 수식이다.
$W <= W - l*dL/dW$
W는 갱신할 가중치 매개변수, dL/dW는 W에 대한 손실함수 기울기. l은 학습률을 의미한다. (학습률은 0.01이나 0.001과 같은 값을 미리 정의함) <=는 우변값을 좌변에 대입한다는 것.
이러한 수식은 그저 기울어진 방향으로 일정 거리만큼만 W를 이동시키겠다는 단순한 방법이다.

dL/dW는 손실값의 미분이 0에 가까워지고 있는지 멀어지고 있는지를 알 수 있다. 접선의 기울기가 양수라면 기울기가 0이 되기 위해서는 W값을 감소해야 하기에 -l * dL/dW = 음의 방향으로 조정하게 된다. 또한 접선 기울기가 음수라면 기울기가 0이 되기 위해서는 W를 증가시켜야 하기에 손실함수 미분이 작아지는 방향에 있는 것이고,  있는 것이고, W값을 