## 발표자 임채윤
### 오차역전파법 구현해보기
앞에서 구현한 계층들을 토대로 조립하여 신경망을 구축할 수 있다.
신경망 학습 전체적인 그림
0) 신경망은 갱신가능한 W(가중치), B(편향)이 있고, 이를 훈련데이터에 적응하도록 조정하는데, 이를 학습이라 한다. 총 네 단계로 진행된다.
1) 미니배치 : 무작위로 일부를 가져와서 미니배치의 손실함수값을 가장 작게하도록 한다
2) 기울기 산출 : 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수값을 가장 작게하는 방향을 제시해줄수있다.
3) 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.
4) 반복 (1에서 3을 반복한다)

수치미분을 이용하면 기울기를 산출하기 위해 구현은 쉽지만 계산이 오래 걸렸다. 하지만 오차역전파법을 이용하면 느린 수치미분과는 달리 기울기를 효율적으로 구할 수 있다.

Affine 계층은 그냥 WX+B 를 의미한다. 우리가 N개의 계층으로 구성된 신경망을 이야기 할 때, WX+B와 활성화 함수를 거치는 것을 N번 반복하는 구조를 말한다. (활성화 함수는 ReLU나 Sigmoid를 사용하고 마지막은 SoftmaxWithLoss 같은 계층을 사용함)

오차역전파법을 사용하지 않았을 때와 전체적인 흐름은 비슷하지만, 이전에는 없던 계층 개념을 활용하고, 또한 수치미분이 아닌 오차역전파를 활용한다. 계층마다의 결과값과 오차에 대한 기울기를 구할 수 있다. 

코드로 구현하면 계층을 각각 구현하고 x값을 입력하여 Layer의 수만큼 layer.forward를 반복하게 한다. 이러면 가장 마지막 활성화함수(Softmax with Loss)를 제외하고 모든 계층을 거치게 된다. loss는 이러한 predict를 수행하고 난 이후 마지막에 lastLayer까지 forward해준다.

gradient 함수는 역전파 1부터 시작하여 lastLayer를 우선 backward해주고, 이후 각 계층들을 역순으로 backward해준다. 그리고 역전파 과정을 통해 얻은 최종적인 W,B에 대한 dW, dB를 gradient로 저장한다. (각 계층 별 가중치값을 얻어낼 수 있음)
이러한 것들을 하나의 grads안에 다 넣어둘수있다. (파라미터 별 값들로 저장됨)

계층으로 구성한 덕에 쉽게 신경망을 구축할 수 있게 되었다. 신경망이 추가됨에 따라 필요한 만큼 계층을 추가하기만 하면 된다.

### 오차역전파로 구한 기울기 검증
수치미분은 느리다. 그런데 과연 아무 쓸모가 없을까? => 수치미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요하다. 구현하기 쉽다는 장점이 있기에 실수하지 안