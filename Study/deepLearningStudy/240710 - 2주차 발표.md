## 발표자 : 임우진 (4챕터 신경망 학습)
- 개요 : 학습의 추구하는 방향부터 손실함수까지 (다양한 손실함수, 왜 설정하는지, 실습)
- 해석적 미분, 수치 미분, 편미분 / 기울기, 경사법, 신경망에서의 기울기, 왜 경사법 사용하는지
- 손실함수를 통해 최적의 매개변수를 찾는 것이 목표이다.
- 뉴런이 증가하면 증가할수록 매개변수가 급격히 증가하기에 수작업은 한계... 따라서 자동으로 만들고자 함
- 세 가지 방안이 있다. 사람이 생각한 알고리즘 / 사람이 특징만 생각하고 기계학습으로 해결 / 사람 개입없이 딥러닝만으로 해결. 우리는 가장 마지막 경우를 바라고자 함
- 학습 횟수도 중요한데, 과소적합과 과잉적합을 모두 피해야 한다. 적절한 정도가 중요
- **손실함수**
- **오차제곱합**. 정답 레이블의 원-핫 인코딩이란 0또는 1
	- 정답 레이블과 예측 레이블이 유사할수록 SSE가 낮게 나옴.
	- 오차제곱합(SSE)는 왜 일반적으로 사용? 연속적인 실수값 예측하는 **회귀문제에 많이 사용**. 연산과정, 미분이 쉽다. 경사하강법같은 알고리즘 적용이 쉬움
- **교차 엔트로피 오차** (CEE) - 정답 신경망 출력이 클수록 오차가 작아진다. 
	- 왜 많이 사용하는지 ? 소프트맥스 함수와 결합하여 **분류 문제에 많이 사용** 확률로 사용가능하다.
- 그 외에도 평균 제곱 오차, 평균 절대 오차, 평균 제곱 로그 오차, 교차 엔트로피 손실 등... 많은 것들이 있다.
##### 왜 손실함수를 설정하는가? 정확도가 아니라.
- 정확도를 기준으로 미분을하면 대부분 장소에서 0이 나온다. 매개변수 갱신이 불가능하다. 하지만 손실함수는 변화가 연속적이기에 미분이 가능하고, 이를 활용해 갱신할 수 있다.
- 손실함수를 활용하면 미분값이 양수인 경우 음의 방향으로 갱신, 음수인 경우 양의 방향으로 갱신.
##### 실습
- 실습에 미니배치를 활용해서 특정 애들을 대표로 하여 손실함수 값을 구함
- 배치용 교차엔트로피 오차, 배치용 오차제곱합 공식이 달라지긴 하는데, 그냥 각각의 배치에서 얻은 CEE와 SSE를 구하고 다 더한뒤 N으로 나눠 평균을 구하는것임.
##### 미분
- 미분 종류는 해석적미분, 수치미분이 있다.
- 차분과정에서 오류를 중앙차분으로 구하였다.
- 기울기는 모든 변수에서의 편미분을 벡터로 정의한 것을 기울기라고 한다.
- 기울기가 가르키는 쪽이 함수 출력 값을 가장 크게 줄이는 방향이다. 낮은 곳과 거리가 멀어질수록 화살표 크기가 커진다.
##### 경사하강법 (SGD)
- 최적 매개변수를 학습으로 구하고, 이는 손실함수가 최소가 될 때의 매개변수를 의미함.
- x0 = x0 - 학습율 * 기울기 로 새롭게 갱신가능하다.
- 학습률은 직접 설정하는 것이고, 너무 커도 너무 작아도 좋지 않다. 
- 지역 최저점에 빠지지 않게 조심해야한다.
- 그렇다면 이러한 경사하강법을 신경망에서 어떻게 적용할까?
##### 기울기
w11를 h만큼 늘리면 손실함수값은 0.204h만큼 감소했고, w23을 h만큼 늘리면 손실함수값이 0.133h만큼 증가했다면, w11은 양의 방향으로 갱신해야 손실함수를 줄일수있고, w23은 음의 방향으로 갱신해야 손실함수를 줄일수있다. 더 영향력 큰 것은 w11이다.
> 이러한 w11부터 wnm까지로 이루어진 기울기를 토대로 전체적인 손실함수 변화율을 알수있고, 그를 토대로 어느 방향으로 갱신할지 정할수있다? 아닌가... w11부터 wnm을 각각 조정하는건가.

초기화 함수를 구현한 이유, 왜 초기화가 중요한가
- 초기값이 너무 크거나 작으면 그래디언트 소실문제, 매개변수 업데이트 잘 안되는 문제가 발생.
- 또한 이상한 위치에 두면 지역최저점에 빠지기도 함.

##### 구현의 평가
- 에폭 : 학습에서 훈련데이터를 모두 소진했을 때.
- 에폭 진행될수록 정확도는 증가한다. 만약 그 과정에서 과적합이 이루어지면 안좋겠지만 실제로 해보니 훈련데이터와 시험데이터에 정확도가 유사하게 나오더라. 그래서 잘 학습된 것을 볼 수 있었다.
- 훈련 / 시험데이터를 나눠 사용하는 것은 학습한 모델의 범용 능력을 평가하는데에 도움이 됨.
- 신경망 학습은 손실함수를 기준으로 한다.
- 수치 미분 활용해 기울기를 구함.
- 경사하강법으로 구해진 것은 꼭 최저가 아닐수있음 (지역최저점 문제 발생가능성)


## 피드백
- 오차제곱합에서 k를 데이터의 차원수라고 나와있는데, 데이터의 index라고 보면 될 것 같다.
- 사실 지역최저점은 해결방법이라는게 딱히 없다. 그래서 greedy search라고 해서 하이퍼파라미터를 그냥 조절해보며 혹은 effoc 등... 이러한 조합에 따라 성능이 달라지는지 다 해보고 가장 좋은 모델을 활용하는게 현재 딥러닝에서의 상황이다.

##### 토론
- 학습률은 교재에서 알아서 구하라했는데, 적절한 학습률 설정하는 방법이 크게 세가지가 있더라. 학습률 감소, 사이클릭 학습률, 적응적 학습률... 
- 김용호 박사과정님 : 학습률도 하이퍼파라미터라서 경험적으로 많이 고르더라 보통 0.1, 0.01로 시작하는 경우가 많더라. 다들 그렇게 하는듯 하다. 정답인 학습률이 딱히 없다.
- 교차 엔트로피 오차를 x값이 0에서 1사이인 값을 주던데 (x를 왜 0에서 1로 제한?) 
- 소프트맥스로 총합이 0에서 1사이니까 그래서 그래프를 0에서 1사이로 나온듯!]

#### 궁금증 해결
- 미니배치 사이즈가 작으면 대표성이 있는가? => 어차피 한 에폭 돌릴때 배치사이즈가 작아도 결국 다 돌기때문에 크게 상관없다? 어차피 사용자가 조작하는 하이퍼파라미터이다. 글고 각각 손실함수 구한뒤 더해서 상관없을듯