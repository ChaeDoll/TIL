# 기계학습 11주차 강의  
특징 추출과 선택  
특징추출은 기계학습 과정에서 많이 경험하고 실험하는 과정에서 뭐가 더 적합한지를 알게되는 것.  
시행착오가 가장 많이 필요한 단계이자 매우 중요한 요소가 바로 <b>'특징추출'</b>이다.  
특징이 만족스럽지 못하면 특징을 버리고 다른 특징을 선택하는 방법..  
거리개념이 없다면 거리개념을 만들어주기도 한다.  

특징마다 동적범위가 크게 다른경우에는 문제가 발생한다.  
예를들어 키를 m단위로 표현하고, 몸무게를 kg단위로 표현하게 되면,  
키는 약 0에서 2의 범위로 표현될 것이고, 몸무게는 약 0에서 200의 범위로 표현이 될 것이다.  
그러면 키는 너무 거리계산에 미미한 영향을 미치고 몸무게는 과하게 영향을 주는 요소가 된다.  

그렇기에 이 둘의 정규화를 해야 보다 정확한 값을 구하는데 도움이 된다.  
어느정도 둘을 맞춰줘야한다는 뜻이다.  
^xi = lowi + (highi-lowi)/(maxi-mini) (xi-mini)  
여기서의 high와 low는 사용자가 정하는 범위 (0~1범위로 나타내는게 일반적?)  
여러 사람들 중 최소 키가 1.6m이고 최대키가 1.95이고, 최소몸무게가 65.5고 최대가 72.0이라면  
^x1 = 0 + (1-0)/(1.95-1.6) (x1-1.6) = (x1-1.6)/0.35 이고(키)  
^x2 = 0 + (1-0)/(72-65.5) (x2-65.5) = (x2-65.5)/6.5 이다(몸무게)  
이들로 이제 거리를 계산하면  
a' = (0,0.692)^T, b'=(0.143,0)^T, c'=(1,0.846)^T, d'=(0.229,1)^T 이런식으로 나타나지는데,  
이제 거리를 다시 구하게 되면 두 요소가 각각 잘 반영된 거리를 얻어낼 수 있다.  
<b>동적 범위가 다른 데이터들을 입맛에 맞게 변경할 수 있게 되었음</b>  

### 특징의 선택  
차원을 무조건적으로 낮추는게좋은것이 아니다.  
원래 특징벡터에서 쓸모없거나, 중복성이 강한 특징을 찾아 제거하는 작업이 바로 특징선택이다.  
차원을 낮추면 계산속도가 향상되고 일반화 능력이 증대되는 효과가 있다.  

좋은 경우 : 부류내 분산은 작고 부류간 분산은 큰 상황 => 아주 적합한 특징  
안좋은 경우 : 부류내 분산은크고 부류간 분산은 작은 상황 => 매우 혼잡하게 섞여있다.  
안좋은 경우2 : 더 심각한데... 부류간 분산이 0에 가까워 그냥 분별력이 거의 없는 상황  

> -특징의 분별력-  
선형분리가능/불가능,  단일모드/다중모드,  다른공분산/같은공분산  
여러가지 요소가 포함될 수 있다.  
일반적으로 매우 높은 차원의 데이터는 시작적 표현이 어렵다.  
예를들면 얼굴인식, 필기문자인식 이런경우 수십차원, 수백차원의 특징벡터가 필요하다.  

특징의 분별력을 어떻게 수치적으로 표현하고 좋은 특징을 선택할지.  
분별력 == 파워라고 생각. 파워가 클수록 분별이 잘됨. 파워가 강한 특징벡터들로 선택  
수치적으로 나타내는 척도로 무엇을 사용하냐에 따라 선택할 특징들이 갈릴것이다.  
사용되는 척도는 크게 3가지가 있다.  
- 다이버전스  
- 훈련 샘플의 거리  
- 분류기 성능  

다이버전스는 베이시안에서 배웠듯... 확률분포간 거리를 이용한다.  
확률분포 간 거리가 멀면 멀수록 좋은 분별력을 가진다.  
2개의 부류를 갖으면 9.1식, M개 부류 가지면 9.2식을 사용하면된다.  

### 훈련 샘플간의 거리  
dij = 1/NiNj 시그마(k=1~Ni)시그마(m=1~Nj)dist(x.i^k, x.j^m)  
x.i^k는 부류 wi의 k번째 샘플  
  
Xi={(1,1)^T, (1,2)^T} 이고, Xj={(3,1)^T, (4,1)^T, (4,2)^T}  
그러면 모든 부류들간의 거리를 다 더한다. 1,1과 3,1거리, 1,1과 4,1거리 ... 총 6개  
그렇게 더한거리들을 각각 샘플의 개수를 곱한 2*3 = 6으로 나누어준다.  
이것이 바로 wi와 wj의 거리라고 할 수 있다. (2.760이 나옴)  

### 분류기 성능  
각 분류기마다 특징이 존재, 아이디어가 다르기에 어떠한 분류기 쓰냐에 따라 결과가 달라짐  
분류기를 SVM으로 결정했으면 특징벡터 x를 SVM으로 평가  
주어진 특징벡터 x에 대해 훈련집합 갖고 SVM훈련하고 검증집합(test)을 갖고 성능 측정  
성능을 특징 벡터 분별력으로 판단.  

사용하고자 하는 분류기에 딱 맞는 특징벡터는 찾을 수 있으나, 새로운 특징 벡터마다 분류기를 훈련해야하기에 시간계산이 상당히 필요하다...  