# 기계학습 강의  
## 계층군집화  
응집 계층 알고리즘, 분열 계층 알고리즘 두 분류가 있다.  
작은군집에서 출발하여 모아가는 응집방식, 큰군집에서 나누어 나가는 분열방식이 있다.  

### <b>응집 계층 알고리즘</b>  
샘플 각각이 군집이 되어 출발하고, 유사한 군집을 모으는 작업을 반복하여 점차 군집의 개수가 하나씩 하나씩 사라지는 형태  
덴드로그램을 통해 만들어 보았을 때, 직관적으로 가까운 군집들이 보인다.  
응집 계층 알고리즘은 기본적으론 Dmin을 사용하여 가장 가까운 군집끼리 합치는데, Dmax를 쓰거나 Dave를 쓰거나 가능하긴 함.  

파이썬을 사용하여 결과만 출력해내는것도 가능하고, 샘플간의 모든 거리도 출력 가능하고 이를 squareform을 사용하여 배열로 표현할 수 있다.  
(x1과 x2간, x1와 x3간, x1과 x4간 거리... 등)  
linkage라는걸 통해 single(Dmin), complete(Dmax), average(Dave) 설정 가능  
군집간 거리, 군집에 속한 샘플 수... 로 덴드로그램 그릴 수 있게됨  

AgglomerativeClustering에서 n_cluster의 수를 조정하여 최종 군집의 개수를 지정할 수 있다.  
만약 Dmin이 아니라 Dmax라면?(complete)  
결과가 다르게 나온다.  
(군집이 만들어지고 다른 군집간의 거리를 측정하여 제일 가까운 군집을 택하는데, 군집샘플이 Dmax라는건 두 군집에서 서로 가장 멀리있는 원소 두개를 기준으로 거리를 측정)

single은 긴 군집, complete는 둥근 군집, average는 중간군집을 선호한다.  
덴드로그램을 보고 완전연결이 n_cluster=3이여도 나쁘지않구나 하는걸 알 수 있어야한다?  

모든 군집화의 문제?  
내가 몇개의 군집을 만들 것인가... 2개가 좋을지 3개가 좋을지 4개가 좋을지 딱 개수가 정해져있으면 편하지만, 그런 개수가 정해져있지 않다면 어려울것..  

### <b>분열 계층 알고리즘</b>  
응집은 x1, x2, x3, x4가 각각 다른 군집에서 시작하여 점점 합쳐지는 구조라면...  
분열은 x1x2x3x4 로 시작하여 점점 나누어지는 방식으로, 하향방식이라 얘기할 수 있다.  

{x1},{x2,x3,x4} {x2},{x1,x3,x4} {x3},{x1,x2,x4} {x4},{x1,x2,x3} {x1,x2},{x3,x4} {x1,x3},{x2,x4} {x1,x4},{x2,x3} ... 아무튼 이렇게 2개의 군집이 되는 경우에서 두 군집간에 거리를 구하고 가장 거리가 먼 것을 분열된 군집으로 택하는 방법이다.  
각 샘플마다의 거리를 구해야하기에 계산량이 더욱 많긴하다.  
분열될 수 있는 모든 경우의수를 계산하고. 거리가 가장 먼 것을 찾기  

### 신경망  
지도학습에서의 신경망 : 퍼셉트론, 다층 퍼셉트론  
비지도학습에서도 신경망을 사용한다.  
대표적으로 SOM(self-organizing map), ART(adaptive resonance theory)  

<b>자기 조직화 맵 (SOM)</b>  
샘플들을 상호 비교하며 스스로 군집을 조직해 내는것.  
조건을 만족할 때까지 여러번 수행하여 군집을 조직해낸다.  
경쟁 학습을 사용하여 하나의 샘플이 입력될 때 여러 대표벡터가 경쟁하여 가장 어울리는 군집만 Update하여 그 샘플을 취하는 방식을 사용한다.  
처음 군집을 잘못설정하면 한 군집이 모든 샘플들을 가져가는 문제가 생기기도 함. 그래서 첫 초기화를 잘 해야한다.  
승자는 그 샘플에 적응하는 방향으로 벡터 값을 조금 변화시킨다.  

입력층이 존재함. d차원의 주어진 샘플이 입력되고 경쟁층이라 불리우는 y1, y2,..., ym 이렇게 m개의 군집이 경쟁을 한다.  
가중치와 샘플간의 거리. 유클리디안 거리로 측정 (1번 군집...m번 군집에 다가가는 입력층 중에 가장 가까운 거리를 갖는 가중치를 찾으면 해당 군집만 Update가 된다)  
즉 가중치 벡터에 의해 결정되고 샘플 x.가 입력되면 m개의 가중치군집(벡터)가 경쟁하고 그들 중 가장 가까운 군집이 승리한다. (가장 가까운 벡터)  

이후 학습을 통해 가중치는 새로운 x.가 들어왔을 때. 가중치들 중 가장 가까운 w.를 고르고, 그 w.는 x.와 좀 더 가깝게 변경된다. w.new = w.old + 로우(x.-w.old) , 학습률(0<=로우<=1)  
1에 가까울수록 x.에 가까워지고 0에 가까울수록 기존 위치를 고수함.  
계속되어 샘플이 입력될때마다 w.값들이 조정된다.  

최대 군집개수 : m, 학습률 : 로우, 이웃반경 : r  

가장 가까운 벡터가 승자가 되어 그것을 취하는 승자독식전략 이기에.. 한 가중치만 계속 좋아지고 나머지 가중치는 의미없는 상황이 되기도 함.  
따라서 이웃반경을 통해 근처의 가중치들도 같이 학습시켜주는 r이 있는 것이다. 우리는 고려 X  

Stop Condition은 그냥 iteration함수 혹은 수렴상태(거의 변경되어지는게 없는 상태).. 등일때 반복에서 빠져나오도록 하면 됨  
이렇게 학습을 거친뒤 수행된 학습(가중치)을 토대로 군집화하면 된다.  

새로운 x.에 가장 가까운 가중치 벡터 w.를 찾아서 x.를 군집c에 배정한다.  
이렇게 군집해를 출력한다. 대신 어떠한 샘플도 군집에 포함되어 있지 않으면 제외하고 하나라도 들어있는 군집해를 출력한다. (m개 넣으면 m보다 작은 개수가 나올 수 있다는 것임)  

예제)  
입력노드수 d=2(차원 수), 최대군집개수 m=3, 초기학습률 로우=0.6 이고 루프돌떄마다 로우new=0.8*로우old, 이웃반경r=0으로 해놨다고 가정.  
w1., w2., w3.를 지정하고 그래프에 표기해봄  
x1.이 D(w1., x1.) D(w2., x1.) D(w3., x1.)를 해보면..  
w3.가 가장 가까운 거리가 나왔음.  
그렇기에 가장 가까운벡터 wq가 w3.가 되고, w3.만 Update된다.  
w3.new = w3.old + 로우(x.-w3.old) = (16 14)^T + 0.6*(2 -9)^T = (17.2 8.6)^T  이것이 새로운 w3.의 값  

그리고 x2.샘플을 또 넣으면.. w3가 또 업데이트  
w3.new = w3.old + 로우(x2.-w3.old)  
= (17.2 8.6)^T + 0.48*(2.8 0.4)^T  

이 예시에서 w2.는 초기값이 너무 별로라서 사용되지를 않는다.  
w1.과 w3.는 변경이 되었지만 w2.는 변경되지 않고 표현된다. 이것이 초기값 잘 정해야하는 이유!  

코랩(Colab)으로 수행할 수 있음  
np.argmin으로 index값을 찾는다. 가장 작은 값을 내는 index를 탐색. (np.argmin(comp)하면 comp리스트에서 가장 작은 값을 갖는 comp[i]에서 i를찾아준다.)  
이렇게 찾고 idx를 저장하고 init_w[idx,:]를 해서 가장 거리가 작은 index의 wi.를 조정한다.  

구했던 식들을 그대로 사용하는 방식이다.  
반복하는 iteration횟수를 몇번할지를 추가해서 완성하면 알고리즘이 완성된다.  
그렇게 Udpate된 w.를 기반으로 샘플들을 update해야한다. 학습된 SOM을 기반으로 결과 도출  

코드를 보면 w2.가 선택되면 w2.만 변경되고, w0.가 선택되면 w0.만 update되는 것을 볼 수 있다.  

# 과제..  
완전연결, 단일연결 두개 각각 군집해 구하고 비교 ~6.6까지