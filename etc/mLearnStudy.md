# 기계학습 중간고사 정리  
## 1주차  
### 기계학습이란..  
기계학습은 Machine Learning이라고도 불린다.  
기계학습은 인공지능 기술의 기초 분야로서, 매우중요한 역할을 한다.  
설명에 앞서, 기계학습 강의에서는 데이터가 중요함, 어떤 모델을 사용하는지가 중요함, 기계학습방법 과정 이 중에서 모델 선택과 학습과정에 대한 설명이 주가 될 것이다.  

과거 10년전부터 각광받은 '인공지능'이라 불리던 것은 사실상 머신러닝이다.  
컴퓨터가 스스로 학습하여 성능을 향상시키는 기술방법으로, 머신러닝 중에서도 인간의 뉴런과 비슷한 방식으로 정보를 처리하는 기술을 딥러닝이라고 부른다.  
인공지능 안에 머신러닝이 있고, 머신러닝 안에 딥러닝이 있다.  
우리는 여러 공부를 하거나, 자전거를 타거나 등등 여러 실패를 거듭하며 오차를 줄여나가며 원하는 목표로 다가가는 학습을 한다.  
기계학습의 기초 원리는 이러한 사람의 학습에서 비롯되었다.  

인공지능은 컴퓨터에 대한 기대감으로 인해 탄생되었다.  
컴퓨터는 매우 뛰어난 능력을 갖고 사람이 어려워하는 일들을 쉽게 해내었는데, 그로 인하여 인간이 하기 쉬운 일도 잘 하지 않을까 하는 기대감이 있었다.  
그리하여 사람이 하는 일을 수행할 수 있는 인공지능이라는 분야가 등장했다.  
초창기엔 지식기반으로 분류하는 방식이 주였다.  
하지만 지식기반으로 하면 오류가 발생하기 쉽다.  
모든 장면을 인식할 때 우리가 왜 그렇게 인식하는지를 설명하려면 매우 어려울 것임.  

그래서, 사람의 지식을 기반으로 학습을 시키는 것이 아닌, 주도권을 인공지능에게 주어 데이터 중심으로 접근하도록 전환하기 시작했다.  
이것이 기계학습 방식인 데이터중심 접근방식이다.  

데이터가 들어왔을 때, 그 결과를 예측하기 위해서 훈련이 필요하고 훈련집합들로 훈련된 학습을 통해 새로운 데이터의 값을 예측한다.  
훈련집합은 가로축은 특징, 세로축은 목표치이다.(X와 Y)  
특징들과 목표치로 이루어진 훈련집합들은 얼핏 보면 직선을이루기에 기계학습의 모델로 직선을 택하는 편이다.  
직선이기에 모델의 수식은 y=wx+b의 형태로 w와 b 두개의 매개변수로 이루어진다.(1차식, 0차식)  

기계학습은 최적의 매개변수를 찾는 작업이다. 가장 정확한 예측을 위해서.  
처음엔 최적값을 모르기에 임의값에서 시작. 점점 성능개선되며 최적에도달  

학습을 하고 학습을 마치면 예측에 사용한다.  
궁극적인 목표는 훈련집합에 없는 새로운 샘플에 대한 오류를 최소화 하는 것이며,  
이런 성능을 <b>일반화능력</b>이라고 부른다.  

사람과 기계는 학습방법이 차이가 있다.  
- 학습 과정 : 사람은 능동적, 기계는 수동적  
- 데이터 형식 : 자연에 존재하는 그대로, 일정 형식에 맞추어 사람이 준비  
- 동시 학업 가능 과업 수 : 자연스럽게 여러 과업 학습, 하나의 과업만 가능  
- 학습 원리에 대한 지식 : 매우 제한적으로 알려짐, 모든 과정이 다 밝혀짐  
- 수학 의존도 : 매우 낮음, 매우 높음  
- 성능 평가 : 경우에 따라 객관적or주관적, 객관적(예를들면 정확률 99.8%)  
- 역사 : 수백만 년, 60년 가량  

### 특징공간  
특징이 x고 목푯값이 y라고 했는데, 이를 활용하여 x축 y축을 통해 특징공간을 나타내기도 한다.  
x가 d개라면 d차원 특징공간이라고 부를 수 있음.  
d차원 데이터는 X=(x1,x2,...,xd)^T 로 특징벡터를 표기할 수 있다.  
직선모델의 경우 매개변수의 수는 d+1개이고, 목푯값 y기준으로는 y=w1x1+w2x2+...+wdxd+b 로 나타낼 수 있다.  
2차 곡선모델 사용하면 매개변수수가 d^2+d+1개라서 매개변수 수가 크게 증가한다.  

### 데이터  
과학기술의 발전은 '데이터 수집->모델 정립->예측->데이터 수집...' 이렇다.  
기계 학습의 경우에는 수학공식을 통해 분류를 표현하기는 불가능하다.  
따라서 자동으로 모델을 찾아내는 과정이 필요하다.  

데이터가 어떻게 만들어지는지(예측값이 어떤 방식으로..) 완전히 아는 인위적 상황을 '데이터 생성과정을 완전히 알고 있다'라고 말한다.  
x를 알면 y를 예측할 수 있고 x의 발생확률 P(x)도 정확히 알 수 있다.  
하지만 실제 기계학습은 데이터의 생성과정(결과도출방법)을 알 수가 없다.  
그저 훈련집합 X, Y를 사용해 예측모델이나 생성모델을 근사 추정할 수 있을 뿐이다.  

데이터베이스가 중요한게, 위 같은 상황에서 훈련집합이 많아야 원하는 근사추정이 가능해질텐데,  
데이터베이스 품질이 좋다면 주어진 응용에 맞는 충분히 다양한 데이터를 충분한 양만큼 충전해서 추정 정확도가 높아진다.(<b>주어진 응용 환경을 잘 살피고 그에 맞는 데이터베이스 확보 아주 중요</b>)  
3가지 공개 데이터베이스는 Iris(꽃), MNIST(숫자), ImageNet(영상)  

### 데이터베이스  
데이터베이스는 솔직히 왜소한 크기이다.  
28*28의 흑백 픽셀은 2^784가지의 경우가 나오지만 정작 MNIST샘플은 6만개밖에 안됨.  
이런 작은 데이터베이스로 높은 성능을 달성할 수 있는 이유는,  
막상 큰 공간에서 실제 데이터가 쓰이는 곳은 매우 작은 부분임.  
또한 <b>매니폴드 가정</b>을 통해 샘플들이 고차원에 있어도 일정한 규칙에 따라 변화한다 보아, 샘플들을 아우르는 저차원공간이 존재할 것이라는 가정  

### 데이터 가시화..  
4차원 이상 초공간은한번에 가시화 안되는데.. 2개씩 조합해서 여러개의 그래프를 그린다.  

### 모델 선택  
기계학습에서 모델을 선택할 때 과소적합이나 과잉적합에 주의해야한다.  
말 그대로 과소적합은 너무 오차가 많은 상황으로, 모델의 용량이 '작을 때' 많이 발생한다.  
ex) 1차 모델을 사용했더니 너무 오차가 큼  
따라서 오차를 줄이기위해 2차 이상의 다항식 곡선을 사용한 비선형 모델을 사용하는 것이 대안이 된다.  
1차 모델에 비해 오차가 크게 감소한다는 장점.  
하지만 너무 큰 차수의 다항식 곡선을 택하면 오히려 너무 과잉적합하다는 문제가 생긴다.  
ex) 12차 다항식 곡선을 모델로 사용했더니 훈련집합에는 잘 맞지만 '새로운' 데이터를 예측하기엔 오히려 오차가 생겼다.  
용량이 너무 크면 학습 시에 잡음까지 담아버리는데, 이 때문에 오차가 생기게 된 것이다.  

요약하면, 과소적합하지도 않고 과잉적합하지도 않는 적절한 용량의 모델을 선택하는 작업이 필요하다.  
높은 일반화 능력을 갖기 위해서 적절한 타협점이 필요한데, 보통 3~4차가 좋은 일반화 능력을 보이기도...  

모델을 선택하게 되면, 바이어스와 분산이 나타나는데 낮은차 다항식은 바이어스가 큰 대신 비슷한 모델을 얻는다는 점에서 낮은 분산을 갖는다. (높은 바이어스, 낮은 분산)  
하지만 고차다항식은 오차가 작은 점에서 바이어스가 작은 대신 크게 다른 모델을 얻기에 높은 분산을 보인다. (낮은 바이어스, 높은 분산)  
이 둘은 서로 역관계이다.  
우리는 낮은 바이어스와 낮은 분산을 갖는 예측기를 필요로한다.  
<b>바이어스를 최소로 희생하도록 유지하며 분산을 최대한 낮추는 전략이 필요하다</b>  

모델은 종류가 다양하다. 1차~12차 다항식이 SVM(Support Vector Machine), 트리분류기, 신경망, 강화학습 등 방법으로 선택될 수 있다.  
모델 종류도 많은데 각각 장단점이 있기때문에 문제 상황마다 더 좋은 모델이 있을 것이다.  

현실에서는 '경험'을 사용하여 큰 틀을 먼저 택한다.  
이후 모델 선택 알고리즘으로 세부 모델을 선택하는 전략을 사용한다.  
현대 기계학습에서는 용량이 큰 모델을 택하고, 선택한 모델이 정상에서 벗어나지 않도록 규제(regularization)하는 기법을 적용한다.  
ex) 12차 다항식 택하고 적절한 규제 적용  

### 규제란...  
<b>데이터확대</b>  
일단 데이터를 많이 확대하면 할수록(훈련집합 많아짐) 일반화 능력이 향상된다.  
데이터수집에 많은 비용이 들기에, 이를 절감하기 위해 훈련집합의 데이터 샘플들을 변형시키며 인위적으로 데이터를 확대할 수 있다.  
(부류 소속이 변하지 않는 선에서 약간 회전하거나 늘리거나 찌부하거나)  
이 방법을 쓰면 큰 용량에서 사용할 수 있을 만큼 많은 훈련집합의 크기를 얻어낼 수 있다.  
<b>가중치 감쇠</b>  
가중치를 작게 조절하는 방식을 사용하여 마치 낮은차수의 곡선처럼 형태를 만드는 것이다.  
점차 개선된 목적함수를 사용하며 가중치를 작게 조절하는 규제기법이다.  

### 지도 방식에 따른 유형  
정리를 해보면 이와 같다.  
- 기계학습  
 - 지도학습 : 정답이 존재  
  1. 회귀 : 데이터를 잘 설명하는 선을 찾아 미래값 예측  
  2. 분류 : 주어진 데이터가 어떤 클래스에 속할지 예측  
 - 비지도학습 : 정답이 존재하지 않음  
  1. 클러스터링 : 그룹에 대한 정보없이 유사한 특징을 가진 개체끼리 군집화  
  2. 차형축소 : 고차원 데이터의 차원을 축소해서 데이터를 더 잘 설명가능하게 함  
 - 강화학습 : 보상과 벌점  

### 그 중에서...  
<b>지도학습</b>  
정답이 주어진 상태에서 학습하는 알고리즘인데,  
훈련 데이터로부터 하나의 함수를 유추하고 종류로는 회귀분석과 분류가 있다.  
<b>비지도학습</b>  
정답이 주어지지 않는 상태에서 데이터의 특성을 학습하는 알고리즘이다.  
데이터가 어떻게 구성되었는지 알아내는 문제를 해결  
지도학습, 강화학습과는 달리 입력값에 대한 목표치(y)가 없다  
군집화와 연관규칙이 여기에 속한다.  
<b>강화학습</b>  
보상이나 벌칙을 통해 여러번 시행착오를 거치며 스스로 학습하는 방법이다.  
분류할 데이터나 정답은 없고 단지 행동에 보상과 벌칙을 통해 학습한다.  
보상을 최대한 많이 얻도록 하는 행동을 유도하는 학습을 진행한다.  
아이가 일어나서 걷는 방식이나 알파고와 같은 방식이 그 예시이다.  


## 2주차 - 기계학습과 Python, 수학  
### Python?  
파이썬이 데이터 과학 분야에서는 표준 프로그래밍 언어이다.  
다양한 라이브러리가 존재하여 데이터를 적재하거나 시각화, 통계, 자연어처리, 이미지처리 등이 편리하다.  
터미널이나 주피터노트북 같은 도구로 대화하듯 프로그래밍이 가능하다.  
머신러닝과 데이터분석은 데이터 주도 분석이라는 점에서 반복작업이 근본적으로 필요로하고,  
이러한 반복작업을 쉽고 빠르게 처리할 도구가 필요하다.  
복잡한 GUI나 웹서비스 만들수있고 기존시스템과 통합하기도 좋아서 쓴다.  
사이킷런이라는 오픈소스. 사이킷런은 NumPy와 SciPy를사용. 그래프 그리기 위해 matplotlib를 대화식으로는 Ipython과 주피터노트북 필요  
필요한 패키지 모아놓은 파이썬 배포판 설치하는것 권장  

### 기계학습 위해 Python에 쓰는 라이브러리들  
- 주피터노트북 : 프로그램 코드를 브라우저에서 실행해주는 대화식 환경 제공  
- NumPy : 파이썬으로 과학 계산을 위한 패키지.  
다차원 배열 위한 기능, 선형대수 연산, 푸리에 변환 등 고수준 수학함수와 유사(pseudo)난수 생성기 포함.  
- SciPy(사이파이) : 과학 계산용 함수 모아놓은 패키지.  
고성능 선형대수, 함수최적화, 신호처리, 특수 수학함수와 통계분포 등 많은 기능  
- matplotlib : 대표적 과학 계산용 그래프 라이브러리.  
선그래프, 히스토그램, 산점도 등  
- pandas : 데이터 처리와 분석을위한 파이썬 라이브러리  

### 구글 Colab  
클라우드 기반 주피터 노트북 개발환경 제공  
브라우저에서 python 작성과 실행이 가능하고 별도 설치 필요도 없다.  
기본적으로 데이터 분석 위한 라이브러리가 설치되어있다.  
GPU 무료로 사용 가능하고 주피터 노트북과 유사하면서 더 좋은 기능 제공한다.  
깃과 연동되어 협업 코딩 가능  

### 기계학습과 수학
기계학습에서 수학은 중요하다.  
수학을 통해 목적함수를 구하고 이를 최적화한다.  
이렇게 구한 최적화된 목적함수가 알고리즘을 구성하는 기본적인 근간이기에, 알고리즘의 원리를 이해하기 위해선 필수적  
사람을 알고리즘을 설계하고 데이터를 수집한다.  
기계학습은 수학/알고리즘/사람이 수집하는 데이터로 이루어지는 것이다.  

- 선형대수학을 통해 어떻게 조사 대상을 형식화할 것인지 알게된다.  
(학습 모델의 매개변수집합, 데이터, 선형연산의 결합 등을 벡터나 행렬로 간결히 표현가능, 데이터 분석해서 유용한 정보 알아내거나 특징 공간을 변환수행)  
- 확률과통계를 통해 데이터의 특징을 알게된다.  
(데이터에 포함된 불확실성 표현하고 처리, 데이터의 특징들 추출하고 많은 데이터의 중요한 속성만 간추리는 역할, 모델설계에 활용)  
- 최적화이론을 통해 알고리즘을 이해하고 훈련과정을 최적화한다.  
(목적함수를 최소화하는 최적해 찾는데 활용. 주로 미분)  

### 선형대수  
샘플을 특징벡터로 표현한다.  
특징이 4개면 x1, x2, x3, x4를 갖는 벡터X로 표현  
여러개의 특징벡터가 존재하면 X1, X2, X3.. 이렇게 구분한다.  
행렬은 여러개의 벡터가 담긴 것이다.  
훈련집합을 담은 행렬을 설계행렬이라고 부르는데,  
이들은 행과 열로 이루어져있다.  
2x3의 행렬이라면  
11 12 13  
21 22 23 이렇게 표현한다.  
이들을 전치행렬하여 A=>A^T로 바꾸면,  
11 21  
12 22  
13 23 으로 변한다.  

행렬을 사용하면 수학이 간결하게 표현되는데,  
다향식이 만약 f(x.)=f(x1,x2,x3)이고,  
2x1x1-4x1x2+3x1x3+x2x1....-4x3+5 이런 식이였다면  
..........(2 -4 3)(x1)...........(x1)  
(x1 x2 x3)(1  2 6)(x2) + (2 3 -4)(x2) + 5  
..........(-2 3 2)(x3)...........(x3)  
= x.^TA.x.+b.^Tx.+c 로 나타낼 수 있다.  

3차원 이상의 구조를 가진 숫자 배열은 텐서라고 부른다. (3차원 구조의 RGB 컬러 영상)  

<b>놈, 유사도</b>  
벡터와 행렬의 크기는 '놈'이라는 것으로 측정한다.  
p차 놈을 구하는 방법은 x1부터 xd까지 절댓값을 구해 각각 p제곱승 해준값들을 모두 더하고, p제곱근을 구하면 된다.  
이때 최대놈은 각 xi들의 절댓값의 최댓값을 찾으면 된다.  
행렬에서의 놈은 프로베니우스 놈으로 구한다.  
각 항을 제곱하여 더해주고 전체에 루트를 씌우면 된다.  
유사도는 거리? 벡터를 기하학적으로 해석.  
코사인유사도(a,b)=(a/||a||)행렬곱(b/||b||)=cos세타  

### 퍼셉트론  
1958년 로젠블렛이 고안한 분류기 모델.  
x1,x2,...xd가 가중치 w1,w2,...,wd에 각각 곱해진값들을 모두 더한 T값을 활성함수에 넣어 분류.  
결정직선이 두개의 부분공간을 나누고, (결정직선은 w.에 수직이고 원점으로부터 T/||w||2만큼 떨어져있음)  
3차원 특징공간 = 결정평면, 4차원 이상은 결정 초평면  

퍼셉트론은 출력이 여러개가 되기도 한다.  
출력도 벡터로 나타내어 o.=(o1,o2,...,oc)^T로 표기한다.  
j번째 퍼셉트론의 가중치벡터는 w.j=(wj1,wj2,...,wjd)^T 로 나타낸다.  
행렬로 간단히하면... o.=활성함수(W.x.)  
가중치벡터를 각 부류 기준벡터로 간주하면 c개부류 유사도 계산하는거..?  

퍼셉트론을 통해 우리는 분류를 하고싶어한다.  
분류는 o.=활성함수(w.x.)중에 W.와 x.를 알고 o.를 알지 못한다.  
학습과정은 o.와 x.를 통해 훈련집합의 샘플을 잘 만족하는 W.를 찾는것이다.  
이렇게 구한 W.를 통해 새로운 테스트샘플의 o.를 예측하여 알아낸다.  

현대 기계학습은 퍼셉트론을 여러층으로 쌓는 딥러닝 기술을 이용하기에 현대기계학습에서 퍼셉트론은 기초로서 중요하다는 것을 알 수 있다.  

선형결합으로 만들어지는 공간을 벡터공간이라 함.  

### 역행렬  
A^(-1)A=AA(-1)=I.  
역행렬 조건. A는 역행렬 갖고, A는 최대계수 갖고,  
A는 모든행, 열은 선형독립이다.  
A 행렬식은 0이 아니고 A^TA는 양의 정부호대칭행렬이고 A고윳값은 모두 0이 아니여야!  
역행렬이 된다.  
(a b)                          (d -b)  
(c d) 의 역행렬은 ==> 1/(ad-bc)*(-c a) 이다.  

근데 수학부분 주로는 안나온댔음...  

## 3주차  
### 확률과통계, 최적화 이론  
베이즈 정리 P(y, x) = P(x|y)P(y)=P(x,y)=P(y|x)P(x) ==> P(y|x)=P(x|y)P(y)/P(x)  
이 식을 사용하면, 이런 문제 해결 가능  
하얀 공이 나왔는데 어느병에서 나온지 모른다. 어느병인지 추정.  

베이즈 정리에서 P(y|x)는 사후확률이고, P(x|y)는 우도, P(y)는 사전확률이다.  
베이즈 정리는 기계학습에 적용된다.  
사후확률을 직접 추정하는 것은 거의 불가능이다  
사전확률은 그냥 ni/N 하면 된다.  
즉, 우도를 잘 추정하면 분류에 도움됨  
근데... 우도도 샘플 데이터가 적으면 구하기 어렵다.  
그래서 우도가 정규분포를 따른다고 가정하고 평균벡터와 공분산을 사용하여 우도를 유추한다.  

엔트로피..? 주사위가 윷보다 엔트로피 높음.  

### 최적화  
최적화 문제 해결의 낱낱탐색 알고리즘은 차원이 조금만 높아져도 적용 불가능...  
4차원에서 1000구간으로 나누려면 1000^4해야함.  
무작위 탐색 알고리즘 => 아무 전략 없는 순진한 알고리즘  
기계학습이 주로 사용하는건 목적함수가 작아지는 방향을 미분으로 찾아냄  
이 방법에서 d세타로 -f'(x)쓰면 경사하강 알고리즘  


## 4주차  
### 베이시언 결정 이론  
x가 주어졌을 때 그것이 부류 wi에서 발생했을 확률을 <b>사후확률</b>이라고 한다.  
위에서 구했었던 베이즈 정리를 사용한다.  
그러면 우도와(P(x|wi)) 사전확률(P(wi))로 사후확률을 유추할 수 있다.  
분모에 P(X)도 있긴한데, 이 값은 비교할 두 부류가 공통으로 갖고있는 확률이기에 비교에서 무시해도 된다.  
사전확률은 P(wi)=ni/N 으로 구한다.  
훈련 집합에서 wi에 속하는 샘플들을 가지고 P(x.|wi)를 추정하는데, 우도를 계산하는것이다.  
부류조건부확률이라고도 한다.  

<b>최소 오류 베이시언 분류기</b>는 이러한 우도와 사전확률을 토대로 사후확률을 유추하고, 그 값들을 통해 더욱 그럴듯한쪽으로 분류하는 분류기이다.  
두 확률이 겹치는 부분의 영역이 오류 확률인데 이 수식에서 t값을 조절하는 것으로 분류에서의 오류확률이 바뀔 수 있다.  

<b>최소 위험 베이시언 분류기</b>는 오류를 성능의 기준으로 하기에는 오류가 나는 것이 위험성을 동반하는 경우가 있을때 사용한다.  
예를들어 암 환자를 정상인으로 분류하는 경우, 하품의 과일을 상품으로 분류하는 경우.  
이 경우 손실행렬이라는 행렬을 만들어서 부류 중 위험이 더 최소로하는 부류를 택하도록 한다.  

최소오류는 P(wi|x.) 혹은 P(x.|wi)P(wi)의 'max'를 구하고,  
최소위험은 qi의 'min'을 구한다. (qi=손실행렬이 포함된ji P(x.|wj)P(wj))

### 분별함수 
위에 있는 두 분류기를 합쳐서 분별함수로 작성할 수 있다. 최소오류는 max를 구하는것이기에 그대로 값을 넣고 최소위험은 min을 구하는것이기에 결과값을 1/q 로 해준다.  
분별함수 내의함수들은 주로 log를 사용한다.  
곱셈이면 덧셈으로 바꿔주고(수식 전개 유리), log취하면 값 규모 커져서 수치 오류에 둔감한 이점이 있다.  

### 베이시언분류기를 정규분포로...  
우도가 정규분포를 따른다고 가정해버리면 평균과 분산이라는 두 종류 매개변수만으로 표현이 가능해진다.  
우도에 관한 식을 로그취하고.. 그러면 분별함수 만들면  
2차식, 1차식, 상수 이렇게 3가지의 값 나온다..?  

### 결정경계  
gi(x)=gj(x)인 점, gij(x)=0인 점들을 이으면 결정경계가 된다.  

만약 공분산이 모든 부류에서 같은 경우에는 결정직선, 선형분리가 가능해지는데 이 식은 4주차 강의자료 25/33 페이지 참고  

임의의 공분산 행렬의 경우에는 
결정경계가 결정곡선으로 2차 다항식으로 정리된다.  
페이지 27p 참고.  

### 최소거리분류기  
얘는 말 그대로 결정경계로부터 최소거리를 구하는 분류기이다? 29페이지  
거리 척도 종류는 두가지가 있다.  
마할라노비스 거리, 유클리디언 거리.  
마할라노비스 수식에서 공분산빼면 유클리디언 거리이다.  

### 베이시언 분류 특성  
아무래도 비현실성이 좀 있다. 실제 확률분포와 정규분포는 차이가 있다. 일반적인 확률분포 쓰면 차원의 저주 발생.  
이런 비현실성만 빼면 베이시언분류기는 오류율 측에서 최적이다. 수학적 증명도 가능  
베이시언 분류기는 각각 M개 부류에 대해 속할 확률도 출력 가능하다. 이걸 후처리에 사용 가능  

서로 특징들이 독립이라면 나이브 베이시언 분류기라는 것도 사용함.  차원의 저주는 피하지만 독립이 아니라면 성능이 급격히 안좋아진다.  


## 5주차  
### 확률분포 추정  
위에서 우리는 우도 P(x.|wi)를 추정하는 과정이 있었다.  
추정하는 방법도 여러가지가 있는데, 그 종류들을 본다.  
단일분포가 아니면 <b>혼합모델</b>, 단일분포면 다음단계  
x가 연속이 아니면 <b>히스토그램</b>, 연속이면 다음단계  
모수적 분포가 맞다면 사전확률을 고려하는지 보고, 아니면 창의 크기를 고정하는지를 본다.  
사전확률을 고려하면 <b>MAP</b>이고, 고려하지않으면 <b>ML</b>이다.  

### 히스토그램  
각 차원을 s개 구간으로 나누면 총 s^d개의 빈이 생긴다.  
전형적인 차원의 저주이다. N은 충분히 크고 d는 작아야함...  

### 최대우도 - ML, MAP 방법  
최적화 식을 로그로 바꾸고, 이분을 활용해서 최적화 문제 풀이.  
P(세타)가 균일하다면 ML 방법,  
P(세타)가 균일하지 않다면 MAP 방법 사용.  

이렇게 확률분포가 매개 변수(모수)로 표현되는 것에 ML, MAP 방법 등이 있고 모수적 방법이라고 한다.  
비모수적 방법에는 확률 분포가 임의의 형태거나, 파젠창, k최근접 이웃 추청 방법을 한다.  

### 파젠창, k-최근접이웃추정방법 (비모수적)  
<b>파젠창</b>  
히스토그램 방법을 확장해서 pdf(확률 밀도함수)를 추정한다.  
창의 크기를 h로 정하고, 임의의 점 x에서의 확률값을 추정한다고 가정할 때 x에 크기 h인 창을 씌운다.  
그리고 그 창 내에 있는 샘플의 개수를 k라고하여  
P(x)=(1/h) * (k/N) 이다.  
여러 차원의 경우 P(x)=(1/h^d)*(kx/N) 이다.  

계단함수를 사용하면 샘플들의 확률밀도함수를 추정해도 불연속적인 함수가 나타난다.  
따라서 창 안 샘플에 가중치를 두어 중앙에 가까워질수록 더 높은 가중치를 갖게한다면 매끄러운 함수를 가질 수 있을 것이다.  
바로 커널함수의 가용이다.  
kx의 값을 시그마(i=1부터N까지)K((x-xi)/h) 로 구하고 파젠창의 P(x)식에 대입하면 된다.  

하지만... 히스토그램을 기반으로 설계되었기에 여전히 차원의 저주에서 자유로울 수 없다.  
실제값과 가까워지려면 창은 작고(h는 작고) 샘플의 개수는(N의 개수) 엄청 많아야한다.  

<b>K-최근접 이웃 추정 방법</b>  
파젠창에서는 h가 고정되고 k값이 가변적으로 이루어졌다.  
K-최근접 이웃 추정 방법에서는 k값(창에 들어오는 샘플의 개수)는 고정적으로 주어지고,  
창 내에 k개의 샘플이 들어올 때까지 h값을 늘리며 창을 확장하는 방식이다.  
따라서 식은 P(x)=(1/hx^d)*(k/N) 이 된다.  
직접 선에 그려보고 하는거 추천..?  

<b>K-NN분류기</b>  
k최근접을 이용한 분류기이다.  
x를 중심으로 창을 씌우고 k개 샘플 들어올때까지 확장. 이떄 창의 크기를 hx라하면 창부피는 hx^d일것이다.  
창 안 샘플 중에 wi에 속하는 것 개수를 ki라고하면..  
P(x.|wi)=ki/hx^d*Ni  
P(wi)=Ni/N, P(x.)=k/hx^d*N  
P(wi|x.)는 위 식 대입하면 ki/k 이다.  
창 안의 k개 샘플 중 wi에 속하는 것 개수의 비율을 구하는 것이다. 결국 ki값 제일 큰걸로 분류  
[x.를 wq로 분류. q=argmaxki (ki값의 최대)]  

### 혼합모델  
서로다른분포를 혼합하여 모델링하기도 한다.  
이렇게 다중모드분포의 경우에는 가우시언 혼합을 사용한다.  
두 가우시언을 합치고, 각 가우시언에 가중치를 주어 적합한 분포를 택하면 된다.  
P(x.)=가중치*N(평균벡터1,공분산1)+가중치*N(평균벡터2, 공분산2)  


## 6주차  
### 신경망, 퍼셉트론  
기계학습 역사에서 가장 오래된 기계학습모델. 다양한 형태가짐  
컴퓨터과학과 의학의 시너지를 보이는 기술  
컴퓨터 : 폰 노이만 컴퓨터, 순차 명령어 처리기  
두뇌 : 뉴런, 고도의 병렬 명령어 처리기  

신경망은 다양한 모델이 존재한다.  
전방신경망, 순환신경망  
얕은신경망, 깊은신경망  
결정론신경망, 스토캐스틱신경망  

퍼셉트론은 노드, 가중치, 층과 같은 새로운 개념 도입. 학습 알고리즘 창안  
원시적 신경망이지만 딥러닝을 포함한 현대 신경망은 퍼셉트론을 병렬순차구조로 결합한 것.  
퍼셉트론은 현대 신경망의 중요한 구성 요소이다.  

### 퍼셉트론  
퍼셉트론은 입력층과 출력층을 갖고, 입력층은 연산을 하지 않기에 퍼셉트론은 단일층구조라고 할 수 있다.  
바이어스(시작)노드는 항상 1이 입력된다.  
출력층은 한개의 노드이고, 특징벡터들과 가중치들을 각각 곱한것을 더한값이 활성함수에 입력되고, 계단함수를 통해 분류가 된다.  
계단함수(s)값은 s>=0이면 1, s<0이면 -1이다.  
s=w1*x1+...+wd*xd + w0 이다.  
벡터로 s=w.^T*x. + w0 이라고 표현 가능하다.  

### 퍼셉트론 결정직선  
결정직선은 부류들을 분류하는 직선이기에, 우리는 일단 계단함수로 -1혹은 1로 분류중이다.  
그래서 d차원에서 w1*x1+w2*x2+...+wd*xd +w0 값이 0이되는 점들을 이으면 그것이 바로 결정함수?가 된다.  
2차원의 경우 w1, w2로 직선의 방향을 정하고 w0가 절편을 결정한다.  
2차원-결정직선, 3차원-결정평면, 4차원이상-결정초편면  

### 퍼셉트론 목적함수  
퍼셉트론은 매개변수 w.을 갖고 이들로 목적함수를 나타낸다.  
J(세타)나 J(w.)로 표기하고,  
세가지의 조건을 갖는다.  
1. J(w.)>=0 이다.  
2. w.가 최적이면, 모든 샘플을 맞히면 J(w.)=0이다.  
3. 틀리는 샘플 많은 w.일수록 J(w.)는 큰값을 가진다.  

J(w.)=시그마(w.가 틀리는샘플 집합) -yk(w.^T*xk.)  
J(w.)가 0이되는 점이 최적이라고 하였으니,  
편미분을 사용하여 그레이디언트를 계산한다.  

시그마(w.가 틀리는 샘플) -yk*xki 이고 i가 0~d까지라면..  
내리막 경사법으로 인해 다음 wi는  
새로운wi = wi+로우(가중치)*시그마(w.가 틀리는 샘플) -yk*xki 가 된다.  
행렬표기를 하면 편리하게 표기가능  
만약 선형분리가 안되면 코드가 무한반복하기 때문에 until(더이상개선이없다면)으로 코드를 설계해야한다.  

x0(첫번째바이어스)는 언제나 1인 점을 기억하자.  

### 다층 퍼셉트론  
퍼셉트론이 입력층->출력층이였다면, 다층 퍼셉트론은 입력층과 출력층사이에 은닉층을 두어 원래의 특징공간을 분류에 유리한 new특징공간으로 변환하는 것이 특징이다.  

퍼셉트론은 계단함수를 활성함수로 사용한 반면,   
다층 퍼셉트론은 시그모이드 활성함수를 도입한다.  
계단함수는 딱딱한 의사결정인데 시그모이드는 유연한 의사결정이 가능하다.  
출력이 연속값이기에 출력을 신뢰도로 간주하여 더욱 융통성있는 의사결정이 가능해진다.  

다층퍼셉트론은 오류 역전파 알고리즘을 사용하는데, 여러층이 순차적으로 이어진 구조에서 역방향으로 진행하며 한번에 한층씩 그레이디언트를 계산하고 가중치 갱신하는 방식으로 점점 더 개선..  

다층 퍼셉트론의 특징으로, 퍼셉트론 2개를 병렬로 결합하여 새로운 특징공간으로 변환할 수 있는데, 이 곳에서 하나의 선으로 선형분리가 가능하기에 분류가 간단히된다.  

일반화하여 p개의 퍼셉트론이 결합하면 p차원 공간으로 변환되는데, 분할되는 양은 1+시그마(1~p)개 이다.  
예를들어 퍼셉트론이 3개라면 1+(1+2+3)=7개 영역  

모델마다 활성함수는 다들 다르다.  
딱딱한 의사결정을 하는 계단함수 (영역을 점으로 변환),  
부드러운 의사결정을 하는 여러 함수 (영역을 영역으로 변환) : 로지스틱시그모이드, 하이퍼볼릭탄젠트시그모이드, softplus나rectifier  

퍼셉트론은 계단함수, 다층 퍼셉트론은 로지스틱시그모이드와 하이퍼볼릭탄젠트, 딥러닝은 ReLU를 주로 사용한다.  

로지스틱 시그모이드와 하이퍼볼릭 탄젠트는 a가 커질수록 계단함수에 가까워진다.  
모두 1차 도함수 계산이 빠르다.  

### 다층 퍼셉트론 구조와 특성  
입력노드는 d+1개이다(특징 개수), 출력노드는 c개이다(부류 개수), 은닉노드는 p개이다(사용자 지정 매개변수. 하이퍼매개변수)  
p가 너무 크면 과잉적합이고 너무 작으면 과소적합이다.  

입력층과 은닉층을 연결하는 U1 행렬..  
은닉층과 출력층을 연결하는 U2 행렬..  
가중치 행렬들임  
입력층을 0번째 은닉층으로 봄. 출력층을 마지막 은닉층으로 봄.  

x.입력에대한 출력결과 o.는 이렇게 정의한다.  
2층은 o.=f(x.)=f2(f1(x.)) 이다.  
d개 은닉층은 o.=f(x.)=fd(fd-1...(f2(f1(x.))))  
층이 4개 이상이면 딥러닝으로 분류한다.  

다층 퍼셉트론은 실용적으로 좋은 성능.  
하지만 잡음섞인 상황에서 음성인식성능 저하..  
필기 주소 인식 능력 저하..  
바둑에서의 한계 등 한계가 있었다.  
but 딥러닝은 이들을 극복하였다!  

다층 퍼셉트론의 경우 매개변수 설정시  
일반적으로 적용되는 보편적 규칙은 없다.  
경험과 실험을 통해 설정해야함  
신경망 성능이 매개변수에 아주 민감하지는 않기에 어느정도 실험과 경험을 해도 된다.  


## 7주차  
### SVM (Support Vector Machine)  
오류율을 최소화하던 기존 분류기와는 다르게,  
SVM은 '여백'을 최대화하여 일반화 능력의 극대화를 노린 방식이다.  
신경망은 부류들을 분류하는데에 성공하면 그곳에서 멈춘다. 하지만 SVM은 최대 여백을 찾는다.  

여백을 최대로 하는 개념을 어떻게 공식화?  
결정초평면은 여백을 어떻게 최대로함?  

임의 점 x.에서 초평면까지의 거리는  
h=|d(x.)| / ||w.|| 이다.  

우리가 결정직선을 찾을때 d(x.)=0 인 점들을 모아 직선으로 한다. 함수가 d(x.)=0을 만족하기만 해도 된다면 식에 모든 항들에 같은 수를 곱하든 나누든 상관없음. 다 똑같은 함수니까.  
직선방향(w.)이 주어져있으면 두 부류와 결정직선과의 거리가 같게되는 b(절편)을 정한다.  
여백은 이렇게 구한 직선과 가장 가까운 샘플까지의 거리*2 (2배)로 정의함. 가장 가까운 샘플들은 <b>서포트 벡터</b>라고 부른다.  

위에서 초평면까지의 거리를 공식화했으니,  
여백은 공식화하면  
여백 = 2*h = 2*|d(x.)| / ||w.|| 이다.  

이제 공식화하면 w.^T*xi + b >= 1이면.. 부류 w1 (결정직선 기준으로 +이면?)  
w.^T*xi + b <= -1 이면.. 부류 w2  
2/||w||를 최대화해라.  

라그랑제 승수방법 기반으로 수식정리 가능  


### SVM 서포터 벡터 머신 공식들 모음 (중요!!)  
SVM에서 중요한 점...  
서포터벡터 2개가 사실상 중요하고 나머지는 별로 쓸모 없다! (영향을 안끼침)  

> w.= 시그마(1~N)ai*ti*xi  
ai(ti(w.^T*xi.+b)-1)=0, i=1,...,N  

선형분리 서로 같은거리로 떨어져있는 직선은  
> a1*t1+a2*t2+...=0인 점들의 집합이다.  

여러개명 그중 두개의 샘플을 골라서 걔네만 남겨두고 여백의 최댓값을 구하면 된다.  
> 여백 = 2*h = 2*|d(x.)| / ||w.||  

> 결정직선 d(x.)=w1*x1 + w2*x2 + ... + b(w0*1)  

w.^T*x.+b 는 결국 d(x.) 잖아? t*d(x.)이네.  
샘플이 들어왔을때 분류는 세가지 상황으로 나뉜다.  
1. 분할 띠 바깥 (1<= t(w.^T*x.+b)를 만족)  
2. 분할 띠 안쪽인데 자기가 속한 부류 영역. (0<= t(w.^T*x.+b) < 1)  
3. 결정경계 넘어 자신이 속하지 않은부류 영역에 놓임. t*d(x)값이 < 0 임  

위에 있던 문제들을 공식화하면
여백을 크게하면서 동시에 2, 3번 상황의 수를 최소로하는 것(1번 상황)이 목표임!! 이를 공식화?  
J(w,s.) = 1/2*||w||^2 + C*시그마(1~N)si.  
목적함수는 0이 최적이니까... 첫번째항은 목적1, 두번째항은 목적2  
> ti(w.^T*xi. + b)>=1-si., i=1,...,N  
si.>=0, i=1,...,N  

라그랑제 승수로 풀면..  
시그마(1~N)ai*ti = 0  
0<=ai<=C, i=1,...,N  
L(a.)=시그마(1~N)ai - 1/2*시그마(i=1~N)*시그마(j=1~N)*ai*aj*ti*tj*x.i^T*x.j  
0<=ai 가 0<=ai< C 로 바뀌었을 뿐.  

빡세네.  
           


## 8주차  