#  기계학습 기말고사 공부  
## 10주차  
### 특징 생성  
필기 숫자 인식과 같은 상황에서 특징 추출을 필요로 한다.  
필기숫자가 적힌 이미지의 크기를 정규화하여 맞추고, 이를 검은색 부분(글자)과 흰색 부분(배경)으로 1과 0을 사용하여 이진화 시킨다.  
이렇게 특징을 추출해낼 수 있는데, 추출 방법은 다음과 같다.  
1. 8x8의 64개의 픽셀을 모두 특징으로 삼아서 64차원의 특징 벡터로 저장하는 방법  
2. 가로 이등분, 세로 이등분을 하여 검은 화소가 몇개 들어있는지 비율을 얻어내어 특징으로 삼는다. 행(세로 2등분) x1=6/18  
열(가로 2등분) x2=14/10  
3. 그 외에도 여러 방법들이 있다.  

<b>특징이 우수한가 판단하는 기준</b>  
- 분별력 : 특징이 어느 한쪽으로 치우치지 않고 좋은 분별력을 가지는가  
- 차원 : 낮은 차원으로도 더 분류를 잘 하는 특징인가  

실제 세계에 있는 정보(Analog)를 패턴을 통해 특징 벡터라는 수학적 표현으로 변환한다.  
이렇게 특징벡터로 어떤 패턴으로 추상화할지는 정해진 것이 아니기에 동일한 데이터라도 특징은 각양각색이 될 수 있다.  
- 보통 특징 추출은 외부환경에 맞게 설계한다.  
숫자와 한글은 다른 특징이 필요할 수 있다는 것, 정면 얼굴만 인식하는 경우와 제약 없이 얼굴 인식하는 경우가 다른 특징이 사용될 수 있다는 것이 그 예시.  

### 특징 추출
특징을 생성하기 위해서는 먼저 특징을 추출해내야 한다.  
실제세계의 정보를 특징벡터로 생성하기 위해 특징을 추출하는데, 그 과정은 센싱을 통해 얻는다.  
센싱에서 얻을 수 있는 신호는 다양한데,  
대표적으로 <b>영상 / 시간성 신호 / 측정 벡터</b>  
이렇게 세가지의 신호를 얻을 수 있다.  
또한 특징은 다양한 상황으로 생성될 수 있다.  
- x.=s. 처럼 센싱된 신호 자체를 특징벡터로 지정할 수도 있고,  
- x.=e(s.) 처럼 특징을 추출하여 특징벡터로 지정할 수도 있고,  
- x.=s(s.) 처럼 특징을 추출한 것 중 마음에 드는 것을 선택하여 특징벡터로 선택할 수도 있고,  
- x.=s(e(s.))처럼 센싱된 신호에서 특징을 추출하고 그 결과 중 마음에 드는 것을 선택하여 특징벡터로 선택할 수도 있다.  

### 주성분 분석 (PCA)  
주성분 분석 : Principal Component Analysis  
훈련집합을 이용해서 매개변수를 추정하고 그것을 통해 특징을 추출하는 것을 의미한다.  
<b>'정보 손실을 최소화하는 조건에서 차원을 축소한다'</b>  
이러한 변환을 Karhunen-Loeve(KL)변환, Hotelling변환 이라고도 부른다.  

<b>주성분 분석을 하는 목적</b>  
정보손실을 최소화하면서 신호 s.를 낮은 차원의 특징벡터 x로 변환하기 위함.  
D차원의 신호s.를 d차원의 특징벡터x.로 변환한다.  
변환행렬 U.는 dxD행렬이다.  

해결해야 하는문제는  
1. 어떻게 차원축소를 표현할지  
2. 정보손실을 어떻게 수량화하여 최소화할지.  

- 차원 축소 표현 : D차원 단위벡터를 u.^T축으로 투영하여 축소한다.  
예를들면 2차원의 s1,s2축으로 이루어진 s.를 임의의 u.^T=(1,0) 벡터를 기준으로 s.에 있는 원소들과 직각이 되는 점을 새로운 1차원의 특징벡터로 나타낸다.  
투영변환된 샘플 ^x = u.^T*s. 이다.

- 정보 손실 공식화 : 원래의 훈련집합이 갖고있는 정보는 샘플들 간 거리, 상대적인 위치 정도이다.  
PCA의 경우에는 샘플들이 원래 공간에 <b>'퍼져있는 정도'</b>를 얼마나 변환된 공간에서 잘 '유지'하는지를 척도로 한다.  
따라서 이를 공식화하면 변환된 샘플들 중 분산을 최대로 하는 축(단위벡터 u.)을 고르는 것이 정보 손실을 줄일 수 있는 공식이라고 할 수 있다.  
> 분산은 샘플들의 평균을 구하고 모든 샘플의 (x-평균)*(x-평균)^T 한 것의 1/n값을 구하면 된다.  

이러한 분산이 가장 큰 공간을 택한다면 차원축소가 그나마 가장 잘 된 경우를 택할 수 있을 것이다.  

분산을 최대화하는 u.를 찾으면 되기에,  
L(u.) 라그랑제 함수를 통해 라그랑제승수가 있는 수식을 미분하고 정리하면  
<b>공분산행렬 * u. = 라그랑제승수 * u. 가 나온다</b>  
즉, 훈련집합의 공분산 행렬 시그마를 구하고 그것의 고유벡터를 구하면 그게 바로 최대분산을 갖는 단위벡터 u.가 된다.  

만약 예로 특정 훈련집합과 그에 관한 공분산 행렬이 존재한다면, 라그랑제승수가 가장 큰 고유벡터를 최대분산을 갖는 u.로 생각하면 된다.  
그렇게 구한 u.^T에 s.를 곱하여 변환된 특징벡터 x.를 구할 수 있다.  

### 변환행렬  
변환행렬을 어떻게 하는지에 따라서 차원축소를 자유롭게 할 수 있다.  
D개의 고유벡터(D차원)가 d개의 고유벡터(d차원)으로 변환된다.  
고유값이 큰 순서대로 d개의 고유벡터를 취한다.  
U. = (u.1^T, u.2^T, u.3^T, ... u.d^T)  
실제로는 x. = U.s. 을 하고 이를통해 d차원으로 변환한다.  

> 구하는 순서는 s.의 집합 X.의 평균벡터를 구하고, 공분산 행렬 시그마를 구한다.  
시그마의 고유벡터와 고유값을 구하고, 고유값을 기준으로 삼아 가장 큰 d개의 고유벡터를 선택한다.  
이렇게 선택된 u1., u2., ... ud.를 통해 변환행렬 U.를 만든다.  

고유값이 작은 u.를 선택했을 때와, 큰 u.를 선택했을 때의 결과를 비교하면 고유값이 작은 u.을 택할때 투영된 점들의 분산값이 더욱 작아지고, 반대로 큰 u.를 택하면 분산값이 커진다.  

### Fisher 선형 분별  
PCA원리와 유사한 분류기 설계이다. 특징추출이 아님. PCA와 목표가 다르다.  
- PCA : 정보 손실을 최소화하며 차원축소 (샘플 부류정보 사용X)  
- Fisher LD : 분별력을 최대화 (샘플 부류정보 사용O)  

축을 기준으로 투영하는것까지는 동일하다. 하지만 분산값을 통해 구분하는 것이 아닌, 여러 축 중에서 가장 기존 분류 결과를 잘 유지하며 차원축소를하는 것을 목표로 한다. (분별력 최대화; 샘플부류정보 필요함)  

**목표 (분별력 공식화)**  
같은 부류는 모여져있고 다른 부류는 멀리 떨어진 것이 가장 이상적인 형태이다.  
w1끼리는 모이고(부류 내 퍼짐은 낮음) w1와 w2는 멀리한(부류 간 퍼짐은 높음) 형태.  
부류 내 퍼짐은 최소화 (평균s1^2 + 평균s2^2 최소화)  
부류 간 퍼짐은 최대화 (평균si^2 = 시그마(y-mi)^2) = |m1-m2| = |w.^Tm1-w.^Tm2| = |w.^T(m1-m2)|

따라서 목적함수 J(w.)는  
J(w.) = 부류간퍼짐/부류내퍼짐 이고, J(w.)를 최대화하는 w.를 찾으면 된다.  
정리하여 다시쓰면 J(w.)=w.^TSbw. / w.^TSwW.  
Sw = S1+S2 이고, Si = 시그마(x.-mi.)(x.-mi.)^T  
구하고자 한 최적의 축은 **w.=알파 Sw^(-1)(m1-m2)**  
알파는 그냥 정해도 되는데 w.의 크기를 조절할 수 있다.  
w.를 1로 만들기 위한 값을 넣는 경우가 많다?  
예를들어 a'(1,4)^T = w. = 1이 되려면..  
a'^2 + (4a')^2 = 1, 17a'^2=1 , a'=1/루트(17)  
w.=(1/루트17, 4/루트17)^T  

### 실용적 관점  
특징 추출은많은 경험을 통해 시행착오를 필요로 하는 단계이다.  
따라서 특징 추출을 위한 기계학습까지 하는 상황. 그 학습을 위해 또 학습을 그 학습을 위해 또 학습을..  

- 특징이 불만족스럽다면 버릴줄도 알아야한다.  
혹은 기존 특징에 새로운 특징을 추가하는 방법으로 특징을 결합해야한다.  
- 특징이 거리개념이 없는 경우도 있다.  
혈액형처럼 거리개념이 없다면, xi를 xi={xi1, xi2, ..., xin}처럼 몇개의 값을 갖는지에 따라 확장시키고, 하나만 1을 갖고 나머지는 0을 갖는 형태로 거리개념처럼 만들어준다.  
예시로 혈액형이 A,B,O,AB이니 xA={1,0,0,0}, xB={0,1,0,0}, ... 이렇게 만들어서 분류해준다.  

특징마다 동적범위가 다른경우가 많다. 예를들어 키는 160-180인데, 몸무게는 40-100으로 큰 차이가 난다고 해보자. 이런 경우 이 둘의 영향력을 같게하기 위해 정규화를 거칠 수 있다.  
- 선형 변환  
new xi = lowi + (highi-lowi)/(maxi-mini) * (xi-mini)  
- 통계에 의한 변환 (평균은 0, 표준편차는 1을 갖도록 정규화)  

이렇게 정규화를 거치면 거리 계산할 때 특징마다 동적범위를 신경쓰지 않고 할 수 있게된다.  


## 11주차  
### 특징선택  
여러개의 특징을 추출하였다면, 그 중에서 분류에 유용한 특징을 찾아내어 선택하는 것이 중요하다.  
특징선택은 추출해낸 기존 특징벡터에서 **쓸모없거나**, **중복성이 강한**특징을 찾아 제거하는 작업이라고 할 수 있다.  
이 과정으로 차원을 낮추어주고 그 덕에 계산속도 향상과 일반화 능력 증대효과를 기대할 수 있다.  
<img src='./FeatureSelection.png' width='50%'>  

직관적으로 이해해보면 부류내 분산은 작고, 부류간 분산이 크다면 가장 좋은 경우일 것이다.  
반면 부류내 분산은 크고 부류간 분산은 작다면 값들이 섞이기 쉬워 좋지 않은 상황일 것이다.  
그리고 최악은 부류간 분산이 0에 가까워서 분별력이 거의 없는 경우일 것이다.  
이처럼 부류내분산과 부류간분산이 끼치는 영향은 정말 크다.  
**부류내 분산은 작고 부류간 분산이 큰 특징벡터가 좋은 특징벡터이다.**  
> 웬만하면 직관적으로 시각적표현을 보고 이해할 수 있지만 매우 높은 차원 데이터는 특징공간을 시각적으로 표현하기 어렵다. 대표적인 예시가 얼굴인식 혹은 필기문자인식이다.  

분별력 (discriminator power) / 부류분리 (class separation)  
우리는 좋은 특징벡터(부류내분산 작고 부류간 분산 큰)를 찾기 위해서 좋고 나쁨을 수량적으로 평가할 수 있는 기준함수가 필요하다.  

이를 평가하기 위한 척도로 사용하는 세 가지가 있다.  
- 다이버전스 : 확률분포 간 거리를 이용하여 분포간 거리가 멀수록 좋은 특징임을 안다. 이를 위한 거리측정은 KL다이버전스를 활용한다.  
부류 2개 : dij = KL(p(x.|wi), p(x.|wj)) + KL(p(x.|wj), p(x.|wi))  
부류 M개 : d = 시그마(i=1에서M)시그마(j=1에서M)P(wi)P(wj)dij  
다만 확률분포를 알고있어야만 적용이 가능하다. 또한 차원의 저주 문제를 이어받는다.  
- 훈련 샘플의 거리 : 다이버전스보다 비교적 현실에서 적용이 쉬운 경우이다.  
훈련집합에 있는 샘플을 가지고 직접 측정을 한다. 훈련집합에 있는 특정 부류 내 샘플들 각각을 기준으로 다른 부류 내 샘플들과의 거리를 전부 더하고 두 부류의 샘플 개수끼리 곱한만큼으로 나누어 부류간 거리의 평균을 구하는 방법이다.  
식으로 쓰면 dij = 1/NiNj * 시그마(k=1에서Ni)시그마(m=1에서Nj)dist(xi.^k, xj.^m) 이다.  
거리측정은 마할라노비스 혹은 유클리디언 거리 측정방식을 사용한다.  
- 분류기 성능 : 선택한 분류기(예를들면 SVM)가 있으면 특징벡터 x를 그 분류기로 평가한다.  
훈련집합을 분류기로 학습하고, 검증 집합을 통해 성능을 측정하는데 이 성능을 벡터의 분별력으로 사용한다.  
'분류기 성능'을 기준으로 좋은벡터를 찾으면 사용하고자하는 분류기에 적합한 특징벡터를 찾을 수 있기에 좋은 결과를 기대할 수 있다.  
하지만 새로운 특징벡터가 추가될 때마다 분류기를 훈련해야한다는 단점이 존재한다.  
이는 계산에 있어서 상당한 시간을 소요한다는 것을 의미한다.  
새로운 특징벡터가 자주 추가되는 상황이면 그닥.. 아니면 괜찮


우리는 위에서 특징선택을 위한 작업으로 쓸모없는것 제거, 중복성 강한 특징 제거를 언급했었다.  
중복성을 제거한다는것은 예시를 들면 강아지와 고양이를 구분하는데에 눈과 코의 개수 같은 특징들은 두 동물을 분류하는데에는 도움이 되지 않기에 쓸모가 없다.  
또한 같은 기능을 하는 두 특징이 있다면 두 기능을 합쳐서 하나만 존재해도 될 것이다.  
이와 같은 특징선택 과정은 직관이 필요하기에 실제 상황에서는 사용이 힘들다. 따라서 효과적 알고리즘이 필요하다.  

특징선택은 원래의특징집합을 부분집합생성기를 통해 특징들의 다양한 조합을 만들어 분별력측정기로 점수를 측정하고, 그 중 가장 높은 점수를 얻는 **최적부분집합을** 가져온다. 그 집합이 바로 선택된 특징들이다.  
특징이 3개라면 {x1}, {x2}, {x3}, {x1,x2}, {x1,x3}, {x2,x3}, {x1,x2,x3} 이렇게 7개 (2^3 -1)의 부분집합이 만들어 질 수 있다.  
이 중 가장 높은 점수를 갖는 부분집합이 바로 '선택된 특징벡터'이다.  

위같은 조합적최적화는 지수승을 갖기에 차원d가 커질수록 걸리는 시간은 방대하게 늘어난다. 부분집합 하나의 평가가 1/1000초가 걸려도 특징벡터가 10개일때 1초가 걸리던 평가가 30개일때 12일이 걸리게 된다.  
그렇기에 모든 경우를 전부 탐색하는것 외에 다른 알고리즘을 찾게 된다.  
- 임의 탐색 알고리즘 : 일정 시간동안 랜덤으로 부분집합을 생성하고 평가하여 가장 좋은 점수를 갖는 부분집합을 택하는 것
- 개별 특징 알고리즘 : 각 훈련샘플들 개별적으로 점수를 먼저 측정한다. 만약 특징이 3개라면 J(x1), J(x2), J(x3)를 계산함. 그렇게 계산되는 값들 중 가장 높은 값을 갖는 훈련샘플들을 택하여 부분집합으로 만드는 방식이다.  
부분집합의 크기가(부분집합 특징이 몇개인지) 얼마나 되는지만 알면 최선의 훈련샘플들을 i개 골라서 특징선택을 할 수 있다.  
하지만 이 알고리즘은 특징 간의 상관관계를 전혀 고려하지 않고있고, 특징의 중복성을 무시한다는 단점을 갖고있다.  

> 그럼에도 간단히 사용하기 쉬운 알고리즘이고, 상관관계가 좀 있어도 어느정도 보완도 된다. 중복을 제거 못하는것이 가장 치명적인 이유  
**전역 탐색 알고리즘**   
- 낱낱 탐색 알고리즘 : 가능성 있는 모든 해를 평가하는 무식한 방법.  
항상 전역최적해를 보장하지만 위에서 언급한대로 해를구하지 못한다든 한계가 존재한다.  
- 한정 분기 알고리즘 : 전역탐색알고리즘의 낱낱탐색알고리즘 강화판이다.  
모든 가능성있는 해를 평가하는 낱낱탐색알고리즘에서 탐색하면서 도중에 가능성이 없다고 판단되는 영역을 아예 배재하여 효율을 높이는 방식의 알고리즘이다.  
차원수가 낮아질수록 점수가 낮아지는데, 2차원 분기 점수보다 3차원에서 점수가 더 낮으면 그쪽 분기의 3차원 아래 분기로는 갈 이유가 없다.  
이미 탐색했던 분기 중 2차원에서 얻은 최대점수 분기보단 최소한 3차원에서 더 높은 점수를 갖는 분기를 탐색해야한다는 생각.  

전역탐색알고리즘은 위와같이 다소 무식한 방법이라 시간이 오래걸린다.  
따라서 순차 탐색 알고리즘이라는 것도 존재한다.  
**순차 탐색 알고리즘**  
- SFS 알고리즘 (sequential forward search algorithm) :  
한번에 하나씩 특징을 추가하는 알고리즘. 공집합을 하나 만들고, 기존특징집합에서 점수가 가장 높은 특징벡터를 하나씩 공집합에 추가해나가며 점수가 가장 높은 부분집합의 경우를 찾는 방식  
[d1,d2]로 부분집합의 개수가 정해지는데, 이건 범위를 나타낸다.  
d개 특징벡터가 d1차원부터 d2차원까지의 결과 중 가장 점수가 높은 조합으로 선택하는 알고리즘이다.  
- PTA 알고리즘 (plus-p-take-away-q algorithm) :  
p개의 특징을 추가하고 q개를 제거하는 연산을 반복하는 알고리즘이다.  
부분집합을 공집합으로 시작하여 가장 점수가 높은 특징벡터들을 p개 추가하고, 그 중에서 가장 점수가 낮은 특징벡터들을 q개 빼서 부분집합으로 만든다. 그리고 [d1,d2]차원 범위에 있는 결과 중 가장 점수가 높은 조합으로 선택하는것이 바로 PTA알고리즘이다.  

### 통계적 탐색 알고리즘  
위에서 말한 순차 탐색 알고리즘들은 욕심(GREEDY) 알고리즘이다.  
전역최적해가 아닌 지역최적해로 빠질 수 있는데, 그나마 PTA가 전역 최적해도 찾아갈 수 있는 가능성이 있다.  
이런 문제는 통계적 탐색 알고리즘을 사용하여 이를 극복할 수 있다.  
- 시뮬레이티드 어닐링 : 언제나 우수한 해로 이동하는 것이 아니라, 열등한 해로 이동도 하는 '담금질 기법'으로 다양한 형태를 나타낸다.  
- 유전 알고리즘 : 자식해가 꼭 좋은쪽으로만 가는것은 아니다. 임의의 해들을 찾아가는데 그곳들 중 더 좋은 성능을 갖는 해쪽으로 향한다.  
즉, 이곳저곳 방문하고 한번 알고리즘이 더 돌아가서 더욱 좋은 성능을 갖는 해쪽으로 향한다.  
이렇게 유전적으로 좀 더 성능이 좋아지도록 향하는, 뒤로 돌아가기도 하고 최적으로 가기도하며 여러 탐색을 하는것이 유전알고리즘이다.  


## 12주차  
### 기계학습 종류
기계학습에는 세가지 유형이 있다.  
- 지도학습 : 모든 훈련샘플이 레이블(결과) 정보를 가짐
- 비지도학습 : 모든 훈련샘플이 레이블(결과) 정보를 갖지 않음 
- 준지도학습 : 레이블을 갖는 샘플과 갖지않는 샘플이 섞여 있음

11주차까지 배운 기계학습은 지도학습에 관한 내용이다.  
현실에서는 많은 데이터들이 준지도학습으로 학습을 한다. 세상엔 방대한 데이터들이 있고 레이블을 갖는 데이터와 갖지않는 데이터가 다양하게 섞여있기 때문이다.  
지금까지는 레이블이 있는 경우를 다루었는데 수많은 데이터가 사실은 레이블이 없이 존재하는 편이다.  

부류정보가 존재하는 지도학습의 종류는 이전에 배운 베이시언분류기, MLP, SVM 등이 있다.  
부류정보가 존재하지 않는 비지도학습은 **군집화**라는 것을 통해 학습을 한다. 군집이 몇개인지 주어지기도 하고, 주어지지 않기도 한다.  
이렇게 군집화하는 과정을 **부류발견 작업**이라고도 한다.  

기계학습은 두 종류의 지식을 통해 학습을 한다.  
훈련집합, 사전지식(세상의 일반적 규칙)  
비지도학습과 준지도학습은 사전지식을 더 명시적으로 활용하는 학습이라고 할 수 있다.  

### 비지도 학습
비지도학습은 군집화를 통해 학습을 한다.  
지도학습에서의 과업이 다양하듯, 비지도학습도 과업이 세가지 있다.  
1. 군집화 : 유사한 샘플을 모아서 같은 그룹으로 묶는 일
2. 밀도 추정 : 데이터로부터 확률분포를 추정하는 일
3. 공간 변환 : 원래 특징공간을 저차원 or 고차원으로 변환하는 일. 보통은 저차원으로 변환한다

과업에서 볼 수 있듯 데이터에 내재한 구조를 잘 파악해서 새로운 정보를 발견해야 한다.  

**위 세가지의 과업을 응용하여 어떠한 일을 할 수 있을까**  
- 군집화를 통해 사람들을 같은 그룹으로 묶어서 맞춤광고, 영상분할, 유전자 데이터 분석, SNS실시간 검색어 분석 후 관심파악 등  
- 밀도추정을 통해 만들어진 그룹을 분류하고 생성모델을 구축하는 등  
- 공간변환을 통해 데이터를 가시화하거나 압축하고, 특징을 추출하는 등

### 군집화  
만약 대형 온라인쇼핑몰을 운영하면서 고객의 취향을 분석하여 그에 따라 4-6개의 팜플렛을 만들어 맞춤홍보를 하려고 생각할 때,  
수백만명이나 되는 고객들을 어떻게 4-6개의 그룹으로 분류할 수 있을까  

바로 **유사함**을 알 수 있어야 한다.  
유사한 샘플의 집합을 군집이라고 하는데, 군집해가 몇개인지 어떤것이 정답이고 오답인지 군집화 알고리즘에서는 존재하지 않는다. 따라서 **주관성**이 들어갈 수밖에 없다.  
왜냐면 군집화 결과의 품질은 처한 상황이나 요구사항에 따라 다르기 때문이다.  
군집화를 구현하기 위해서는 '거리에 대한 척도', '유사한 샘플을 군집화하는 알고리즘' 이렇게 두가지가 필요하다.  

군집화의 목표는, 같은 군집내 샘플을 서로 가깝고 다른 군집에 속한 샘플 사이의거리는 멀게하는 것이다.  
가깝다 멀다는 거리개념인데, 이 거리개념의 계산 방법은 개발되어 있고 어떤 방법을 쓰느냐에 따라 결과를 달라질수도 있다.  
따라서 적합한 거리측정방법을 사용하는것이 중요하다.  

거리와 유사도는 반비례관계인데, 따라서 거리와 유사도 둘 중 하나만 알면 다른 나머지 하나를 알 수 있다.  
거리 UP -> 유사도 DOWN , 거리 DOWN -> 유사도 UP  

특징의 종류는 다양하다.  
- 양적 특징  
    - 수량값 - 거리개념 있음  
- 질적 특징
    - 순서값(오디널) - 거리개념 있음
    - 명칭값(노미널) - 거리개념 없음

양적특징은 나이, 연봉, 몸무게 등 수치로 거리개념이 측정이 가능하다.  
질적특징은 순서값(오디널)과 명칭값(노미널)로 나뉘는데,  
**반품성향, 제품선호도, 소비자만족도(상중하)** 등 처럼 수치로는 나타내지지 않지만 거리개념은 존재하는 **순서값(오디널)**,  
그리고 **혈액형, 직업, 성별**처럼 거리개념이 존재하지 않는 **명칭값(노미널)**이 존재한다.  
즉, 순서값(오디널)이란 Sorting이 가능한 질적특징을 말한다.  

### 거리, 유사도 측정  
거리와 유사도를 측정하는 방법은 다양하다  
**거리 측정**  
- Minkowski 거리 : 두 점 xi.와 xj.간의 거리 척도  
dij = (시그마(k=1에서d)|xik - xjk|^p)^(1/p)  
p=2면 유클리디안 거리, p=1이면 도시블록 거리  
- Hamming 거리 : 이진 특징벡터에 적용가능 (서로 다른 비트의 개수)
특징벡터가 0과 1로만 이루어져있다면 서로 같은 개수의 특징벡터를 갖는 두 군집에서 같은 index의 특징벡터가 다른지 같은지를 통해 해밍거리를 측정한다.  
예시로 (1,0,1,0,0,0,1,1)^T와 (1,0,0,1,0,0,1,0)^T가 있으면 다른 벡터는 총 3개. 따라서 해밍거리는 3이다.  

**유사도 측정**  
- 코사인 유사도 : 문서검색 응용에서 주로 사용 (단어가 특징이고 출현빈도가 특징값이다)  
sij = cos세타 = xi.^T * xj. / ||xi.|| * ||xj.||  
- 이진 특징 벡터의 유사도 :  
sij = n00 + n11 / n00 + n11 + n01 + n10  
sij = n11 / n11 + n01 + n10  
- 유사도와 거리는 쉽게 상호변환 할 수 있다  
sij = dmax - dij  
dij = smax - sij  

### 점 집합에서 거리 구하기  
점x.와 점집합군집cj 간의 거리를 D(xi., cj)로 표기한다.  
두 군집 ci cj간 거리는 D(ci, cj)로 표기한다.  
- 점과 군집사이 거리를 구하는 방법은 여러가지가 있다.  
Dmax / Dmin / Dave / Dmean / Drep  
- Dmax와 Dmin은 외톨이(outlier)에 민감하다. 군집 간 가장 먼(max), 가까운(min) 원소들을 대표로 설정하는 방법이다.
- Dmean은 평균을 대표로 삼는 방법이다.
- Drep은 같은 부류 내, 다른 원소들과 가장 가까운 샘플을 그 군집의 대표로 삼는다.

### 군집화 알고리즘 분류  
군집화 알고리즘은 너무 다양하다.  
그렇기에 어떻게 군집화 시킬지는 주관이 많이 개입된다.  
분류는 크게 두개로 나뉜다.  
- 계층 군집화 : 군집 결과를 계층 덴드로그램으로 표현
- 분할 군집화 : 각 샘플을 군집에 배정하는 연산을 사용

### 분할 군집화
- 순차 알고리즘 : 군집개수를 찾아주는데 대신 **임계값 설정**해야함. 순서에 민감하고 빠르다 (선형시간)  
군집 개수를 설정해주면 한개 군집을 갖고 출발하여 xi와 가장 가까운 군집 cq를 찾고 혹시 임계값보다 멀면 새로운 군집을 만들고, 임계값보다 가까우면 xi이 속한 군집에 넣는다.  
- k-means 알고리즘 : 가장 널리 쓰이고 직관적이해가 되고 구현이 간편하다. **단 군집의 개수를 설정**해주어야한다.  
k개의 군집 중심 Z. 를 초기화하여 시작. 만약 5개 샘플 있는데 군집개수가 3개면 {x1},{x2},{x3,x4,x5}처럼 3개의 군집으로 초기화해준다.  
xi.를 가장 가까운군집 중심에 배정하고, zj에 배정된 샘플 평균으로 zj를 대치하는데 이를 반복.  
루프했는데 이전이랑 결과가 같을때까지 반복  
**이론적인 배경**은  
J(Z,U.)=시그마(i=1에서N)시그마(j=1에서k)uji||xi.-zj.||^2  
위 함수를 비용으로 하는 내리막경사법의 일종  
k-means 알고리즘은 항상 지역최적점으로 수렴하고(전역최적점 보장X)  
초기 군집 중심에 민감하다. 그리고 외톨이에 민감하다. 그래도 빠르다  
k-medoids라는것도 있는데, 얘는 중간점을 대표로 삼는 방법이다. 이 방법을 쓰면 외톨이에 보다 강한 군집을 만들 수 있다.  
- 모델 기반 알고리즘 : 샘플로부터 가우시언을 추정하고 그 결과에 따라 군집을 배정하는 방법이다.  
가우시언 추정은 EM 알고리즘 사용가능


## 13주차  
### 비지도학습



## 14주차  
