# 기계학습 강의  
문제 형태는 중간고사와 동일?  
파이썬을사용해라, 손으로해라 나타난게 없으면 어떻게 풀든 상관없다.  
근데 파이썬을 이용해서, 직접구해라 라는건 그거에 맞게 해야한다.  

## 준지도학습
이러한 학습방법이 있겠구나 정도만 알고있으면 된다.  

**준지도학습의 알고리즘**  
- 밀집 지역 회피 : 결정 경계가 밀집지역을 지나면 오분류 가능성이 높아지기때문에, 밀집 지역을 회피해서 결정경계를정한다. 예:TSVM(Transductive SVM)  
원래라면 labeling이 된 애들만을 기준으로 SupportVector를 구하고 그걸 통해 직선을 만들어서 학습을 수행하였는데,  
TSVM의 경우에는 labeling이 되지 않은 샘플들까지 사용하여 labeling이 된 SupportVector샘플들을 기준으로 하되, 밀집된 지역을 피하여 선을 긋는 방법으로 이루어져 있다.  

## 강화학습
기존의 지도학습과 비지도학습, 준지도학습에서는 불가능했던 과업을 달성할 수 있는 방법이다.  
시간적으로 흐르는 상황에서 그때마다 상태변화, 행동을 번갈아 수행하면서 보상과 벌을통해 목적을 달성한다.  

대표적인 예시가 '아기가 걸음마를 떼는 과정'이라고 할 수 있다.  
아이가 걸음마를 배울때 매우 많은 상황들이 있는데 이것들을 모두 만족하도록 해야할 일들을 학습하는건 말이안됨.  
그저 반복수행하는것만이 답.  

게임, 장기나 바둑같은 게임, 자동차의 자율주행, 로켓발사제어 등  
무한한 상태의 경우 어떻게 행동을 취해야하는가를 알게되는 것이 강화학습의 학습 결과이다.  

핵심연산은 시간에따른 상태, 행동, 보상의 순차적 처리를 통해 점차 강화되는 것.  
s0,r0 --a0-> s1,r1 --a1-> s2,r2 ... --at-1-> st,rt  
보상이 꼭 좋은것만은 아님. 잘못된 행동으로 벌을 받는것도 보상으로 인한 학습  
걷기위해 다양한 행동을 시도하다가 넘어진다? => 보상으로 마이너스 점수  
잘 걸어서 도착지에 무사히 도착했다? => 보상으로 플러스 점수  

### 보상 책정 방안
한걸음 잘 가면 +1점, 넘어지면 -1점, 크게 넘어지면 -10점, 도착지에 잘 도착하면 +100점 막 이런식으로...  
- 보상 책정은 에이전트(agent)와 환경(environment)  
에이전트는 정책을 가지고 행동을 결정함. 정책은 학습으로 알아내야 함  
환경은(MDP)를 갖고 상태전환과 보상액을 결정함.  

강화학습의 목표는 **누적 보상액을 최대화**하는것이다.  
정책에 따라 행동을 결정하기에 좋은 정책을 필요로 한다.  

상태값, 행동값, 보상값은 집합으로 표기가능하다.  
만약 보드판에 칸을 나누고 동서남북으로 이동가능. 이라고하고 판 밖으로 나가면 -1점수, 이동하면 0, 도착지 도착하면 +5로 해두면  
점점 여러번 반복할수록 어떠한 칸에서는 북쪽으로 가면 안되고, 어떠한 칸에서는 동쪽으로 가는게 제일 좋고. 이런 결과가 나오게된다.  
결국 여러번 반복하면 더욱 빠른 시간내에 +5점을 획득하는 최적의 방법을 찾겠지. 보상을 최대화하면서 어떻게 움직일것인지!  
이 경우.. Policy(정책)을 잘 만들어야함. 확률 규칙으로 행동을 결정하는데, 그냥 동일확률로 만약 동,서,남,북으로 갈 확률을 1/4로 하게되면 언젠가는 최종Goal에 도착할텐데.  
그럴때 최대 보상값인 5점을 받기위한 학습을 할것임 (나가지 않기만 하면 됨)  
따라서 확률이 원래는 어디에서나 동일하게 1/4씩이였는데, 이제는 16개의 칸중 가장자리 칸들에 대해서는 밖으로 나가지 않도록 확률을 조정할 수 있다. (맨 위 칸들은 북쪽으로 갈 확률 0, 왼쪽칸들은 서쪽 갈 확률 0,...)  
그러면 누적보상액은 항상 5점이 될 것이다.  
이렇게 조정하는것이 정책결정이다. 내생각엔 여기서 시간제한 (움직임의 제한. 10번 안에 도착 못하면 -1점같이..) 추가하면 가능해보임  

행동이나 보상, 상태가 여러개일수록 훨씬 복잡한 강화학습이 될 것이다.  

떄로는 잘못된 행동도 할수있지 다양한 방법을 찾아보기위해서.  
### 탐험과 탐사
**탐사** : 보상액이 꽤 괜찮아서 그 방법을 계속 수행하는것  
**탐험** : 보상은 좋지만 그래도 다른 공간들을 골고루 찾아보는것  
강화학습에서는 탐험과 탐사를 적절히 조율하며 사용해야한다.  
너무 탐사로 치우쳐지지 않게 주의를 기울여야한다.  

탐사는 2번에서 당첨? => 2번만 고집  
탐험은 2번에서 당첨? => 아 글쿤. 다른것도 해봐야지  
왜 탐사에 기울어지면 안되나면 더 좋은 방식이 있을 수 있는데 그걸 묵과하게됨  

균형방식은 탐사와 탐험을 둘다 적절히 섞어서 사용하는 방법이다.  
2번에서 돌리는 확률을 조금 더 증가시켜보는 방법이다.  
강화학습은 **균형방식**을 사용한다.  

### 마르코프 성질
강화학습은 모든 문제를 풀어주지않고 특정 조건을 만족해야만 해결해줄수있음. 대표적으로 **마르코프 성질**.  
행동을 결정할 때, 이전 이력이 중요하지 않다면 마르코프 성질을 만족한다.  

바둑판에서 첫번째에 두었던, 두번째에 두었던 과거에 영향을 받는것이 아닌, 그때그때 펼쳐진 상황에 대해 최적의 방법을 찾아야한다면 그게 마르코프성질을 만족하는 것이다.  

$P(s_{t+1}, r_{t+1}|s_t, a_t) = P(s',r|s,a)$  
마르코프 성질을 만족하지 못하는 경우는 어떤 경우냐,  
어제 비가 왔다면 오늘 비올확률이 좀 더 높고 그러니까.. 이런경우는 만족못한다.  
마르코프 성질을 만족하게 하기 위해서는 상태 자체를 여러개로 늘려서 문제를 다룰수있게 한다.  
맑은후맑음, 비온후맑음, 눈온후맑음, ..., 눈온후눈 이렇게 여러개로 늘림  

환경 모델로서 MDP(Markov decision process; 마르코프 결정 프로세스)  
- 결정론적 MDP : 상태 10에서 동쪽이라는 행동취하면 항상 11상태로 전환..
- 스토캐스틱 MDP : 가아아끔 돌풍이 불어서 동쪽 행동을 취했는데 일정 확률로 11이 아니라 7이나 15로도 갈수있는 경우를 만들수도 있다?  